# coding=utf8

import os
os.environ["CUDA_VISIBLE_DEVICES"]="0"
'''
    **************************** ABOUT 有關本檔案             ****************************

        NOMTrainer.py:

            This is a simple and small Python library for parsing
            Neural Object Models (NOM) to construct a TensorFlow
            neural network model for model training.
            這是一個簡單、小型的 Python 資源庫，用以解讀 Neural Object
            Models (NOM) 並編寫 TensorFlow 類神經網絡模型進行訓練。

        Creators 創作團隊: 

            Yu-Kai Zhou, Kai-Hsiang Lin, Hsiu-Chen Weng, Hoe-Yuan Ong, Pei-Hsuan Wu, Yin-Chung Leung, Jui-Hung Chang
            周育楷、林凱翔、翁琇甄、王皓玄、吳珮瑄、梁彥聰、張瑞紘

            @ Creative System and Software Applications Laboratory, National Cheng Kung Univerisity, Taiwan
            @ 創新系統軟體應用實驗室 —— 台灣 國立成功大學
        
        Version 版本:

            2109.00

        Website & Docs 網站及文件庫:
        
            English:    http://cssa.cc.ncku.edu.tw/ladder/NOMTrainer/
            中文:       http://cssa.cc.ncku.edu.tw/ladder/NOMTrainer/?lang=zh_TW

        Production in TAIWAN | 台灣製作
        

    **************************** COPYRIGHT NOTES 版權資訊     ****************************

        Copyright © 2020-2021 Creative System and Software Applications Laboratory

        This program is free software: you can redistribute it and/or modify
        it under the terms of the GNU General Public License as published by
        the Free Software Foundation, either version 3 of the License, or
        (at your option) any later version.

        This program is distributed in the hope that it will be useful,
        but WITHOUT ANY WARRANTY; without even the implied warranty of
        MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
        GNU General Public License for more details.

        You should have received a copy of the GNU General Public License
        along with this program.  If not, see <https://www.gnu.org/licenses/>.

        本程式為自由軟體，在自由軟體聯盟發佈的GNU通用公共授權合約的約束下，你可以對其
        進行再發佈及修改。協議版本為第三版或（隨你）更新的版本。

　　    我們希望發佈的這款程式有用，但不保證，甚至不保證它有經濟價值和適合特定用途。詳
        情參見GNU通用公共授權合約。

　　    你理當已收到一份GNU通用公共授權合約的副本，如果沒有，請查閱<http://www.gnu.org/licenses/> 
　　    同時提供你的電子郵寄地址或傳統的郵件聯繫方式。

        Contact us 連繫我們:  ladder.cssa@gmail.com

'''
import collections, os, csv, struct, math, functools, random, json, sys, re
import tensorflow as tf
from datetime import datetime
import numpy as np
from sklearn.metrics.cluster import normalized_mutual_info_score
from enum import Enum
from typing import Any, Type, List, Dict, Set, Tuple, Union, Callable, Iterable, Optional
from scipy import stats

nowPath = sys.argv[0]
nowPathLength = len(nowPath)
actualPath = nowPath[0:(max(nowPath.rfind('/'),0) or nowPathLength)][0:(max(nowPath.rfind('\\'),0) or 0)]
if (len(actualPath)): os.chdir(actualPath)

random.seed()


class CSVLogger():
    '''
			
        Class representing a helper for creating logs as a CSV file. This is often used in tracking the training steps.   --- UPDATED (Dexter) 20180615
    '''

    def __init__(self, location: str, fileName: str, header: List[str]=None):
        '''
			Creates a CSV logger.   --- UPDATED (Dexter) 20191001
            
            Parameters
            ------------------------------

            location    `str`           - The directory where the CSV file to be saved.

            fileName    `str`           - The name of the training where the logging takes place.

            header      `list[str]`     - The header column names in a 'list' format.
        '''
        # Ensure the data type of the parameters.
        location = str(location); fileName = str(fileName); 
        header = list(header) if header is not None else None

        # Create a new folder if needed.
        if not os.path.isdir(location):
            tf.io.gfile.makedirs(location)

        # Stores the attribute values.
        self._filename = filename = location + fileName+".csv"
        self._header = header
        self._logTable = []

        # Initiate a file if it's a new file.
        if (not os.path.isfile(filename)):
            with open(filename, 'w', newline='', encoding='utf-8-sig') as csvfile:
                toWriter = csv.writer(csvfile)
                if (header):
                    toWriter.writerow(header)
    
    def __len__(self) -> int:
        '''
			Alias for `self.length`.
        '''
        return self.length
    
    @property
    def length(self) -> int:
        '''
			Get the length (row count) of the current log table.   --- UPDATED (Dexter) 20180615

            Returns
            ------------------------------

            `int`   - The length of `self._logTable`.
        '''
        return len(self._logTable)
    
    @property
    def size(self) -> int:
        '''
			Get the size (column count) of this log table.   --- UPDATED (Dexter) 20180615

            Returns
            ------------------------------

            `int`   - The length of the first column of `self._logTable`, assuming all records in the log table are the same in column count.
        '''
        return 0 if self.length == 0 else len(self._logTable[0])

    def __repr__(self) -> str:
        '''
			Get a string representation of this log table.   --- UPDATED (Dexter) 20180615

            Returns
            ------------------------------

            `str`   - The representation including the row and column count.
        '''
        return "<CSV Logger: " + self.length + " rows, " + self.size + " columns>"

    def __bool__(self) -> bool:
        '''
			Get a boolean value of this log table.   --- UPDATED (Dexter) 20180615

            Returns
            ------------------------------

            `bool`  - True if there has any rows of data.
        '''
        return self.length > 0

    def log(self, *rows: List[Any]):
        '''
			Append rows in the log table.   --- UPDATED (Dexter) 20180615

            Parameters
            ------------------------------

            *rows   `list[*]+`       - Arbitrary count of a data row.
        '''
        for r in rows:
            self._logTable.append([*r])
    
    def push(self, *rows: List[Any]):
        '''
			Alias for `self.log()`
        '''
        self.log(*rows)
    
    def append(self, *rows: List[Any]):
        '''
			Alias for `self.log()`
        '''
        self.log(*rows)
        
    def flush(self):
        '''
			Save this log table, and clear all temporarily logged rows.   --- UPDATED (Dexter) 20180615
        '''
        if (len(self._logTable)):
            with open(self._filename, 'a', newline='', encoding='utf-8-sig') as csvfile:
                toWriter = csv.writer(csvfile)

                # Write the rows into CSV file.
                for row in self._logTable:
                    toWriter.writerow([str(ele) for ele in row])

                # Clear the existing log array.
                self._logTable = []
    
    def logAndSave(self, *rows: List[Any]):
        '''
			Append new rows, then save the log table and clear all temporarily logged rows.   --- UPDATED (Dexter) 20180716

            Parameters
            ------------------------------

            *rows   `list[*]+`       - Arbitrary count of data row.
        '''
        self.log(*rows)
        self.flush()

class TimeHelper():
    '''
			Class of functions assisting time-related operations.   --- UPDATED (Dexter) 20180615
    '''

    @staticmethod
    def getDateStr(nowDate: 'datetime.datetime' = datetime.now(), expression: str = "mmdd", delimiter: str = "") -> str:
        '''
			Stringify a datetime object for the date component.   --- UPDATED (Dexter) 20180615

            Parameters
            ------------------------------

            nowDate     `datetime.datetime`     - The datetime object to be stringified.
            
            expression  `str`                   - The string format.

            delimiter   `str`                   - The delimiter between each sub-components.

            Returns
            ------------------------------

            `str`   - The date string.
        '''
        if (expression == "mmdd"):
            return '{:%m{delim}%d}'.format(nowDate, delim=delimiter)
        elif (expression == "yyyymmdd"):
            return '{:%Y{delim}%m{delim}%d}'.format(nowDate, delim=delimiter)
        elif (expression == "yymmdd"):
            return '{:%y{delim}%m{delim}%d}'.format(nowDate, delim=delimiter)
    
    @staticmethod
    def getTimeStr(nowDate: 'datetime.datetime' = datetime.now(), expression: str = "hhmm", delimiter: str = "") -> str:
        '''
			Stringify a datetime object for the time component.   --- UPDATED (Dexter) 20190119

            Parameters
            ------------------------------

            nowDate     `datetime.datetime`     - The datetime object to be stringified.
            
            expression  `str`                   - The string format.

            delimiter   `str`                   - The delimiter between each sub-components.

            Returns
            ------------------------------

            `str`   - The time string.
        '''
        if (expression == "mmss"):
            return '{:%M{delim}%S}'.format(nowDate, delim=delimiter)
        elif (expression == "hhmm"):
            return '{:%H{delim}%M}'.format(nowDate, delim=delimiter)
        elif (expression == "hhmmss"):
            return '{:%H{delim}%M{delim}%S}'.format(nowDate, delim=delimiter)
        elif (expression == "mmsstttttt"):
            return '{:%M{delim}%S{delim}%f}'.format(nowDate, delim=delimiter)
        elif (expression == "tttttt"):
            return '{:%f}'.format(nowDate, delim=delimiter)
    
    @staticmethod
    def getDateTimeStr(nowDate: 'datetime.datetime' = datetime.now(), dateExpr: str = "mmdd", timeExpr: str = "hhmm", dateDelimiter: str = "", timeDelimiter: str = "", dateTimeDelimiter: str = " ") -> str:
        '''
			Stringify a datetime object for the time component.   --- UPDATED (Dexter) 20180615

            Parameters
            ------------------------------

            nowDate     `datetime.datetime`     - The datetime object to be stringified.
            
            dateExpr            `str`           - The string format for the date component.

            timeExpr            `str`           - The string format for the time component.

            dateDelimiter       `str`           - The delimiter between each sub-components for the date component.

            timeDelimiter       `str`           - The delimiter between each sub-components for the time component.

            dateTimeDelimiter   `str`           - The delimiter between the date and time components.

            Returns
            ------------------------------

            `str`   - The datetime string.
        '''
        return TimeHelper.getDateStr(nowDate, dateExpr, dateDelimiter) + dateTimeDelimiter + TimeHelper.getTimeStr(nowDate, timeExpr, timeDelimiter)

    class Recorder():
        '''
			Class representing a time recorder.   --- UPDATED (Dexter) 20180615
        '''
        def __init__(self):
            '''
			Create a recorder.   --- UPDATED (Dexter) 20180615
            '''
            # Set the initial and current time as now.
            self._now = self._initial = datetime.now()
        
        def log(self) -> float:
            '''
			Log a time, checking the different between initial time.   --- UPDATED (Dexter) 20180615

                Returns
                ------------------------------

                `float` - The number of seconds, in micro-second accuracy in most cases, between the current time and the initial time.
            '''
            self._now = nowTime = datetime.now()
            return (nowTime - self._initial).total_seconds()
        
        def logAndRestart(self) -> float:
            '''
			Log a time, checking the different between initial time and restarting the initial time.   --- UPDATED (Dexter) 20180615

                Returns
                ------------------------------

                `float` - The number of seconds, in micro-second accuracy in most cases, between the current time and the initial time.
            '''
            oriTime = self._initial
            self._now = self._initial = nowTime = datetime.now()
            return (nowTime - oriTime).total_seconds()

        @property
        def now(self) -> 'datetime.datetime':
            '''
			Get the latest recorded time.   --- UPDATED (Dexter) 20180615

                Returns
                ------------------------------

                `datetime.datetime` - The previous time at which this recorder has logged.
            '''
            return self._now
        
        @property
        def initial(self) -> 'datetime.datetime':
            '''
			Get the initial time.   --- UPDATED (Dexter) 20180615

                Returns
                ------------------------------

                `datetime.datetime` - The initial time at which this recorder has started.
            '''
            return self._initial

class Enumeration(Enum):
    @classmethod
    def getName(cls, value: int) -> str:
        """
            Get the name from the value within this enumeration.   --- UPDATED (Dexter) 20190425

            Parameters
            ------------------------------

            value   `int`   The value of the enumeration.

            Returns
            ------------------------------

            name   `str`   The name of the enumeration.
        """
        for enumItem in cls:
            if (enumItem.value == value):
                return enumItem.name

    @classmethod
    def getValue(cls, name: str) -> int:
        """
            Get the value from the name within this enumeration.   --- UPDATED (Dexter) 20190425

            Parameters
            ------------------------------

            name   `str`   The name of the enumeration.

            Returns
            ------------------------------

            value   `int`   The value of the enumeration.
        """
        return getattr(cls, name).value

    @classmethod
    def parse(cls, value: Union[int, str]) -> 'Enumeration':
        """
            Parse the value of the enumeration.   --- UPDATED (Dexter) 20190922

            Parameters
            ------------------------------

            value `int|str` - The value from the JSON file.  WARNING: JSON value will be stored as name string instead of numerical value from the next version.
        """
        if (isinstance(value, str)):
            return getattr(cls, value)
        else:
            return getattr(cls, cls.getName(value))

def classof(obj: Any) -> str:
    '''
		Get the class name of a certain object.   --- UPDATED (Dexter) 20180615

        Parameters
        ------------------------------

        obj     `*`     - Any objects.

        Returns
        ------------------------------

        `str` - The class name of the given object.
    '''
    return obj.__class__.__name__

def escapeNaN(obj: Any) -> Any:
    '''
        Escape NaN to a string NaN for universal JSON parsing.   --- --- UPDATED (Dexter) 20190209

        Parameters
        ------------------------------

        obj     `*`     - Any objects.

        Returns
        ------------------------------

        `*` - The original object, or `"NaN"` string if it's NaN.
    '''
    return "NaN" if (obj is np.nan or obj != obj) else obj

def escapeNaNList(listObj: List[Any]) -> List[Any]:
    '''
        Escape NaN to a string NaN for universal JSON parsing in a list.   --- UPDATED (Dexter) 20190209

        Parameters
        ------------------------------

        obj     `*`     - Any objects.

        Returns
        ------------------------------

        `*` - The original object, or elements replacing with a `"NaN"` string if it's NaN.
    '''
    return [("NaN" if (obj is np.nan or obj != obj) else obj) for obj in listObj]

def escapeNaNNPAry(nparray: Any) -> Any:
    '''
        Escape NaN to a string NaN for universal JSON parsing in a numpy array.   --- --- UPDATED (Dexter) 20190209

        Parameters
        ------------------------------

        nparray     `np.ndarray`     - A numpy array of any type.

        Returns
        ------------------------------

        `np.ndarray` - The original object, or elements replacing with a `"NaN"` string if it's NaN.
    '''
    return np.where(np.isnan(nparray), None, nparray) if np.issubdtype(nparray.dtype, np.float) else nparray

class IndexRange:
    '''
			Class of methods handling index range strings.   --- UPDATED (Dexter) 20181210
    '''
    @staticmethod
    def parse(selectionLength: int, rangeStr: str, repeatable: bool = False) -> 'list[int]':
        '''
			Parse an index range string into a list of selected indices.   --- UPDATED (Dexter) 20181210

            Parameters
            ------------------------------

            selectionLength     `int`   - The total seleection length.

            rangeStr            `str`   - A range string, validated before submission.

            repeatable          `bool`  - Whether selection can be repeatably selected.

            Returns
            ------------------------------

            `list[int]` - A list of indices to be selected.
        '''
        if not IndexRange.validate(selectionLength, rangeStr):
            raise ValueError("Index range string is not valid.")

        if (rangeStr.startswith("[") and rangeStr.endswith("]")):
            allIdx = [((selectionLength + n) if n < 0 else n) for n in [int(string.strip()) for string in rangeStr[1:-1].split(",")] if n < selectionLength and n >= -selectionLength]
            return allIdx if repeatable else [*(set(allIdx))]
        elif (":" in rangeStr):
            rangeSel = [string.strip() for string in rangeStr.split(":")]
            if (rangeSel[0] == "None" or rangeSel[0] == ""):
                rangeSel[0] = 0
            if (rangeSel[1] == "None" or rangeSel[1] == ""):
                rangeSel[1] = selectionLength
            rangeSel = [min(max(((selectionLength + n) if n < 0 else n), 0), selectionLength) for n in [int(n) for n in rangeSel]]
            nums = []
            for i in range(rangeSel[0], rangeSel[1]):
                nums.append(i)
            return nums
        else:
            allIdx = [((selectionLength + n) if n < 0 else n) for n in [int(string.strip()) for string in rangeStr.split(",")] if n < selectionLength and n >= -selectionLength]
            return allIdx if repeatable else [*(set(allIdx))]
    
    @staticmethod
    def validate(selectionLength: int, rangeStr: str) -> bool:
        '''
            Validate whether a range string is syntatically correct.   --- UPDATED (Dexter) 20181210

            Parameters
            ------------------------------

            selectionLength     `int`   - The total seleection length.

            rangeStr            `str`   - A range string, validated before submission.

            Returns
            ------------------------------

            `bool`              - Whether the range string is correct.
        '''
        if (rangeStr.startswith("[") and rangeStr.endswith("]")):
            nums = [string.strip() for string in rangeStr[1:-1].split(",")]
            return all([re.match("[0-9\-]+", num) for num in nums]) and all([(n < selectionLength and n >= - selectionLength) for n in [int(n) for n in nums]])
        elif (":" in rangeStr):
            nums = [string.strip() for string in rangeStr.split(":")]
            return len(nums) == 2 and all([(num == "" or num == "None" or re.match("[0-9\-]+", num)) for num in nums])
        else:
            nums = [string.strip() for string in rangeStr.split(",")]
            return all([re.match("[0-9\-]+", num) for num in nums]) and all([(n < selectionLength and n >= - selectionLength) for n in [int(n) for n in nums]])

    @staticmethod
    def toIndexer(rangeStr: str, repeatable: bool = False) -> Union[List[int], slice]:
        '''
            Parse an index range string into a list of selected indices.   --- UPDATED (Dexter) 20190508

            Parameters
            ------------------------------

            rangeStr            `str`   - A range string, validated before submission.

            Returns
            ------------------------------

            `list[int]|slice` - A list of indices to be selected.
        '''
        if (rangeStr.startswith("[") and rangeStr.endswith("]")):
            # Request of specific indices (list of indices)
            allIdx = [n for n in [int(string.strip()) for string in rangeStr[1:-1].split(",")]]
            return allIdx if repeatable else [*(set(allIdx))]

        elif (":" in rangeStr):
            # If it's a slicer, conver to slice object.
            rangeSel = [string.strip() for string in rangeStr.split(":")]

            # Convert the object into None or integer
            if (rangeSel[0] == "None" or rangeSel[0] == ""):
                rangeSel[0] = None
            else:
                rangeSel[0] = int(rangeSel[0])
            if (rangeSel[1] == "None" or rangeSel[1] == ""):
                rangeSel[1] = None
            else:
                rangeSel[1] = int(rangeSel[1])

            # Return the native slice object.
            return slice(*rangeSel)

        else:
            # Request of specific indices (comma seperated values)
            allIdx = [n for n in [int(string.strip()) for string in rangeStr.split(",")]]
            return allIdx if repeatable else [*(set(allIdx))]

class DataGenerator:
    """
        Module controlling data generators.   --- UPDATED (Dexter) 20190528
    """
    class Dataset:
        """
            Module containing class and methods regarding source raw datasets.   --- UPDATED (Dexter) 20190506
        """
        class Types(Enumeration):
            """
                Enumeration defining the types of dataset sources for a training process.   --- UPDATED (Dexter) 20200105
            """
            # `int` - Training dataset.
            Train = 0
            # `int` - Training dataset during cross validation phase.
            ValidationTrain = 1
            # `int` - Validation dataset during cross validation phase.
            Validation = 2
            # `int` - Test dataset.
            Test = 3
            # `int` - Prediction dataset.
            Prediction = 4
        
        class TestSource(Enumeration):
            """
                Enumeration defining the way for getting test sources.   --- UPDATED (Dexter) 20190506
            """
            # `int` - To split from collected data.
            Split = 0
            # `int` - To assign test datasets for each source.
            Assign = 1
    
    class Detail:
        """
            Abstract class reprenting the data generator detail for each dataset.   --- UPDATED (Dexter) 20190528
        """
        def __init__(self, controller: 'DataGenerator.Controller'):
            """
                Virtual method to initialize the generator loop.   --- UPDATED (Dexter) 20190706

                Parameters
                ------------------------------

                controller `DataGenerator.Controller` - The dataset controller maintaining this data generator detail.
            """
            # `bool` - Whether the loop has been initialized for data generation.
            self._initialized: bool = False
            # `int` - The epoch size of this generator detail. Defined when the data is first read.
            self._epochSize: int = None
            # `list[int]` - The original shape of the data source, including the batch dimension.
            self._oriShape: List[int] = None
            # `DataGenerator.Controller` - The dataset controller maintaining this data generator detail.
            self._controller: DataGenerator.Controller = controller
            # `int` - The itereation counter.
            self._itrIdx: int = None
            # `int` - The batch size of this generator detail, the override value from the training config.
            self._batchSize: int = None
            # `bool` - Whether to shuffle this generator detail, the override value from the training config.
            self._shuffle: bool = None
        
        def parseFromJSON(self, obj: Dict[str, Any]):
            """
                Vitual method to parse a previously saved object into this @DataGenrator.Detail object.   --- UPDATED (Dexter) 20191013

                Parameters
                ------------------------------

                obj     `dict<str,*>`   - JSON object from Project file.
            """
            # Parse for each key of dataset detail.
            for k,v in obj.items():
                if (k not in ["_controller"]):
                    setattr(self, k, v)

        def overrideConfig(self, configs: Dict[str, Any]):
            """
                Override the default value from the original training build config, only a limited range of configurations can be overriden.   --- UPDATED (Dexter) 20190921
                
                Parameters
                ------------------------------

                configs `dict<str,*>` - An detail object with necessary public information to override on this detail object.
            """
            # Get every key of the object and replace those values with the incoming object specified values.
            for k,v in configs.items():
                if (k in ["shuffle", "batchSize"]):
                    setattr(self, "_" + k, v)

        def copy(self) -> 'DatasetGenerator.Detail':
            """
                Abstract method to copy this generator detail   --- UPDATED (Dexter) 20190921

                Returns
                ------------------------------

                `DatasetGenerator.Detail` - The copied generator detail object.
            """
            pass

        @property
        def initialized(self) -> bool:
            """
                Whether the generator is initialized for a generator loop.   --- UPDATED (Dexter) 20190528

                Returns
                ------------------------------

                `bool` - Whether this generator loop is initialized.
            """
            return self._initialized
        
        @property
        def epochSize(self) -> int:
            """
                Get the epoch size of this training source.   --- UPDATED (Dexter) 20190606

                Returns
                ------------------------------

                `int` - The epoch size.
            """
            return self._epochSize
            
        @property
        def oriShape(self) -> List[int]:
            """
                Get the shape of the data of this data generator detail.   --- UPDATED (Dexter) 20190606

                Returns
                ------------------------------

                `list<int>` - The shape of the data of this data generator detail.
            """
            return self._oriShape
        
        @property
        def controller(self) -> 'Dataset.Controller':
            """
                The dataset controller maintaining this data generator detail.   --- UPDATED (Dexter) 20190706

                Returns
                ------------------------------

                `DataGenerator.Controller` - The dataset controller maintaining this data generator detail.
            """
            return self._controller

        @property
        def attachObject(self) -> Union['Source.Config', 'ModelNode.Config']:
            """
                The object this data generator controller is embedded with.   --- UPDATED (Dexter) 20190706

                Returns
                ------------------------------

                `(Source.Config|ModelNode.Config)` - The object this data generator controller is embedded with.
            """
            return self.controller.attachObject

        @property
        def batchSize(self) -> int:
            """
                Get the batch size of current training.   --- UPDATED (Dexter) 20190921

                Returns
                ------------------------------

                `int` - The batch size.
            """
            return self.attachObject.batchSize if self._batchSize is None else self._batchSize
        
        @property
        def dropRemainder(self) -> bool:
            """
                Whether to drop the remaining batch of data in case the full dataset cannot be fully divided by the requested batch size.   --- UPDATED (Dexter) 20190708

                Returns
                ------------------------------

                `bool` - Whether to drop the remaining batch of data in case the full dataset cannot be fully divided by the requested batch size.
            """
            return self.controller.attachObject.train.currentBuildConfig.dropRemainder

        @property
        def batchCountPerEpoch(self) -> int:
            """
                Get the number of batches included in one epoch.   --- UPDATED (Dexter) 20190915

                Returns
                ------------------------------

                `bool` - The number of batches included in one epoch.
            """
            return max(self.epochSize // self.batchSize, 1) if self.dropRemainder else max(math.ceil(self.epochSize / self.batchSize), 1)

        @property
        def shuffle(self) -> bool:
            """
                Whether the data source will be shuffled on training.   --- UPDATED (Dexter) 20190921

                Returns
                ------------------------------

                `bool`   - Whether the data source will be shuffled on training.
            """
            return self.attachObject.shuffle if self._shuffle is None else self._shuffle

        def __len__(self) -> int:
            """
                Get the epoch size of this training source.   --- UPDATED (Dexter) 20190606

                Returns
                ------------------------------

                `int` - The epoch size.
            """
            return self._epochSize
        
        @property
        def batchIdx(self) -> int:
            """
                Get the current batch index (aka iteration index).   --- UPDATED (Dexter) 20190708

                Returns
                ------------------------------

                `int` - The current batch index.
            """
            return self._itrIdx
        
        def batchIdxIncrement(self):
            """
                Increment the batch index, typically called when getting a new batch of data.   --- UPDATED (Dexter) 20190708
            """
            self._itrIdx += 1
        
        def batchIdxReset(self):
            """
                Reset the batch index, typically called after an epoch is looped.   --- UPDATED (Dexter) 20190708
            """
            self._itrIdx = 0

        def getLocations(self) -> List[str]:
            """
                Abstract method to get all the location that this source will reference for.   --- UPDATED (Dexter) 20190727

                Returns
                ------------------------------

                `list<str>` - A list of paths that this sources is referencing on.
            """
            return []
            
        def initialize(self):
            """
                Virtual method to intitiate a generator loop.   --- UPDATED (Dexter) 20190528
            """
            self._initialized = True
        
        def prepareIteration(self):
            """
                Abstract method to prepare iteration before each epoch of dataset looping.   --- UPDATED (Dexter) 20190528
            """
            pass
        
        def getNextBatch(self) -> Any:
            """
                Abstract method to get the next batch from the core loop.   --- UPDATED (Dexter) 20190528

                Returns
                ------------------------------

                `np.ndarray` - The next batch of data of this generator detail.
            """
            pass
        
        def __next__(self) -> Any:
            """
                Alias for .getNextBatch() method.   --- UPDATED (Dexter) 20190528

                Returns
                ------------------------------

                `np.ndarray` - The next batch of data of this generator detail.
            """
            return self.getNextBatch()
        
        def close(self):
            '''
                Virtual method for ending a life cycle of using data source. No action taken, and a sub-class should be used.   --- UPDATED (Dexter) 20190606
            '''
            self._initialized = False

        def getData(self, start: int = None, end: int = None, indexes: List[int] = None) -> Any:
            """
                Abstarct method to get the data of with specific indexes.   --- UPDATED (Dexter) 20190528

                Parameters
                ------------------------------

                start       `int`       - Batch starting index (inclusive). If `None`, it notates `0`.

                end         `int`       - Batch ending index (exclusive). If `None`, it notes the epoch size.

                indexes     `list[int]` - A list of indexes of data rows.

                Returns
                ------------------------------

                `np.ndarray` - The requested slice of data of this generator detail.
            """
            pass
        
        def getRandItems(self, randomSeed: int = None, randomCount: int = None) -> Any:
            """
                Abstarct method to generate the random items of data. No action taken, and a sub-class should be used.   --- UPDATED (Dexter) 20190528

                Parameters
                ------------------------------

                randomSeed      `int`       - Random seed.

                randomCount     `int`       - The count of random items to collect.

                Returns
                ------------------------------

                `np.ndarray` - Random samples of data of this generator detail.
            """
            pass

        def partition(self, assignTo: 'DataGenerator.Dataset.Types', prop: float = 0.2, shuffle: float = False):
            '''
                Abstract method for partitioning current data into 2 parts.   --- UPDATED (Dexter) 20190708

                Parameters
                ------------------------------

                assignTo        `DataGenerator.Dataset.Types`   - Assign the splitted dataset with the `prop` assigned to another set of data generator controller.

                prop            `float`     - Proportion of original dataset to be the secondary dataset.

                shuffle         `bool`      - Whether to shuffle before data splitting.
            '''
            pass
        
    class Controller:
        """
            Abstract class representing a composition of loop information on a source-like generator.   --- UPDATED (Dexter) 20190522
        """
        def __init__(self, attachObject: Union['Source.Config', 'DataPreprocessing.Node.SourceLike'] = None):
            """
                Create a data generator controller.   --- UPDATED (Dexter) 20190713

                Parameters
                ------------------------------

                attachObject `(Source.Config|DataPreprocessing.Node.SourceLike)` - The object this data generator controller is embedded with.
            """
            # `dict<DataGenerator.Dataset.Types, DataGenerator.Detail>` - The dictionary storing the generator detail of each type of dataset.
            self._datasets = {}
            # `int` - The cross validation time of generating data.
            self.validationTime = 0
            # `(Source.Config|DataPreprocessing.Node.SourceLike)` - The object this data generator controller is embedded with.
            self._attachObject = attachObject
            
            # For each dataset type, initiate each info of generator detailobjects.
            for datasetType in DataGenerator.Dataset.Types:
                self._datasets[datasetType] = self.detailType(self)
        
        def parseFromJSON(self, obj: Dict[str, Any]):
            """
                Vitual method to parse a previously saved object into this @DataGenrator.Controller object.   --- UPDATED (Dexter) 20191013

                Parameters
                ------------------------------

                obj     `dict<str,*>`   - JSON object from Project file.
            """
            for k,v in obj.items():
                # Parse for each key of dataset detail.
                if (k == "_datasets"):
                    for dk, dv in v.items():
                        datasetType = DataGenerator.Dataset.Types.parse(dk)
                        if datasetType not in self:
                            self[datasetType] = self[DataGenerator.Dataset.Types.Train].copy()
                        self[datasetType].parseFromJSON(dv)
                
                # Set for other attributes.
                elif (k not in ["_attachObject"]):
                    setattr(self, k, v)

        @property
        def attachObject(self) -> Union['Source.Config', 'DataPreprocessing.Node.SourceLike']:
            """
                The object this data generator controller is embedded with.   --- UPDATED (Dexter) 20190706

                Returns
                ------------------------------

                `(Source.Config|DataPreprocessing.Node.SourceLike)` - The object this data generator controller is embedded with.
            """
            return self._attachObject

        @property
        def detailType(self) -> Callable[..., 'DataGenerator.Detail']:
            """
                Get the data generator detail type.   --- UPDATED (Dexter) 20190713

                Returns
                ------------------------------

                `Callable<*, DataGenerator.Detail>`    - The type of the generator detail.
            """
            return DataGenerator.Detail
        
        def getLocations(self) -> List[str]:
            """
                Get all the location that this source will reference for.   --- UPDATED (Dexter) 20190727

                Returns
                ------------------------------

                `list<str>` - A list of paths that this sources is referencing on.
            """
            return sum([detail.getLocations() for detail in self], [])
            
        def __getitem__(self, datasetType: 'DataGenerator.Dataset.Types') -> 'DataGenerator.Detail':
            """
                Get the generator detail of a specific dataset type.   --- UPDATED (Dexter) 20190523

                Parameters
                ------------------------------

                datasetType     `DataGenerator.Dataset.Types`      - The dataset type of the generator detail.
                
                Returns
                ------------------------------

                `DataGenerator.Detail`     - The generator detail of a specific dataset type.
            """
            return self._datasets[datasetType]
        
        def __setitem__(self, datasetType: 'DataGenerator.Dataset.Types', detail: 'DataGenerator.Detail'):
            """
                Set the generator detail of a specific dataset type.   --- UPDATED (Dexter) 20191013

                Parameters
                ------------------------------

                datasetType     `DataGenerator.Dataset.Types`      - The dataset type of the generator detail.

                detail `DataGenerator.Detail` - The generator detail.
            """
            self._datasets[datasetType] = detail
        
        def __iter__(self) -> Iterable[str]:
            '''
                Iterator for dataset controller.   --- UPDATED (Dexter) 20190727
                
                Returns
                ------------------------------

                `iterable<str>`     - Iterable keys of this dataset controller.
            '''
            return self._datasets.__iter__()
        
        def __values__(self) -> Iterable['DataGenerator.Detail']:
            '''
                Iterator for dataset controller.   --- UPDATED (Dexter) 20191001
                
                Returns
                ------------------------------

                `iterable<DataGenerator.Detail>`     - Iterable generator detail of this dataset controller.
            '''
            return self._datasets.values()
        
        def values(self) -> Iterable['DataGenerator.Details']:
            """
                Iterator for dataset controller details.   --- UPDATED (Dexter) 20190727

                Returns
                ------------------------------

                `iterable<DataGenerator.Details>` - Iterable on every dataset generator detail values in this controller.
            """
            return self._datasets.values()
        
        def keys(self) -> Iterable['DataGenerator.Dataset.Type']:
            """
                Iterator for dataset controller keys.   --- UPDATED (Dexter) 20190727

                Returns
                ------------------------------

                `iterable<DataGenerator.Dataset.Type>` - Iterable on every key refering the dataset generator detail keys in this controller.
            """
            return self._datasets.keys()
        
        def initialize(self, datasetType: 'DataGenerator.Dataset.Types'):
            """
                Initialize the loop detail of a specific dataset type.   --- UPDATED (Dexter) 20190527

                Parameters
                ------------------------------

                datasetType     `DataGenerator.Dataset.Types`      - The dataset type of the loop detail.
            """
            self[datasetType].initialize()

        def getNextBatch(self, datasetType: 'DataGenerator.Dataset.Types') -> 'np.ndarray':
            """
                Generate the next batch of data rows of this data source.   --- UPDATED (Dexter) 20190606

                Parameters
                ------------------------------

                datasetType   `DataGenerator.Dataset.Types`  - The type of dataset of a source that's this data is referencing.

                Returns
                ------------------------------

                'np.ndarray' - The next batch data. In the future, the data will be output as `tf.Tensor` objects.
            """
            return self[datasetType].getNextBatch()

        def splitValidationDataset(self, validation: float = 0.1, randomFold: bool = False):
            '''
                Abstract method for splitting current data into training and validation datasets.   --- UPDATED (Dexter) 20190528
                
                Parameters
                ------------------------------

                validation      `float`     - Proportion of original dataset to be as the validation dataset.

                randomFold `bool` - Whether to perform random sub-sampling validation.
            '''
            pass
        
        def splitTestDataset(self, test: float = 0.2, shuffle: float = False):
            '''
                Abstract method for splitting original train data into train and test sets.   --- UPDATED (Dexter) 20190528

                Parameters
                ------------------------------

                test            `float`     - Proportion of original dataset to be as the test dataset.

                shuffle         `bool`      - Whether to shuffle before data splitting.
            '''
            pass

        def close(self, datasetType: 'DataGenerator.Dataset.Types'):
            '''
                End a life cycle of using this data source.   --- UPDATED (Dexter) 20190914

                Parameters
                ------------------------------

                datasetType   `DataGenerator.Dataset.Types`  - The type of dataset of a source that's this data is referencing.
            '''
            self[datasetType].close()

class _DataPreprocessingTransformationConfig:
    '''
        Class representing a transformation configuration on data preprocessing.   --- UPDATED (Dexter) 20190201
    '''
    def __init__(self, instanceClass: 'DataPreprocessing.Transformation.Types', method: 'Enumeration'):
        '''
            Create a configruation for data transformation in data preprocessing.   --- UPDATED (Dexter) 20190130

            Parameters
            ------------------------------

            instanceClass       `DataPreprocessing.Transformation.Types`    - The instance class, as defined in @DataPreprocessing.Transformation.Types .

            method              `Enumeration`      - The method type of transformation.
        '''
        # `DataPreprocessing.Transformation.Types` - The instance class, as defined in @DataPreprocessing.Transformation.Types .
        self._instanceClass = instanceClass
        # `Enumeration` - The method type of transformation.
        self._method = method
    
    @property
    def instanceClass(self) -> 'DataPreprocessing.Transformation.Types':
        '''
            The enumeration for the instance class of this @DataPreprocessing.Transformation.Config node.   --- UPDATED (Dexter) 20190131

            Returns
            ------------------------------

            `DataPreprocessing.Transformation.Types`    - The enumeration for the instance class of this @DataPreprocessing.Transformation.Config node.
        '''
        return self._instanceClass
    
    @property
    def method(self) -> 'Enumeration':
        '''
            The enumeration for the transformation method type of this @DataPreprocessing.Transformation.Config node.   --- UPDATED (Dexter) 20190131

            Returns
            ------------------------------

            `Enumeration`   - The enumeration for the transformation method type of this @DataPreprocessing.Transformation.Confign node.
        '''
        return self._method
    
    def copy(self) -> 'DataPreprocessing.Transformation.Config':
        """
            Abstract method for copying a new object of this inastance.   --- UPDATED (Dexter) 20190326

            Returns
            ------------------------------

            `DataPreprocessing.Transformation.Config` - A copied object.
        """
        pass
    
    def _copyAttributesTo(self, newTransformation, *attrs):
        """
            Copy a list of attributes to a new object from this object.   --- UPDATED (Dexter) 20190326

            Parameters
            ------------------------------
            
            newTransformation   `DataPreprocessing.Transformation.Config`   - Another @DataPreprocessing.Transformation.Config object where the values of this object is copied to.

            attrs               `*str`  - One or multiple attribute name for those attributes to be copied.
        """
        for attr in attrs:
            setattr(newTransformation, attr, getattr(self, attr))

    def parseJSON(self, obj: Dict[str, Any]):
        '''
            Parse a previously saved object into this @DataPreprocessing.Transformation.Config object.   --- UPDATED (Dexter) 20200110

            Parameters
            ------------------------------

            obj `dict<str,*>` - The JSON object to be parsed.
        '''
        # All values will be parsed directly,except the class identification, which has been set in the constructor.
        for (k, v) in obj.items():
            if (k not in ["_instanceClass", "_method"]):
                setattr(self, k, v)

class _DataPreprocessingTransformationColumnsConfig(_DataPreprocessingTransformationConfig):
    '''
        Abstract class representing a transformation configuration on columns preprocessing for @Source.Table objects.   --- UPDATED (Dexter) 20190130
    '''
    def __init__(self, method: 'DataPreprocessing.Transformation.Columns.Types', colSel: str = "None:None"):
        '''
            Create a transformation configuration on columns preprocessing for @Source.Table objects.   --- UPDATED (Dexter) 20190315

            Parameters
            ------------------------------
            
            method  `DataPreprocessing.Transformation.Columns.Types`    - The method of data transformation, as defined in @DataPreprocessing.Transformation.Columns.Methods .
        '''
        super().__init__(DataPreprocessing.Transformation.Types.Columns, method)
        # `bool` - Whether full training data pre-extraction is required before any transformation of data.
        self._requirePreExtraction = False
        # `str` - An @IndexRange parsable string specifying the column selection to be applied for this transformation.
        self.colSel = colSel
        # `bool` - Whether the transformation has pre-extract necessary information before transformations. 
        self._cachedPrefetch = False

    def parseJSON(self, obj: Dict[str, Any]):
        '''
            Parse a previously saved object into this @DataPreprocessing.Transformation.Config object.   --- UPDATED (Dexter) 20190914

            Parameters
            ------------------------------

            obj `dict<str,*>` - The JSON object to be parsed.
        '''
        # All values will be parsed directly,except the class identification, which has been set in the constructor.
        for (k,v) in obj.items():
            if (k not in ["_instanceClass", "_method", "dppNodes", "colConfigs", "_cachedPrefetch"]):
                setattr(self, k, v)

    @property
    def requirePreExtraction(self) -> bool:
        '''
            Whether full training data pre-extraction is required before any transformation of data.   --- UPDATED (Dexter) 20190315

            Returns
            ------------------------------

            `bool`  - Whether full training data pre-extraction is required before any transformation of data.
        '''
        return self._requirePreExtraction

    @property
    def cachedPrefetch(self) -> bool:
        """
             Whether the transformation has pre-extract necessary information before transformations.    --- UPDATED (Dexter) 20190322

             Returns
             ------------------------------

             `bool` - Whether the transformation has pre-extract necessary information before transformations. 
        """
        return self._cachedPrefetch

    def preExtract(self, preFetchedColumns: 'np.ndarray'):
        """
            Abstract method to pre-extract the entire dataset for getting necessary information before transformation.   --- UPDATED (Dexter) 20190322

            Parameters
            ------------------------------

            preFetchedColumns   `np.ndarray`    - Array of prefetched incoming data of this data preprocessing node.
        """
        pass
    
    def transformTo(self, val: Union[int, float, str]) -> float:
        """
            Abstract method, to transform a value of original columns to a new value.   --- UPDATED (Dexter) 20190323

            Parameters
            ------------------------------

            val     `int|float|str`     - The original value of the selected columns.

            Returns
            ------------------------------

            `float`     - The transformed value.
        """
        pass
    
    def transformFrom(self, val: float) -> Union[int, float, str]:
        """
            Abstract method, to transform a value back to an original range of value, typically used in recovering from data model predictions.   --- UPDATED (Dexter) 20190323

            Parameters
            ------------------------------

            val     `float`         - The predicted value.

            Returns
            ------------------------------

            `int|float|str`         - The predicted value in the original range.
        """
        pass

class _DataPreprocessingTransformationColumnsNormalize(_DataPreprocessingTransformationColumnsConfig):
    '''
        Class representing a normalization configuration to be transformed on columns preprocessing for @Source.Table objects.   --- UPDATED (Dexter) 20190315
    '''
    class Methods(Enumeration):
        """
            Enumeration defining the handling method on feature scaling of data transformation.   --- UPDATED (Dexter) 20190320
        """
        # `int` - Normalize the data within the data range. 
        MinMax = 0
        # `int` - Normalize the data within the range [min(0, data range min), max(0, data range max)]. 
        RelativeToZero = 1
        # `int` - Remove records with missing data. 
        RelativeToMean = 2
        # `int` - Normalize the data within a specific data range.
        RelativeToRange = 3

    def __init__(self, colSel: str = "None:None", scaleType: 'DataPreprocessing.Transformation.Columns.Normalize.Methods' = Methods.MinMax, rangeMin: float = None, rangeMax: float = None, relValue: float = None):
        '''
            Create a normalization configuration on columns preprocessing.   --- UPDATED (Dexter) 20190406

            Parameters
            ------------------------------
            
            colSel      `str`   - An @IndexRange parsable string specifying the column selection to be applied for this transformation.

            scaleType   `bool`  - The handling method on feature scaling of data transformation.

            rangeMin    `float` - The max of the data range. `None` if it's to be determined by full extraction of training data.

            rangeMax    `float` - The min of the data range. `None` if it's to be determined by full extraction of training data.

            relValue      `float` - Relative point of the data range.
        '''
        super().__init__(DataPreprocessing.Transformation.Columns.Types.Normalize, colSel)
        # `bool` - Whether full training data pre-extraction is required before any transformation of data.
        self._requirePreExtraction = True
        # `DataPreprocessing.Transformation.Columns.Normalize.Methods` - The handling method on feature scaling of data transformation.
        self.scaleType = scaleType
        # `float` - The data range. 
        self.range = (rangeMax - rangeMin) if (rangeMin is not None and rangeMax is not None) else None
        # `float`- Relative point of the data range.
        self.relValue = relValue
    
    def parseJSON(self, obj: Dict[str, Any]):
        '''
            Parse a previously saved object into this @DataPreprocessing.Transformation.Columns.Normalize object.   --- UPDATED (Dexter) 20190922

            Parameters
            ------------------------------

            obj `dict<str,*>` - The JSON object to be parsed.
        '''
        # All values will be parsed directly,except the class identification, which has been set in the constructor.
        for (k, v) in obj.items():
            if (k == "scaleType"):
                setattr(self, k, DataPreprocessing.Transformation.Columns.Normalize.Methods.parse(v))
            elif (k not in ["_instanceClass", "_method", "dppNodes", "colConfigs"]):
                setattr(self, k, v)

    @property
    def requirePreExtraction(self) -> bool:
        '''
            Whether full training data pre-extraction is required before any transformation of data.   --- UPDATED (Dexter) 20190325

            Returns
            ------------------------------
            
            `bool`  - Whether full training data pre-extraction is required before any transformation of data.
        '''
        return False if self.scaleType == DataPreprocessing.Transformation.Columns.Normalize.Methods.RelativeToRange else True
    
    def preExtract(self, preFetchedColumns: 'np.ndarray'):
        '''
            Pre-extract the entire dataset for getting necessary information before transformation.   --- UPDATED (Dexter) 20190406

            Parameters
            ------------------------------

            `np.ndarray`    - Array of prefetched incoming data of this data preprocessing node.
        '''
        # The mins and maxs are of arrays.
        selColData = preFetchedColumns[..., IndexRange.toIndexer(self.colSel)]

        # Save the min/max of the dataset.
        maxV = np.max(selColData)
        minV = np.min(selColData)

        # Cache the actual min / max for normalization.
        if (self.scaleType == DataPreprocessing.Transformation.Columns.Normalize.Methods.MinMax):
            # Determine the normalization scale.
            self.range = maxV - minV
            self.relValue = minV
        elif (self.scaleType == DataPreprocessing.Transformation.Columns.Normalize.Methods.RelativeToZero):
            # Determine the scale relative to zero.
            self.range = max([0, maxV])- min([0, minV])
            self.relValue = 0
        elif (self.scaleType == DataPreprocessing.Transformation.Columns.Normalize.Methods.RelativeToMean):
            # Determine the normalization scale.
            self.range = maxV - minV
            self.relValue = np.sum(selColData)/len(selColData)

        # Cached pre-fetch.
        self._cachedPrefetch = True
    
    def transformTo(self, val: float) -> float:
        """
            Transform a value of original columns to a new value.   --- UPDATED (Dexter) 20200411

            Parameters
            ------------------------------

            val     `float`     - The original value of the selected columns.

            Returns
            ------------------------------

            `float`     - The transformed value.
        """
        return (float(val) - self.relValue) / self.range
    
    def transformFrom(self, val: float) -> float:
        """
            Transform a value back to an original range of value, typically used in recovering from data model predictions.   --- UPDATED (Dexter) 20190323

            Parameters
            ------------------------------

            val     `float`     - The predicted value.

            Returns
            ------------------------------

            `float`     - The predicted value in the original range.
        """
        return val * self.range + self.relValue

class _DataPreprocessingTransformationColumnsLog(_DataPreprocessingTransformationColumnsConfig):
    '''
        Class representing a logarithm configuration to be transformed on columns preprocessing for @Source.Table objects.   --- UPDATED (Dexter) 20190315
    '''
    def __init__(self, colSel: str = "None:None", base: float = math.e):
        '''
            Create a logarithm configuration on columns preprocessing.   --- UPDATED (Dexter) 20190315

            Parameters
            ------------------------------
            
            base    `float`     - The logarithm base.
        '''
        super().__init__(DataPreprocessing.Transformation.Columns.Types.Log, colSel)
        # `float` - The logarithm base.
        self.base = base
    
    def transformTo(self, val: float) -> float:
        """
            Transform a value of original columns to a new value.   --- UPDATED (Dexter) 20200411

            Parameters
            ------------------------------

            val     `float`     - The original value of the selected columns.

            Returns
            ------------------------------

            `float`     - The transformed value.
        """
        return math.log(max([float(val), 0]), self.base)
    
    def transformFrom(self, val: float) -> float:
        """
            Transform a value back to an original range of value, typically used in recovering from data model predictions.   --- UPDATED (Dexter) 20190323

            Parameters
            ------------------------------

            val     `float`     - The predicted value.

            Returns
            ------------------------------

            `float`     - The predicted value in the original range.
        """
        return self.base ** val

class _DataPreprocessingTransformationColumnsExponential(_DataPreprocessingTransformationColumnsConfig):
    '''
        Class representing an exponential configuration to be transformed on columns preprocessing for @Source.Table objects.   --- UPDATED (Dexter) 20190315
    '''
    def __init__(self, colSel: str = "None:None"):
        '''
            Create an exponential configuration on columns preprocessing.   --- UPDATED (Dexter) 20190315
        '''
        super().__init__(DataPreprocessing.Transformation.Columns.Types.Exponential, colSel)

    def transformTo(self, val: float) -> float:
        """
            Transform a value of original columns to a new value.   --- UPDATED (Dexter) 20200411

            Parameters
            ------------------------------

            val     `float`     - The original value of the selected columns.

            Returns
            ------------------------------

            `float`     - The transformed value.
        """
        return math.e ** float(val)
    
    def transformFrom(self, val: float) -> float:
        """
            Transform a value back to an original range of value, typically used in recovering from data model predictions.   --- UPDATED (Dexter) 20190323

            Parameters
            ------------------------------

            val     `float`     - The predicted value.

            Returns
            ------------------------------

            `float`     - The predicted value in the original range.
        """
        return math.log(max([val, 0]))

class _DataPreprocessingTransformationColumnsPower(_DataPreprocessingTransformationColumnsConfig):
    '''
        Class representing an exponentiation configuration to be transformed on columns preprocessing for @Source.Table objects.   --- UPDATED (Dexter) 20190315
    '''
    def __init__(self, colSel: str = "None:None", exponent: float = 1):
        '''
            Create an exponentiation configuration on columns preprocessing.   --- UPDATED (Dexter) 20190315

            Parameters
            ------------------------------
            
            exponent    `float`    - The exponent to be taken.
        '''
        super().__init__(DataPreprocessing.Transformation.Columns.Types.Power, colSel)
        # `float`    - The exponent to be taken.
        self.exponent = exponent
    
    def transformTo(self, val: float) -> float:
        """
            Transform a value of original columns to a new value.   --- UPDATED (Dexter) 20200411

            Parameters
            ------------------------------

            val     `float`     - The original value of the selected columns.

            Returns
            ------------------------------

            `float`     - The transformed value.
        """
        return (float(val) ** self.exponent) if self.exponent % 1 == 0 else (max([val, 0]) ** self.exponent)
    
    def transformFrom(self, val: float) -> float:
        """
            Transform a value back to an original range of value, typically used in recovering from data model predictions.   --- UPDATED (Dexter) 20190323

            Parameters
            ------------------------------

            val     `float`     - The predicted value.

            Returns
            ------------------------------

            `float`     - The predicted value in the original range.
        """
        return max([val, 0]) ** (1/self.exponent)

class _DataPreprocessingTransformationColumnsClassify(_DataPreprocessingTransformationColumnsConfig):
    '''
        Class representing a classification configuration to be transformed on columns preprocessing for @Source.Table objects.   --- UPDATED (Dexter) 20190315
    '''
    def __init__(self, colSel: str = "None:None"):
        '''
            Create a classification configuration on columns preprocessing.   --- UPDATED (Dexter) 20190914
        '''
        super().__init__(DataPreprocessing.Transformation.Columns.Types.Classify, colSel)
        # `bool`    - Whether full training data pre-extraction is required before any transformation of data.
        self._requirePreExtraction = True
        # `dict<*, int>`    - A key-id map for transforming the data.
        self.valueList = {}
        # `dict<int, *>`    - A id-key map for transforming the data.
        self._valueListReversed = {}
    
    @property
    def valueListReversed(self):
        """
            Get the reversed value list.
        """
        return self._valueListReversed

    def preExtract(self, preFetchedColumns: 'np.ndarray'):
        '''
            Pre-extract the entire dataset for getting necessary information before transformation.   --- UPDATED (Dexter) 20190914

            Parameters
            ------------------------------

            `np.ndarray`    - Array of prefetched incoming data of this data preprocessing node.
        '''
        # The mins and maxs are of arrays.
        selColData = np.reshape(preFetchedColumns[..., IndexRange.toIndexer(self.colSel)], [-1])

        # Get the value list.
        self.valueList = Source.Table.convertCatToNumbers(selColData)["dict"]

        # Set the reversed value list.
        self._valueListReversed = {v: k for (k, v) in self.valueList.items()} 

        # Cached pre-fetch.
        self._cachedPrefetch = True
    
    def transformTo(self, val: float) -> float:
        """
            Transform a value of original columns to a new value.   --- UPDATED (Dexter) 20190325

            Parameters
            ------------------------------

            val     `any`     - The original value of the selected columns.

            Returns
            ------------------------------

            `float`     - The transformed value.
        """
        return self.valueList[val]

    def transformFrom(self, val: float) -> str:
        """
            Transform a value back to an original range of value, typically used in recovering from data model predictions.   --- UPDATED (Dexter) 20190914

            Parameters
            ------------------------------

            val     `float`         - The predicted value.

            Returns
            ------------------------------

            `str`         - The predicted value in the original range.
        """
        return self.valueListReversed[val]

class _DataPreprocessingTransformationColumnsMissingData(_DataPreprocessingTransformationColumnsConfig):
    '''
        Class representing a missing data configuration to be transformed on columns preprocessing for @Source.Table objects.   --- UPDATED (Dexter) 20190315
    '''
    class Methods(Enumeration):
        '''
            Enumeration defining the handling method on missing data.   --- UPDATED (Dexter) 20190315
        '''
        # `int` - Takes the mean for numerical data; the mode value for string data.
        Auto = 0
        
        # `int` - Takes the mode value of the data.
        Mode = 1
        
        # `int` - Remove records with missing data.   --- RESERVED
        Delete = 2

    def __init__(self, handleType: 'DataPreprocessing.Transformation.Columns.MissingData.Methods'):
        '''
            Create a missing data configuration on columns preprocessing.   --- UPDATED (Dexter) 20190315

            Parameters
            ------------------------------
            
            handleType  `DataPreprocessing.Transformation.Columns.MissingData.Methods` - The handling method of missing data, as defined in @DataPreprocessing.Transformation.Columns.MissingData.Methods .
        '''
        super().__init__(DataPreprocessing.Transformation.Columns.Types.MissingData, True)
        # `bool`    - Whether full training data pre-extraction is required before any transformation of data.
        self._requirePreExtraction = True
        # `DataPreprocessing.Transformation.Columns.MissingData.Methods` - The handling method of missing data, as defined in @DataPreprocessing.Transformation.Columns.MissingData.Methods .
        self._handleType = handleType
    
    def parseJSON(self, obj: Dict[str, Any]):
        '''
            Parse a previously saved object into this @DataPreprocessing.Transformation.Columns.MissingData object.   --- UPDATED (Dexter) 20190922

            Parameters
            ------------------------------

            obj `dict<str,*>` - The JSON object to be parsed.
        '''
        # All values will be parsed directly,except the class identification, which has been set in the constructor.
        for (k, v) in obj.items():
            if (k == "_handleType"):
                setattr(self, k, DataPreprocessing.Transformation.Columns.MissingData.Methods.parse(v))
            elif (k not in ["_instanceClass", "_method", "dppNodes", "colConfigs"]):
                setattr(self, k, v)

    @property
    def handleType(self) -> 'DataPreprocessing.Transformation.Columns.MissingData.Methods':
        '''
            The handling method of missing data, as defined in @DataPreprocessing.Transformation.Columns.MissingData.Methods .   --- UPDATED (Dexter) 20190315

            Returns
            ------------------------------
            
            `DataPreprocessing.Transformation.Columns.MissingData.Methods` - The handling method of missing data.
        '''
        return self._handleType

    @handleType.setter
    def handleType(self, value: 'DataPreprocessing.Transformation.Columns.MissingData.Methods'):
        '''
            Set the handling method of missing data, as defined in @DataPreprocessing.Transformation.Columns.MissingData.Methods .   --- UPDATED (Dexter) 20190315

            Parameters
            ------------------------------
            
            `DataPreprocessing.Transformation.Columns.MissingData.Methods` - The handling method of missing data.
        '''
        self._requirePreExtraction = False if (value == DataPreprocessing.Transformation.Columns.MissingData.Methods.Delete) else True
        self._handleType = value
    
    def preExtract(self, preFetchedColumns: 'np.ndarray'):
        '''
            Pre-extract the entire dataset for getting necessary information before transformation.   --- UPDATED (Dexter) 20190917

            Parameters
            ------------------------------

            `np.ndarray`    - Array of prefetched incoming data of this data preprocessing node.
        '''
        # The mins and maxs are of arrays.
        selColData = np.reshape(preFetchedColumns[..., IndexRange.toIndexer(self.colSel)], [-1])

        # Determine what value to fill with.
        if (self.handleType == DataPreprocessing.Transformation.Columns.MissingData.Methods.Auto):
            if (np.issubdtype(selColData.dtype, np.integer) or np.issubdtype(selColData.dtype, np.float)):
                self.missingValue = selColData.reduce(lambda a,b: a+b) / len(selColData)
            else:
                self.missingValue = stats.mode(selColData, axis=None)[0][0]
        elif (self.handleType == DataPreprocessing.Transformation.Columns.MissingData.Methods.Mode):
            self.missingValue = stats.mode(selColData, axis=None)[0][0]

        # Cached pre-fetch.
        self._cachedPrefetch = True
    
    def transformTo(self, val: float) -> float:
        """
            Transform a value of original columns to a new value.   --- UPDATED (Dexter) 20190325

            Parameters
            ------------------------------

            val     `float`     - The original value of the selected columns.

            Returns
            ------------------------------

            `float`     - The transformed value.
        """
        return self.missingValue if (val is None or str(val).strip() == "") else val

class _DataPreprocessingTransformationImageConfig(_DataPreprocessingTransformationConfig):
    '''
        Abstract class representing a transformation configuration on image preprocessing for @Source.Image objects.   --- UPDATED (Dexter) 20190328
    '''
    def __init__(self, method: 'DataPreprocessing.Transformation.Image.Types'):
        """
            Create a configruation for image augmentation in columns preprocessing for @Source.Image objects.   --- UPDATED (Dexter) 20190328

            Parameters
            ------------------------------

            method  `DataPreprocessing.Transformation.ImageAugmentation.Types`  - The method type of transformation, as defined in @DataPreprocessing.Transformation.Image.Types .
        """
        super().__init__(DataPreprocessing.Transformation.Types.Image, method)

class _DataPreprocessingTransformationImageNormalize(_DataPreprocessingTransformationImageConfig):
    '''
        Sub-class representing an image transformation configuration to normalize iamges from [0,255] into [0,1].   --- UPDATED (Dexter) 20190328
    '''
    def __init__(self):
        '''
            Create an image transformation configuration to normalize iamges from [0,255] into [0,1].   --- UPDATED (Dexter) 20190328
        '''
        super().__init__(DataPreprocessing.Transformation.Image.Types.Normalize)
    
    def copy(self) -> 'DataPreprocessing.Transformation.Image.Normalize':
        """
            Copy a new object of this inastance.   --- UPDATED (Dexter) 20190328

            Returns
            ------------------------------

            'DataPreprocessing.Transformation.Image.Normalize'   - The copied object.
        """
        newObj = DataPreprocessing.Transformation.Image.Normalize()
        self._copyAttributesTo(newObj)
        return newObj
    
    
    def transformTo(self, val: 'tf.Tensor') -> 'tf.Tensor':
        """
            Transform a value of original image to a new image.   --- UPDATED (Dexter) 20190922

            Parameters
            ------------------------------

            val     `tf.Tensor`     - The original image.

            Returns
            ------------------------------

            `tf.Tensor`     - The transformed image.
        """
        return val/255

    
    def transformFrom(self, val: 'np.ndarray') -> 'np.ndarray':
        """
            Transform an image back to the original image.   --- UPDATED (Dexter) 20190922

            Parameters
            ------------------------------

            val     `np.ndarray`     - The original image.

            Returns
            ------------------------------

            `np.ndarray`     - The transformed image.
        """
        return val * 255

class _DataPreprocessingTransformationImageResize(_DataPreprocessingTransformationImageConfig):
    '''
        Sub-class representing an image transformation configuration to resize iamges.   --- UPDATED (Dexter) 20190328
    '''
    class Methods(Enumeration):
        """
            Enumeration defining the resize method type of a @DataPreprocessing.Transformation.Image.Resize object.   --- UPDATED (Dexter) 20190328
        """
        # `int` - Resize with bilinear interpolation.
        Bilinear = 1
        # `int` - Resize with nearest neighbor interpolation.
        NearestNeighbor = 2
        # `int` - Resize with bicubic interpolation.
        Bicubic = 3
        # `int` - Resize with area interpolation.
        Area = 4

    def __init__(self, width: int = 32, height: int = 32, resizeMethod: 'DataPreprocessing.Transformation.Image.Resize.Methods' = Methods.Bilinear, preserveAspectRatio: bool = False):
        '''
            Create an image transformation configuration to resize iamges.   --- UPDATED (Dexter) 20190507

            Parameters
            ------------------------------

            width                   `int`    - The crop width in pixels.

            height                  `int`    - The crop height in pixels.

            resizeMethod            `DataPreprocessing.Transformation.Image.Resize.Methods`    - The resize algorithm, as specified in @DataPreprocessing.Transformation.Image.Resize.Methods .

            preserveAspectRatio     `bool`   - Whether to keep target size as the original aspect ratio.
        '''
        super().__init__(DataPreprocessing.Transformation.Image.Types.Resize)
        # `int` - The crop width in pixels.
        self.width = width
        # `int` - The crop height in pixels.
        self.height = height
        # `DataPreprocessing.Transformation.Image.Resize.Methods` - The resize algorithm, as specified in @DataPreprocessing.Transformation.Image.Resize.Methods .
        self.resizeMethod = resizeMethod
        # `bool` -  Whether to keep target size as the original aspect ratio.
        self.preserveAspectRatio = preserveAspectRatio

    def parseJSON(self, obj: Dict[str, Any]):
        '''
            Parse a previously saved object into this @DataPreprocessing.Transformation.Image.Resize object.   --- UPDATED (Dexter) 20190922

            Parameters
            ------------------------------

            obj `dict<str,*>` - The JSON object to be parsed.
        '''
        # All values will be parsed directly,except the class identification, which has been set in the constructor.
        for (k, v) in obj.items():
            if (k == "resizeMethod"):
                setattr(self, k, DataPreprocessing.Transformation.Image.Resize.Methods.parse(v))
            elif (k not in ["_instanceClass", "_method", "dppNodes", "colConfigs"]):
                setattr(self, k, v)

    def copy(self) -> 'DataPreprocessing.Transformation.Image.Resize':
        """
            Copy a new object of this inastance.   --- UPDATED (Dexter) 20190328

            Returns
            ------------------------------

            'DataPreprocessing.Transformation.Image.Resize'   - The copied object.
        """
        newObj = DataPreprocessing.Transformation.Image.Resize()
        self._copyAttributesTo(newObj, "width", "height", "resizeMethod", "preserveAspectRatio")
        return newObj
    
    
    def transformTo(self, val: 'tf.Tensor') -> 'tf.Tensor':
        """
            Transform a value of original image to a new image.   --- UPDATED (Dexter) 20190507

            Parameters
            ------------------------------

            val     `tf.Tensor`     - The original image.

            Returns
            ------------------------------

            `tf.Tensor`     - The transformed image.
        """
        if (len(val.shape) == 3):
            # There is only one image. extend as array.
            val = tf.expand_dims(val, 0)
            return tf.image.resize(val, (self.height, self.width), method = self.resizeMethod, preserve_aspect_ratio = self.preserveAspectRatio)[0]
        elif (len(val.shape) == 4):
            return tf.image.resize(val, (self.height, self.width), method = self.resizeMethod, preserve_aspect_ratio = self.preserveAspectRatio)
        else:
            raise ValueError("Incorrect image transformation parameter.")
    
class _DataPreprocessingTransformationImageCentralCrop(_DataPreprocessingTransformationImageConfig):
    '''
        Sub-class representing an image transformation configuration to crop iamges from the central.   --- UPDATED (Dexter) 20190328
    '''
    def __init__(self, centralFraction: float = 0.9):
        '''
            Create an image transformation configuration to crop iamges from the central.   --- UPDATED (Dexter) 20190328

            Parameters
            ------------------------------

            centralFraction     `float`    - The crop width in pixels.
        '''
        super().__init__(DataPreprocessing.Transformation.Image.Types.CentralCrop)
        # `int` -  Whether to keep target size as the original aspect ratio.
        self.centralFraction = centralFraction
    
    def copy(self) -> 'DataPreprocessing.Transformation.Image.CentralCrop':
        """
            Copy a new object of this inastance.   --- UPDATED (Dexter) 20190328

            Returns
            ------------------------------

            'DataPreprocessing.Transformation.Image.CentralCrop'   - The copied object.
        """
        newObj = DataPreprocessing.Transformation.Image.CentralCrop()
        self._copyAttributesTo(newObj, "centralFraction")
        return newObj

    
    def transformTo(self, val: 'tf.Tensor') -> 'tf.Tensor':
        """
            Transform a value of original image to a new image.   --- UPDATED (Dexter) 20190507

            Parameters
            ------------------------------

            val     `tf.Tensor`     - The original image.

            Returns
            ------------------------------

            `tf.Tensor`     - The transformed image.
        """
        if (len(val.shape) == 3):
            # There is only one image.
            return tf.image.central_crop(val, self.centralFraction)
        elif len(val.shape) == 4:
            # There are multiple image.
            return tf.map_fn(lambda img: tf.image.central_crop(img, self.centralFraction), val)
        else:
            raise ValueError("Incorrect image transformation parameter.")

class _DataPreprocessingTransformationImageToGrayscale(_DataPreprocessingTransformationImageConfig):
    '''
        Sub-class representing an image transformation configuration to convert RGB image (3-color channels) into grayscale images (1 channel).   --- UPDATED (Dexter) 20190328
    '''
    def __init__(self, centralFraction: float = 0.9):
        '''
            Create an image transformation configuration to convert RGB image (3-color channels) into grayscale images (1 channel).   --- UPDATED (Dexter) 20190328

            Parameters
            ------------------------------

            centralFraction     `float`    - The crop width in pixels.
        '''
        super().__init__(DataPreprocessing.Transformation.Image.Types.ToGrayscale)
    
    def copy(self) -> 'DataPreprocessing.Transformation.Image.ToGrayscale':
        """
            Copy a new object of this inastance.   --- UPDATED (Dexter) 20190328

            Returns
            ------------------------------

            'DataPreprocessing.Transformation.Image.ToGrayscale'   - The copied object.
        """
        newObj = DataPreprocessing.Transformation.Image.ToGrayscale()
        self._copyAttributesTo(newObj)
        return newObj

    
    def transformTo(self, val: 'tf.Tensor') -> 'tf.Tensor':
        """
            Transform a value of original image to a new image.   --- UPDATED (Dexter) 20190507

            Parameters
            ------------------------------

            val     `tf.Tensor`     - The original image.

            Returns
            ------------------------------

            `tf.Tensor`     - The transformed image.
        """
        if (len(val.shape) == 3):
            # There is only one image.
            return tf.image.rgb_to_grayscale(val)
        elif len(val.shape) == 4:
            # There are multiple image.
            return tf.map_fn(lambda img: tf.image.rgb_to_grayscale(img), val)
        else:
            raise ValueError("Incorrect image transformation parameter.")

class _DataPreprocessingTransformationImageToRGB(_DataPreprocessingTransformationImageConfig):
    '''
        Sub-class representing an image transformation configuration to convert grayscale images (1 channel) into RGB image (3-color channels).   --- UPDATED (Dexter) 20190328
    '''
    def __init__(self, centralFraction: float = 0.9):
        '''
            Create an image transformation configuration to convert grayscale images (1 channel) into RGB image (3-color channels).   --- UPDATED (Dexter) 20190328

            Parameters
            ------------------------------

            centralFraction     `float`    - The crop width in pixels.
        '''
        super().__init__(DataPreprocessing.Transformation.Image.Types.ToRGB)
    
    def copy(self) -> 'DataPreprocessing.Transformation.Image.ToRGB':
        """
            Copy a new object of this inastance.   --- UPDATED (Dexter) 20190328

            Returns
            ------------------------------

            'DataPreprocessing.Transformation.Image.ToRGB'   - The copied object.
        """
        newObj = DataPreprocessing.Transformation.Image.ToRGB()
        self._copyAttributesTo(newObj)
        return newObj
    
    
    def transformTo(self, val: 'tf.Tensor') -> 'tf.Tensor':
        """
            Transform a value of original image to a new image.   --- UPDATED (Dexter) 20190507

            Parameters
            ------------------------------

            val     `tf.Tensor`     - The original image.

            Returns
            ------------------------------

            `tf.Tensor`     - The transformed image.
        """
        if (len(val.shape) == 3):
            # There is only one image.
            return tf.image.grayscale_to_rgb(val)
        elif len(val.shape) == 4:
            # There are multiple image.
            return tf.map_fn(lambda img: tf.image.grayscale_to_rgb(img), val)
        else:
            raise ValueError("Incorrect image transformation parameter.")

class _DataPreprocessingTransformationImageAugmentationConfig(_DataPreprocessingTransformationConfig):
    '''
        Abstract class representing an image transformation configuration on image preprocessing for @Source.Image objects.   --- UPDATED (Dexter) 20190131
    '''
    def __init__(self, method: 'DataPreprocessing.Transformation.ImageAugmentation.Types'):
        """
            Create a configruation for image augmentation in columns preprocessing for @Source.Image objects.   --- UPDATED (Dexter) 20190131

            Parameters
            ------------------------------

            method  `DataPreprocessing.Transformation.ImageAugmentation.Types`  - The method type of augmentation, as defined in @DataPreprocessing.Transformation.ImageAugmentation.Types .
        """
        super().__init__(DataPreprocessing.Transformation.Types.ImageAugmentation, method)

class _DataPreprocessingTransformationImageAugmentationCrop(_DataPreprocessingTransformationImageAugmentationConfig):
    '''
        Sub-class representing an image augmentation configuration for random cropping the size.   --- UPDATED (Dexter) 20190131
    '''
    class TestCropMethods(Enumeration):
        """
            Enumeration defining the test cropping method type of a @DataPreprocessing.Transformation.ImageAugmentation.Crop object.   --- UPDATED (Dexter) 20190326
        """
        # `int` - Resize and crop the image to if either original width/height is larger than the target width/height; pad with zeros if either original width/height is smaller than the target width/height.
        ResizePadOrCrop = 1
        # `int` - Resize the image to the target width/height, and pad with zeros if the aspect ratio is not the same.
        ResizePad = 2
        # `int`- Resize and change the aspect ratio to fit the target width/height.
        Resize = 3

    def __init__(self, width: int = None, height: int = None, testCrop: 'DataPreprocessing.Transformation.ImageAugmentation.Crop.TestCropMethods' = TestCropMethods.ResizePadOrCrop):
        '''
            Create an image augmentation configuration for random cropping the size.   --- UPDATED (Dexter) 20190326

            Parameters
            ------------------------------

            width       `int`       - The random crop width in pixels.

            height      `int`       - The random crop height in pixels.

            testCrop    `DataPreprocessing.Transformation.ImageAugmentation.Crop.TestCropMethods`   - The method of cropping applied in test images, as defined in the enumeration @DataPreprocessing.Transformation.ImageAugmentation.Crop.TestCropMethods .
        '''
        super().__init__(DataPreprocessing.Transformation.ImageAugmentation.Types.Crop)
        # `int` - The random crop width in pixels.
        self.width = width
        # `int` - The random crop height in pixels.
        self.height = height
        # `DataPreprocessing.Transformation.ImageAugmentation.Crop.TestCropMethods` - The method of cropping applied in test images, as defined in the enumeration @DataPreprocessing.Transformation.ImageAugmentation.Crop.TestCropMethods .
        self.testCrop = testCrop
    
    def parseJSON(self, obj: Dict[str, Any]):
        '''
            Parse a previously saved object into this @DataPreprocessing.Transformation.ImageAugmentation.Crop object.   --- UPDATED (Dexter) 20190922

            Parameters
            ------------------------------

            obj `dict<str,*>` - The JSON object to be parsed.
        '''
        # All values will be parsed directly,except the class identification, which has been set in the constructor.
        for (k, v) in obj.items():
            if (k == "testCrop"):
                setattr(self, k, DataPreprocessing.Transformation.ImageAugmentation.Crop.TestCropMethods.parse(v))
            elif (k not in ["_instanceClass", "_method", "dppNodes", "colConfigs"]):
                setattr(self, k, v)

    def copy(self) -> 'DataPreprocessing.Transformation.ImageAugmentation.Crop':
        """
            Copy a new object of this inastance.   --- UPDATED (Dexter) 20190326

            Returns
            ------------------------------

            'DataPreprocessing.Transformation.ImageAugmentation.Crop'   - The copied object.
        """
        newObj = DataPreprocessing.Transformation.ImageAugmentation.Crop()
        self._copyAttributesTo(newObj, "width", "height", "testCrop")
        return newObj
    
    def augmentTo(self, val: 'tf.Tensor') -> 'tf.Tensor':
        """
            Augment a value of original image to a new image.   --- UPDATED (Dexter) 20200322

            Parameters
            ------------------------------

            val     `tf.Tensor`     - The original image.

            Returns
            ------------------------------

            `tf.Tensor`     - The transformed image.
        """
        if (len(val.shape) == 3):
            # There is only one image.
            return tf.image.random_crop(val, (self.height, self.width, val.shape[-1]))
        elif len(val.shape) == 4:
            # There are multiple image.
            return tf.map_fn(lambda img: tf.image.random_crop(img, (self.height, self.width, val.shape[-1])), val)
        else:
            raise ValueError("Incorrect image transformation parameter.")

    
    def transformTo(self, val: 'tf.Tensor') -> 'tf.Tensor':
        """
            Transform a value of original image to a new image.   --- UPDATED (Dexter) 20190507

            Parameters
            ------------------------------

            val     `tf.Tensor`     - The original image.

            Returns
            ------------------------------

            `tf.Tensor`     - The transformed image.
        """
        if (len(val.shape) == 3):
            # There is only one image.
            if (self.testCrop == DataPreprocessing.Transformation.ImageAugmentation.Crop.TestCropMethods.ResizePadOrCrop):
                return tf.image.resize_with_crop_or_pad(val, self.height, self.width)
            elif (self.testCrop == DataPreprocessing.Transformation.ImageAugmentation.Crop.TestCropMethods.ResizePad):
                return tf.image.resize_with_pad(val, self.height, self.width)
            elif (self.testCrop == DataPreprocessing.Transformation.ImageAugmentation.Crop.TestCropMethods.Resize):
                return tf.image.resize(tf.expand_dims(val, 0), (self.height, self.width))[0]
            else:
                raise ValueError("Non-supported test crop method of this image transformation.")
        elif len(val.shape) == 4:
            # There are multiple image.
            if (self.testCrop == DataPreprocessing.Transformation.ImageAugmentation.Crop.TestCropMethods.ResizePadOrCrop):
                return tf.map_fn(lambda img: tf.image.resize_with_crop_or_pad(img, self.height, self.width), val)
            elif (self.testCrop == DataPreprocessing.Transformation.ImageAugmentation.Crop.TestCropMethods.ResizePad):
                return tf.map_fn(lambda img: tf.image.resize_with_pad(img, self.height, self.width), val)
            elif (self.testCrop == DataPreprocessing.Transformation.ImageAugmentation.Crop.TestCropMethods.Resize):
                return tf.image.resize(val, (self.height, self.width))
            else:
                raise ValueError("Non-supported test crop method of this image transformation.")
        else:
            raise ValueError("Incorrect image augmentation parameter.")

class _DataPreprocessingTransformationImageAugmentationFlip(_DataPreprocessingTransformationImageAugmentationConfig):
    '''
        Sub-class representing an image augmentation configuration for random flipping.   --- UPDATED (Dexter) 20190326
    '''
    class FlipMethods(Enumeration):
        """
            Enumeration defining the flipping method type of a @DataPreprocessing.Transformation.ImageAugmentation.Crop object.   --- UPDATED (Dexter) 20190326
        """
        # `int` - Flip the image left/right.
        LeftRight = 1
        # `int` - Flip the image up/down.
        UpDown = 2
        # `int`- Flip the image left/right/up/down.
        Any = 3

    def __init__(self, flipDirection: 'DataPreprocessing.Transformation.ImageAugmentation.Flip.FlipMethods' = FlipMethods.LeftRight):
        '''
            Create an image augmentation configuration for random flipping.   --- UPDATED (Dexter) 20190326

            Parameters
            ------------------------------

            flipDirection    `DataPreprocessing.Transformation.ImageAugmentation.Flip.FlipMethods`   - The direction of flipping applied in test images, as defined in the enumeration @DataPreprocessing.Transformation.ImageAugmentation.Flip.FlipMethods .
        '''
        super().__init__(DataPreprocessing.Transformation.ImageAugmentation.Types.Flip)
        # `DataPreprocessing.Transformation.ImageAugmentation.Flip.FlipMethods` - The method of cropping applied in test images, as defined in the enumeration @DataPreprocessing.Transformation.ImageAugmentation.Crop.TestCropMethods .
        self.flipDirection = flipDirection
    
    def parseJSON(self, obj: Dict[str, Any]):
        '''
            Parse a previously saved object into this @DataPreprocessing.Transformation.ImageAugmentation.Flip object.   --- UPDATED (Dexter) 20190922

            Parameters
            ------------------------------

            obj `dict<str,*>` - The JSON object to be parsed.
        '''
        # All values will be parsed directly,except the class identification, which has been set in the constructor.
        for (k, v) in obj.items():
            if (k == "flipDirection"):
                setattr(self, k, DataPreprocessing.Transformation.ImageAugmentation.Flip.FlipMethods.parse(v))
            elif (k not in ["_instanceClass", "_method", "dppNodes", "colConfigs"]):
                setattr(self, k, v)

    def copy(self) -> 'DataPreprocessing.Transformation.ImageAugmentation.Flip':
        """
            Copy a new object of this inastance.   --- UPDATED (Dexter) 20190326

            Returns
            ------------------------------

            'DataPreprocessing.Transformation.ImageAugmentation.Flip'   - The copied object.
        """
        newObj = DataPreprocessing.Transformation.ImageAugmentation.Flip()
        self._copyAttributesTo(newObj, "flipDirection")
        return newObj

    def augmentTo(self, val: 'tf.Tensor') -> 'tf.Tensor':
        """
            Augment a value of original image to a new image.   --- UPDATED (Dexter) 20191010

            Parameters
            ------------------------------

            val     `tf.Tensor`     - The original image.

            Returns
            ------------------------------

            `tf.Tensor`     - The transformed image.
        """
        # Create a random factor for whether to flip.
        rand: 'tf.Tensor' = tf.cast(tf.random.uniform([], minval=0, maxval=2, dtype=tf.int32), tf.float32)
        if (len(val.shape) == 3):
            # There is only one image.
            if (self.flipDirection == DataPreprocessing.Transformation.ImageAugmentation.Flip.FlipMethods.LeftRight):
                return rand * tf.reverse(val, [1]) + (1. - rand) * val
            elif (self.flipDirection == DataPreprocessing.Transformation.ImageAugmentation.Flip.FlipMethods.UpDown):
                return rand * tf.reverse(val, [0]) + (1. - rand) * val
            elif (self.flipDirection == DataPreprocessing.Transformation.ImageAugmentation.Flip.FlipMethods.Any):
                # Flip for one direction first, and get another random chance to flip for another direction.
                val = rand * tf.reverse(val, [1]) + (1. - rand) * val
                rand: 'tf.Tensor' = tf.cast(tf.random.uniform([], minval=0, maxval=2, dtype=tf.int32), tf.float32)
                return rand * tf.reverse(val, [0]) + (1. - rand) * val
            else:
                raise ValueError("Non-supported flip direction of this image transformation.")
        elif len(val.shape) == 4:
            # If there are multiple images, .
            if (self.flipDirection == DataPreprocessing.Transformation.ImageAugmentation.Flip.FlipMethods.LeftRight):
                return rand * tf.reverse(val, [2]) + (1. - rand) * val
            elif (self.flipDirection == DataPreprocessing.Transformation.ImageAugmentation.Flip.FlipMethods.UpDown):
                return rand * tf.reverse(val, [1]) + (1. - rand) * val
            elif (self.flipDirection == DataPreprocessing.Transformation.ImageAugmentation.Flip.FlipMethods.Any):
                val = rand * tf.reverse(val, [2]) + (1. - rand) * val
                rand = tf.cast(tf.random.uniform([], minval=0, maxval=2, dtype=tf.int32), tf.float32)
                return rand * tf.reverse(val, [1]) + (1. - rand) * val
            else:
                raise ValueError("Non-supported flip direction of this image transformation.")
        else:
            raise ValueError("Incorrect image augmentation parameter.")

class _DataPreprocessingTransformationImageAugmentationRotate(_DataPreprocessingTransformationImageAugmentationConfig):
    '''
        Sub-class representing an image augmentation configuration for random rotation.   --- RESERVED --- UPDATED (Dexter) 20190326
    '''
    def __init__(self, deviation: int = 10):
        '''
            Create an image augmentation configuration for random rotation.   --- RESERVED --- UPDATED (Dexter) 20190326

            Parameters
            ------------------------------

            deviation   `float` - The deviation of random rotation in degree.
        '''
        super().__init__(DataPreprocessing.Transformation.ImageAugmentation.Types.Rotate)
        # `float` - The deviation of random rotation in degree.
        self.deviation = deviation
    
    def copy(self) -> 'DataPreprocessing.Transformation.ImageAugmentation.Rotate':
        """
            Copy a new object of this inastance.   --- RESERVED --- UPDATED (Dexter) 20190326

            Returns
            ------------------------------

            'DataPreprocessing.Transformation.ImageAugmentation.Rotate'   - The copied object.
        """
        newObj = DataPreprocessing.Transformation.ImageAugmentation.Rotate()
        self._copyAttributesTo(newObj, "deviation")
        return newObj

    
    def augmentTo(self, val: 'tf.Tensor') -> 'tf.Tensor':
        """
            Augment a value of original image to a new image.   --- UPDATED (Dexter) 20190507

            Parameters
            ------------------------------

            val     `tf.Tensor`     - The original image.

            Returns
            ------------------------------

            `tf.Tensor`     - The transformed image.
        """
        raise ValueError("Random rotation augmentation is not supported yet.")

class _DataPreprocessingTransformationImageAugmentationHue(_DataPreprocessingTransformationImageAugmentationConfig):
    '''
        Sub-class representing an image augmentation configuration for random hue rotation.   --- UPDATED (Dexter) 20190326
    '''
    def __init__(self, maxDelta: int = 0.1):
        '''
            Create an image augmentation configuration for random hue rotation.   --- UPDATED (Dexter) 20190326

            Parameters
            ------------------------------

            maxDelta   `float` - The deviation of random hue rotation adjustment in the interval of [-maxDelta, maxDelta], where maxDelta should be in the interval of [0, 0.5].
        '''
        super().__init__(DataPreprocessing.Transformation.ImageAugmentation.Types.Hue)
        # `float` - The deviation of random hue rotation adjustment in the interval of [-maxDelta, maxDelta], where maxDelta should be in the interval of [0, 0.5].
        self.maxDelta = maxDelta
    
    def copy(self) -> 'DataPreprocessing.Transformation.ImageAugmentation.Hue':
        """
            Copy a new object of this inastance.   --- UPDATED (Dexter) 20190326

            Returns
            ------------------------------

            'DataPreprocessing.Transformation.ImageAugmentation.Hue'   - The copied object.
        """
        newObj = DataPreprocessing.Transformation.ImageAugmentation.Hue()
        self._copyAttributesTo(newObj, "maxDelta")
        return newObj

    
    def augmentTo(self, val: 'tf.Tensor') -> 'tf.Tensor':
        """
            Augment a value of original image to a new image.   --- UPDATED (Dexter) 20190507

            Parameters
            ------------------------------

            val     `tf.Tensor`     - The original image.

            Returns
            ------------------------------

            `tf.Tensor`     - The transformed image.
        """
        if (len(val.shape) == 3):
            # There is only one image.
            return tf.image.random_hue(val, self.maxDelta)
        elif len(val.shape) == 4:
            # There are multiple image.
            return tf.map_fn(lambda img: tf.image.random_hue(img, self.maxDelta), val)
        else:
            raise ValueError("Incorrect image augmentation parameter.")

class _DataPreprocessingTransformationImageAugmentationContrast(_DataPreprocessingTransformationImageAugmentationConfig):
    '''
        Sub-class representing an image augmentation configuration for random contrast adjustment.   --- UPDATED (Dexter) 20190326
    '''
    def __init__(self, lower: float = 0.9, upper: float = 1.1):
        '''
            Create an image augmentation configuration for random contrast adjustment.   --- UPDATED (Dexter) 20190326

            Parameters
            ------------------------------

            lower   `float` - The lowest value of random contrast adjustment scale.

            upper   `float` - The largest value of random contrast adjustment scale.
        '''
        super().__init__(DataPreprocessing.Transformation.ImageAugmentation.Types.Contrast)
        # `float` - The lowest value of random contrast adjustment scale.
        self.lower = lower
        # `float` - The largest value of random contrast adjustment scale.
        self.upper = upper
    
    def copy(self) -> 'DataPreprocessing.Transformation.ImageAugmentation.Contrast':
        """
            Copy a new object of this inastance.   --- UPDATED (Dexter) 20190326

            Returns
            ------------------------------

            'DataPreprocessing.Transformation.ImageAugmentation.Contrast'   - The copied object.
        """
        newObj = DataPreprocessing.Transformation.ImageAugmentation.Contrast()
        self._copyAttributesTo(newObj, "lower", "upper")
        return newObj

    
    def augmentTo(self, val: 'tf.Tensor') -> 'tf.Tensor':
        """
            Augment a value of original image to a new image.   --- UPDATED (Dexter) 20190507

            Parameters
            ------------------------------

            val     `tf.Tensor`     - The original image.

            Returns
            ------------------------------

            `tf.Tensor`     - The transformed image.
        """
        if (len(val.shape) == 3):
            # There is only one image.
            return tf.image.random_contrast(val, self.lower, self.upper)
        elif len(val.shape) == 4:
            # There are multiple image.
            return tf.map_fn(lambda img: tf.image.random_contrast(img, self.lower, self.upper), val)
        else:
            raise ValueError("Incorrect image augmentation parameter.")

class _DataPreprocessingTransformationImageAugmentationSaturation(_DataPreprocessingTransformationImageAugmentationConfig):
    '''
        Sub-class representing an image augmentation configuration for random saturation adjustment.   --- UPDATED (Dexter) 20190326
    '''
    def __init__(self, lower: float = 0.9, upper: float = 1.1):
        '''
            Create an image augmentation configuration for random saturation adjustment.   --- UPDATED (Dexter) 20190326

            Parameters
            ------------------------------

            lower   `float` - The lowest value of random saturation adjustment scale.

            upper   `float` - The largest value of random saturation adjustment scale.
        '''
        super().__init__(DataPreprocessing.Transformation.ImageAugmentation.Types.Saturation)
        # `float` - The lowest value of random saturation adjustment scale.
        self.lower = lower
        # `float` - The largest value of random saturation adjustment scale.
        self.upper = upper
    
    def copy(self) -> 'DataPreprocessing.Transformation.ImageAugmentation.Saturation':
        """
            Copy a new object of this inastance.   --- UPDATED (Dexter) 20190326

            Returns
            ------------------------------

            'DataPreprocessing.Transformation.ImageAugmentation.Saturation'   - The copied object.
        """
        newObj = DataPreprocessing.Transformation.ImageAugmentation.Saturation()
        self._copyAttributesTo(newObj, "lower", "upper")
        return newObj

    
    def augmentTo(self, val: 'tf.Tensor') -> 'tf.Tensor':
        """
            Augment a value of original image to a new image.   --- UPDATED (Dexter) 20190507

            Parameters
            ------------------------------

            val     `tf.Tensor`     - The original image.

            Returns
            ------------------------------

            `tf.Tensor`     - The transformed image.
        """
        if (len(val.shape) == 3):
            # There is only one image.
            return tf.image.random_saturation(val, self.lower, self.upper)
        elif len(val.shape) == 4:
            # There are multiple image.
            return tf.map_fn(lambda img: tf.image.random_saturation(img, self.lower, self.upper), val)
        else:
            raise ValueError("Incorrect image augmentation parameter.")

class _DataPreprocessingTransformationImageAugmentationBrightness(_DataPreprocessingTransformationImageAugmentationConfig):
    '''
        Sub-class representing an image augmentation configuration for random brightness adjustment.   --- UPDATED (Dexter) 20190326
    '''
    def __init__(self, maxDelta: float = 0.1):
        '''
            Create an image augmentation configuration for random brightness adjustment.   --- UPDATED (Dexter) 20190327

            Parameters
            ------------------------------

            maxDelta   `float` - The lowest value of random brightness adjustment scale.
        '''
        super().__init__(DataPreprocessing.Transformation.ImageAugmentation.Types.Brightness)
        # `float` - The maximum multiplication factor of random brightness adjustment in the interval of [-maxDelta, maxDelta).
        self.maxDelta = maxDelta
    
    def copy(self) -> 'DataPreprocessing.Transformation.ImageAugmentation.Brightness':
        """
            Copy a new object of this inastance.   --- UPDATED (Dexter) 20190327

            Returns
            ------------------------------

            'DataPreprocessing.Transformation.ImageAugmentation.Brightness'   - The copied object.
        """
        newObj = DataPreprocessing.Transformation.ImageAugmentation.Brightness()
        self._copyAttributesTo(newObj, "maxDelta")
        return newObj

    
    def augmentTo(self, val: 'tf.Tensor') -> 'tf.Tensor':
        """
            Augment a value of original image to a new image.   --- UPDATED (Dexter) 20190507

            Parameters
            ------------------------------

            val     `tf.Tensor`     - The original image.

            Returns
            ------------------------------

            `tf.Tensor`     - The transformed image.
        """
        if (len(val.shape) == 3):
            # There is only one image.
            return tf.image.random_brightness(val, self.maxDelta)
        elif len(val.shape) == 4:
            # There are multiple image.
            return tf.map_fn(lambda img: tf.image.random_brightness(img, self.maxDelta), val)
        else:
            raise ValueError("Incorrect image augmentation parameter.")

class _DataPreprocessingCircularConfig:
    """
        Class representing a circular definition.   --- UPDATED (Dexter) 20190404
    """
    def __init__(self, colSel: str = "None:None", minV = 0, maxV = 360):
        """
            Create a circular definition.   --- UPDATED (Dexter) 20190505
            
            Parameters
            ------------------------------

            colSel `str` - An @IndexRange parsable string specifying the column selection from the source.

            minV `float` - The minimum value of the circular range (inclusive).

            maxV `float` - The maximum value of the circular range (exclusive).
        """
        # `str` - An @IndexRange parsable string specifying the column selection from the source.\
        self.colSel = colSel
        # `float` - The minimum value of the circular range (inclusive).
        self.min = minV
        # `float` - The maximum value of the circular range (exclusive).
        self.max = maxV
    
    def parseJSON(self, obj: Dict[str, Any]):
        """
            Parse a previously saved object into this class of @DataPreprocessing.CircularConfig.   --- UPDATED (Dexter) 20190505

            Parameters
            ------------------------------

            obj `dict<str,*>` - A JSON object from Project file.
        """
        # Iterate the object keys and take actions on mapping back to the DataPreprocessing.CircularConfig. Class.
        for (k, v) in obj.items():
            if (k == "cols"):
                # Old circular info are having .cols attribute instead of .colSel
                setattr(self, "colSel", v)
            else:
                setattr(self, k, v)
    
    @staticmethod
    def createFromJSON(obj: Dict[str, Any]):
        """
            Parse a previously saved object into a @DataPreprocessing.CircularConfig object.   --- UPDATED (Dexter) 20190404 

            Parameters
            ------------------------------

            obj `dict<str,*>` - A JSON object from Project file.

            Returns
            ------------------------------

            `DataPreprocessing.CircularConfig` - The newly created @DataPreprocessing.CircularConfig object.
        """
        circularConfig = DataPreprocessing.CircularConfig()
        circularConfig.parseJSON(obj)
        return circularConfig

class _DataPreprocessingNodeStepEnum:
    '''
        Enumeration defining the step of data preprocessing within a @DataPreprocessing.Node.Config object.   --- UPDATED (Dexter) 20190129
    '''
    # The initial data received from previous root data or @DataPreprocessing.Node.Config object.
    Input = 0

    # Data just after transformations.
    Transformed = 1

    # The output of the data preproccessing.
    Output = 10

class DataPreprocessing:
    '''
        Module containing classes regarding data preprocessing.   --- UPDATED (Dexter) 20190201
    '''
    class Node:
        '''
            Module containing classes on node-based data preprocessing.   --- UPDATED 20180302
        '''
        class Types(Enumeration):
            '''
                Enumeration defining the instance / sub-class type of a @DataPreprocessing.Node.Config object.   --- UPDATED (CYK, Dexter) 20200308
            '''
            # Abstract class representing a data preprocessing node on a data source (Ref: @Source.Config object).
            Config = 0

            # Class representing a column configuration, i.e. preprocessing node of on a table-like source (Ref: @Source.Table object).
            Columns = 1

            # Class representing a image configuration, i.e. preprocessing node of on a image-like source (Ref: @Source.Image object).
            Image = 2

            # Abstract class of a source like data-preprocessing node config (Ref: @Source.SourceLike object).
            SourceLike = 100

            # Class representing a series configuration, i.e. preprocessing node of on a series-like source (Ref: @Source.Image object).
            TimeSeries = 101
       
            # Class representing a noise configuration, i.e. preprocessing node of on a noise-like source (Ref: @Source.Image object).
            Noise = 102
       
            # Class representing a noise configuration, i.e. preprocessing node of on a noise-like source (Ref: @Source.Image object).
            BERT = 103
         
        class GetDataMode(Enumeration):
            """
                Enumeration defining the method of getting data on a data preprocessing node.   --- UPDATED (Dexter) 20190522
            """
            # `int` - Get from previous nodes.
            Propagate = 0
            # `int` - Get from previous nodes but with specific item indexing options.
            Current = 1
            # `int` - Get from caches of this node.   --- RESERVED
            Cache = 2
        
        StepEnum = _DataPreprocessingNodeStepEnum
                
        @staticmethod
        def createFromJSON(obj: Dict[str, Any], train: 'Train') -> 'Node':
            ''' 
                Parse a previously saved object into a new @DataPreprocessing.Node.Config object. This will auto-determine the sub-class of the object, and pass the JSON object to the inner method to continue to parse.   --- UPDATED (Dexter) 20190508

                Parameters
                ------------------------------

                obj     `dict<str,*>`   - JSON object from Project file.

                train   `Train`         - The @Train object to attach to.

                Returns
                ------------------------------

                `DataPreprocessing.Node.Config`     - A @DataPreprocessing.Node.Config object.
            '''
            # Parse this DataPreprocessing.Node.Config object.
            dataPreprocessingNode = getattr(DataPreprocessing.Node,DataPreprocessing.Node.Types.getName(obj["_instanceClass"]))()
            dataPreprocessingNode.parseJSON(obj, train)
            return dataPreprocessingNode

        class Config:
            ''' 
                Class representing a data preprocessing node on a data source (@Source.Config object).   --- UPDATED (Dexter) 20190505
            '''
            def __init__(self, instanceClass: 'DataPreprocessing.Node.Types' = None, dtype: 'tf.DType' = None, sourceCol = "None:None",
                            source: Union[str,int,List[str]] = None):
                ''' 
                    Creates a column configuration.   --- UPDATED (Dexter) 20190506

                    Parameters
                    ------------------------------

                    instanceClass       `DataPreprocessing.Node.Types`   - The instnace class, as defined in @DataPreprocessing.Node.Types .

                    dtype               `tf.DType`  - The data type of the data output. If `null`, it will be automatically determined during data pre-processing, and converted to `tf.float32` if it's using as an input of data model.

                    sourceCol           `str`       - An @IndexRange parsable string specifying the column selection (data feature/color channel) to be selected from the source data preprocessing node.

                    source              `str|int`   - The source of this @DataPreprocessing.Node.Config. It's a `Number` if it's a @Source.Config object; a `String` if it's another @DataPreprocessing.Node.Config node.   --- DEPRECATED --- MAXVER 1904

                    defaultHeader       `str`       - A list of strings specifying the default header names of the data.   --- DEPRECATED --- MAXVER 1904
                '''
                # `DataPreprocessing.Node.Types` -The instnace class, as defined in @DataPreprocessing.Node.Types .
                self._instanceClass: DataPreprocessing.Node.Types = instanceClass
                # `str|int|list<str>` - The source of this @DataPreprocessing.Node.Config. It's a `Number` if it's a @Source.Config object; a `String` if it's another @DataPreprocessing.Node.Config node.
                self.source: List[str, int, List[str]] = None
                # `str` - An @IndexRange parsable string specifying the column selection (data feature/color channel) to be selected from the source data preprocessing node.
                self.sourceCol: str = sourceCol
                # `dict<int,int>` - An array of @IndexRange parsable string specifying the dimension selection (d = 1, 2, ... n-2 for an n dimension input) from the source data preprocessing node.
                self.crops: Dict[int,str] = []
                # `int` - The new epoch size if it's affected. If `None`, it's inherited from the parent node or source.
                self._epochSize: int = None
                # `DataPreprocessing.Node.GetDataMode` - The method for getting data from previous nodes or by itself.
                self._getDataMode: DataPreprocessing.Node.GetDataMode = DataPreprocessing.Node.GetDataMode.Propagate

                # `str` - The dataset key within the data source. Applicable only when appending on a multi-dataset data source.
                self.outputset: str = None
                # `tf.DType` - The data type of the data output.
                self.dtype: tf.DType = dtype
                # `int` - The topological order of this @DataPreprocessing.Node.Config in data preprocessing.
                self._order: int = 0
                # `list<int>` - The shape of each item of the data output, so the batch dimension is not included.
                self._shape: List[int] = None
                # `list<DataPreprocessing.Transformation.Config>` - An array of transformation info.
                self.transformations: List[DataPreprocessing.Transformation.Config] = []
                # `bool` - Whether to preprocess the data in batch. `False` if it's preprocessed using Dataset API in an element-wise manner. 
                self.preprocessInBatch: bool = True

                # `list<int>` - The receiving data shape from previous node.
                self._dataShape: List[int] = None
                # `Train`   - The @Train object where this @DataPreprocessing.Node.Config object is attached to.
                self._train: Train = None
                # `Source.Config|DataPreprocessing.Node.Config` - The previous @Source.Config or @DataPreprocessing.Node.Config this data preprocessing node is linked to.
                self._fromNode: Union[Source.Config,DataPreprocessing.Node.Config] = None
                # `str` - The column config key of this @DataPreprocessing.Node.Columns object.
                self._key: str = None

            def parseJSON(self, obj: Dict[str, Any], train: 'Train'):
                '''
                    Parse a previously saved object into this @DataPreprocessing.Node.Config object.   --- UPDATED (Dexter) 20200312

                    Parameters
                    ------------------------------

                    obj     `dict<str, *>`  - JSON object.

                    train   `Train`         - The train object this @DataPreprocessing.Node.Config object is attached in.
                '''
                # All values will be parsed directly except those of specific data structure.
                for k,v in obj.items():
                    if (k == "transformations"):
                        # If there is .instanceClass property, use DataPreprocessing.Transformation to parse; otherwise, it's a legacy normal object.
                        setattr(self, k, [DataPreprocessing.Transformation.createFromJSON(tx) for tx in v])
                    elif (k == "dtype"):
                        if (v is not None):
                            self.dtype = Source.Config.getDataType(v)
                        else:
                            self.dtype = None 
                    elif (k == "source"):
                        self.source = v
                        self._train = train
                        # It's a number if it's a source; a string if it's another data preprocessing node.
                        self._fromNode = train.sources[v] if isinstance(v, int) else [train.dppNodes[dppKey] for dppKey in v] if isinstance(v, list) else train.dppNodes[v]
                    elif (k == "_getDataMode"):
                        setattr(self, k, DataPreprocessing.Node.GetDataMode[v])
                    elif (k == "crops"):
                        self.crops = {idx: crop for idx, crop in enumerate(v) if crop is not None}
                    elif (k not in ["train", "fromNode", "_instanceClass"]):
                        setattr(self, k, v)

            @property
            def getDataMode(self) -> 'DataPreprocessing.Node.GetDataMode':
                """
                    The method for getting data from previous nodes or by itself.   --- UPDATED (Dexter) 20190522

                    Returns
                    ------------------------------

                    `DataPreprocessing.Node.GetDataMode`   - The method for getting data from previous nodes or by itself.
                """
                return self._getDataMode
                
            @property
            def hasHeader(self) -> bool:
                """
                    Return whether this table source include a heading.   --- UPDATED (Dexter) 20191002

                    Returns
                    ------------------------------

                    `bool`  - Whether the table has heading.
                """
                return all([(isinstance(s, Source.Image) or (isinstance(s, Source.Table) and s.hasHeader)) for s in self.getRootSources()])

            @property
            def order(self) -> int:
                '''
                    The topological order of this @DataPreprocessing.Node.Config data preprocessing.   --- UPDATED (Dexter) 20190201

                    Returns
                    ------------------------------
                    
                    `int`   - The topological order of this @DataPreprocessing.Node.Config data preprocessing.
                '''
                return self._order

            @property
            def train(self) -> 'Train':
                """
                    The @Train object where this @DataPreprocessing.Node.Config object is attached to.   --- UPDATED (Dexter) 20190404

                    Returns
                    ------------------------------

                    `Train`  - The @Train object where this @DataPreprocessing.Node.Config object is attached to.
                """
                return self._train

            @property
            def fromNode(self) -> Union['Source.Config', 'DataPreprocessing.Node.Config']:
                """
                    The previous @Source.Config or @DataPreprocessing.Node.Config this data preprocessing node is linked to.   --- UPDATED (Dexter) 20190404

                    Returns
                    ------------------------------

                    `Source.Config|DataPreprocessing.Node.Config`  - The previous @Source.Config or @DataPreprocessing.Node.Config this data preprocessing node is linked to. 
                """
                return self._fromNode

            @property
            def key(self) -> str:
                ''' 
                    The column config key of this @DataPreprocessing.Node.Config object.   --- UPDATED (Dexter) 20190201

                    Returns
                    ------------------------------

                    `str`  - The column config key of this @DataPreprocessing.Node.Config object.
                '''
                return self._key

            @property
            def dataShape(self) -> List[int]:
                ''' 
                    The receiving data shape from referencing @DataPreprocessing.Node.Config node or root source of @Source.Config object.   --- UPDATED (Dexter) 20190201

                    Returns
                    ------------------------------

                    `list<int>`  - The receiving data shape from referencing @DataPreprocessing.Node.Config node or root source of @Source.Config object.
                '''
                return self._dataShape
            
            @property
            def itemShape(self):
                ''' 
                    The output item shape of this @DataPreprocessing.Node.Config node.   --- UPDATED (Dexter) 20190201

                    Returns
                    ------------------------------

                    `list<int>`  - The output item shape of this @DataPreprocessing.Node.Config node.
                '''
                return self._shape

            @property
            def instanceClass(self):
                ''' 
                    The instance class of this @DataPreprocessing.Node.Config node, as defined in @DataPreprocessing.Node.Types.   --- UPDATED (Dexter) 20190201

                    Returns
                    ------------------------------

                    `str`  - The instance class of this @DataPreprocessing.Node.Config node, as defined in @DataPreprocessing.Node.Types.
                '''
                return self._instanceClass
            
            @property
            def epochSize(self) -> int:
                """
                    The new epoch size if it's affected. If `None`, it's inherited from the parent node or source.   --- UPDATED (Dexter) 20190505

                    Returns
                    ------------------------------

                    `int` - The new epoch size if it's affected. If `None`, it's inherited from the parent node or source.
                """
                return self._epochSize
            
            def getEpochSize(self, refresh: bool = False) -> int:
                """
                    Get the epoch size of this data preprocessing node using the current source dataset.   --- UPDATED (Dexter) 20200126

                    Parameters
                    ------------------------------

                    refresh `bool` - Whether to refresh cached epoch size.

                    Returns
                    ------------------------------

                    `int` - The epoch size of this data preprocessing node using the current source dataset.
                """
                # Check if there is epoch size in this dpp node.
                if (self.epochSize is not None and not refresh):
                    return self.epochSize

                # Otherwise, return the epoch size of root sources.
                else:
                    sources = self.getRootSources()
                    epochSizes = [s.epochSize for s in sources]
                    self._epochSize = [*epochSizes][0]
                    return self.epochSize
            
            def appendOn(self, source: Union['Source.Config', 'DataPreprocessing.Node.Config', List['DataPreprocessing.Node.Config']], key: str, outputset: str = None) -> 'DataPreprocessing.Node.Config':
                """
                    Virtual method to append this data preprocessing node on a @Source.Config or @DataPreprocessing.Node.Config.   --- UPDATED (Dexter) 20190506

                    Parameters
                    ------------------------------

                    source  `Source.Config | DataPreprocessing.Node.Config | list<DataPreprocessing.Node.Config>`     - The previous @Source.Config or @DataPreprocessing.Node.Config this data preprocessing node is linked to.

                    key     `str`       - The key for accessing this data preprocessing node.

                    outputset `str`       - The dataset key within the data source. Applicable only when appending on a multi-outputset data source.

                    Returns
                    ------------------------------

                    `DataPreprocessing.Node.Config`     - This object if it has been successfully appended.
                """
                # Ensure this is not connected yet.
                if (self._fromNode is not None):
                    raise ValueError("This data preprocessing node has been connected to anther item.")

                # Ensure the source is connected.
                if (isinstance(source, list)):
                    if (any([((not isinstance(s, DataPreprocessing.Node.Config)) or (s.train is None) or (s.train != source[0].train)) for s in source])):
                        raise ValueError("The item attaching to by this data preprocessing node is not included in any Train object.")
                elif (source.train is None):
                    raise ValueError("The item attaching to by this data preprocessing node is not included in any Train object.")
                
                # Reference to the train and source node.
                if (isinstance(source, Source.Config)):
                    self._train = source.train
                    self._fromNode = source
                    self.source = source.sourceID
                    self.outputset = outputset
                elif (isinstance(source, DataPreprocessing.Node.Config)):
                    self._train = source.train
                    self._fromNode = source
                    self.source = source.key
                elif (isinstance(source, list)):
                    self._train = source[0].train
                    self._fromNode = source
                    self.source = [s.key for s in source]
                else:
                    return None
                
                # Assign this data preprocessing node to the train object.
                source.train.dppNodes[key] = self
                self._key = key

                # Update the data shape and order.
                self.refreshDataShape()
                self.updateOrder()

                return self

            def getRootSources(self, rawGraph: bool = True) -> List[Union['Source.Config','DataPreprocess.Node.SourceLike']]:
                """
                    Get the root data source of this @DataPreprocessing.Node.Config node.   --- UPDATED (Dexter) 20190522

                    Paramters
                    ------------------------------

                    rawGraph   `bool`  - Whether to get root sources based on the raw graph connections.
                    
                    Returns
                    ------------------------------

                    `list<Source.Config|DataPreprocess.Node.SourceLike>` - The root data sources of this @DataPreprocessing.Node.Config node.
                """
                # Let all sources be a set.
                sources = set()

                # Determine whether to collect root sources from parent nodes.
                if not rawGraph and self.getDataMode != DataPreprocessing.Node.GetDataMode.Propagate:
                    return [self]

                # According to different types of this.fromNode, add the source into the set.
                if isinstance(self.fromNode, Source.Config):
                    sources.add(self.fromNode)
                elif isinstance(self.fromNode, DataPreprocessing.Node.Config):
                    for s in self.fromNode.getRootSources(rawGraph = rawGraph):
                        sources.add(s)
                elif isinstance(self.fromNode, list):
                    for n in self.fromNode:
                        for s in n.getRootSources(rawGraph = rawGraph):
                            sources.add(s)
                
                # Return the sources as an array.
                return [*sources]

            def getNextBatch(self, sourceDataset: 'DataGenerator.Dataset.Types' = None):
                '''
                    Virtual method for generating the next batch of data. No action taken, and a sub-class should be used.   --- UPDATED (Dexter) 20190522

                    Parameters
                    ------------------------------

                    sourceDataset   `DataGenerator.Dataset.Types`  - The type of dataset of a source that's this data is referencing.
                '''
                # Ensure the get data mode is not propagate.
                if (self.getDataMode == DataPreprocessing.Node.GetDataMode.Propagate):
                    raise ValueError("Data cannot be collected by data preprocessing node if its .getDataMode is Propagate.")
                else:
                    raise ValueError("This data preprocessing node should have .getNextBatch() method overwritten.")

            def copy(self, node: 'DataPreprocessing.Node.Config'):
                '''
                    Copy the configuration of this @DataPreprocessing.Node.Config object to another @DataPreprocessing.Node.Config object   --- UPDATED (Dexter) 20190408

                    Parameters
                    ------------------------------

                    node        `DataPreprocessing.Node.Config`    - Another @DataPreprocessing.Node.Config object.
                '''
                node.dtype = self.dtype
                node.transformations = [tx.copy() for tx in self.transformations]
                node.setInputShape(self.dataShape)
                node.source = self.source
                node.sourceCol = self.sourceCol
                node.updateOrder()
            
            def updateOrder(self):
                '''
                    Update the topological order of the data preprocessing node.   --- UPDATED (Dexter) 20190508
                '''
                if (isinstance(self.source, int) or self.source is None):
                    self._order = 0
                else:
                    self._order = self.train.dppNodes[self.source].order + 1

            def getShape(self, refresh: bool = True) -> List[int]:
                '''
                    Get the shape of the output of this @DataPreprocessing.Node.Config object, including the batch dimension.   --- UPDATED (Dexter) 20190130

                    Parameters
                    ------------------------------

                    refresh     `bool`      -Whether to refresh the shape from root data.

                    Returns
                    ------------------------------

                    `list<int>` - The shape of the output of this @DataPreprocessing.Node.Config object. This returns a new list instance instead a pointer to the original shape.
                '''
                # If refresh is needed, compute the new shapes from the source of this DataPreprocessing.Node.Config object.
                if (refresh):
                    self.refreshDataShape()
                
                # The batch dimension is always None for flexible training, conresponding to the placeholder Tensor in NeuralSimplycode
                if (self._shape is None):
                    self.refreshItemShape()
                
                return [None, *self._shape]
            
            def getInputShape(self, refresh: bool = False) -> List[int]:
                '''
                    Get the shape of the input of this @DataPreprocessing.Node.Config object, including the batch dimension.   --- UPDATED (Dexter) 20190130

                    Parameters
                    ------------------------------

                    refresh     `bool`      -Whether to refresh the shape from root data.

                    Returns
                    ------------------------------

                    `list<int>` - The shape of the input of this @DataPreprocessing.Node.Columns object. This returns a new list instance instead a pointer to the original shape.
                '''
                # If refresh is needed, compute the new shapes from the source of this DataPreprocessing.Node.Config object.
                if (refresh):
                    self.refreshDataShape()
                
                # The batch dimension is always None for flexible training, conresponding to the placeholder Tensor in NeuralSimplycode
                return [*self._dataShape]
            
            def setItemShape(self, shape: List[int]):
                '''
                    Set the shape of the output of this @DataPreprocessing.Node.Config object.   --- UPDATED (Dexter) 20190128

                    Parameters
                    ------------------------------

                    shape   `list<int>` -  The new item shape of the output of this @DataPreprocessing.Node.Config object.
                '''
                self._shape = [*shape]
            
            def setInputShape(self, shape: List[int]):
                '''
                    Set the input shape of the output of this @DataPreprocessing.Node.Config object.   --- UPDATED (Dexter) 20190130

                    Parameters
                    ------------------------------

                    shape   `list<int>`  - The new shape of the output of this @DataPreprocessing.Node.Config object.
                '''
                self._dataShape = [*shape]
                self.refreshItemShape()
            
            def refreshItemShape(self):
                '''
                    Refresh the item shape, typically after updates of datashape.   --- UPDATED (Dexter) 20190822
                '''
                # Get the shape after column selection.
                inputShape = self.getInputShape()[1:]
                selShape = [*inputShape[:-1], len(Source.Table.getColList(inputShape[-1], self.sourceCol))]

                # Set the item shape.
                self.setItemShape([*selShape])
            
            def refreshDataShape(self):
                '''
                    Refresh the shape of this @DataPreprocessing.Node.Config object, based on updates of previous preprocessing shapes.   --- UPDATED (Dexter) 20190506
                '''
                if (isinstance(self.fromNode, Source.Config)):
                    self.setInputShape(self.fromNode.getShape(self.outputset))

            def asType(self, dtype: 'tf.DType' = tf.float32):
                '''
                    Enforce a data type conversion when getting the output of this column config.   --- UPDATED (Dexter) 20190128

                    Parameters
                    ------------------------------

                    dtype       `tf.DType`          - The TensorFlow data type of this source column config.
                '''
                self.dtype = dtype
            
            def getHeader(self, step: 'DataPreprocessing.Node.Image.StepEnum' = _DataPreprocessingNodeStepEnum.Output) -> List[str]:
                """
                    Get the header names of this data preprocessing node.   --- UPDATED (Dexter) 20190522

                    Parameters
                    ------------------------------

                    step    `DataPreprocessing.Node.Image.StepEnum`   - The step of data preprocessing within a @DataPreprocessing.Node.Image object.

                    Returns
                    ------------------------------

                    `list<str>` - A list of header names.
                """
                # Get the source header.
                incomingHeader = self.fromNode.getHeader() if isinstance(self.fromNode, DataPreprocessing.Node.Config) else self.fromNode.getHeader(self.outputset)

                # Select header of specific columns.
                toHeader = Source.Table.arraySlice([incomingHeader], self.sourceCol)[0].tolist()

                # Returns if it needs to check through the data preprocessing.
                return toHeader

            def getData(self, step: 'DataPreprocessing.Node.StepEnum' = _DataPreprocessingNodeStepEnum.Output, sourceDataset: 'DataGenerator.Dataset.Types' = None, start: int = None, end: int = None):
                """
                    Virtual method to get the output data of this data preprocessing node, with specification of the range of the batch.   --- UPDATED (Dexter) 20200501

                    Parameters
                    ------------------------------

                    step        `DataPreprocessing.Node.StepEnum`   - The step of data preprocessing within a @DataPreprocessing.Node.Columns object.

                    sourceDataset   `DataGenerator.Dataset.Types`  - The type of dataset of a source that's this data is referencing.

                    start  `int`       - Batch starting index (inclusive). If `None`, it notates `0`.

                    end    `int`       - Batch ending index (exclusive). If `None`, it notes the column count.

                    Returns
                    ------------------------------

                    `np.ndarray`  - A data array after the requested processing step at this data preprocessing node.
                """
                # Get the incoming data.
                rootSources = Train.RootSources(self.train, self.getRootSources(False), sourceDataset = sourceDataset)

                # Ensure all of the sources are tables.
                rootData = rootSources.getData(start = start, end = end)

                # Return the transformed data.
                return self.getProcessedData(rootData, step = step)

            def getClassCount(self) -> int:
                """
                    Abstract method to get the class count of all values in a particular column config.   --- UPDATED (Dexter) 20190921 
                """
                pass

            def setTransform(self, transformationConfig: 'DataPreprocessing.Transformation.Config' = None):
                '''
                    Transform data in this @DataPreprocessing.Transformation.Config preprocessing node.   --- UPDATED (Dexter) 20190428

                    Parameters
                    ------------------------------

                    transformationConfig    `DataPreprocessing.Transformation.Config` - A transformation configuration object.
                '''
                # Ensure there is given a transformation config.
                if (transformationConfig is None):
                    raise ValueError("No Transforamtion Config is given")

                # If there needs pre-extraction, the transformation cannot follow another transformation in the same data preprocessing node.
                if (isinstance(transformationConfig, DataPreprocessing.Transformation.Columns.Config) and transformationConfig.requirePreExtraction):
                    # Get the shape of the incoming data preprocessing node.
                    shape = self.getInputShape()

                    # Get the shape after column selection.
                    shape = [*shape[0:-1], len(IndexRange.parse(shape[-1], self.sourceCol))]

                    # Get the last dimension.
                    lastDim = shape[-1]

                    # Get the columns to be transformed.
                    toTxCol = IndexRange.parse(lastDim, transformationConfig.colSel)

                    # Get the column index list of each existing transformations.
                    for tx in self.transformations:
                        colIdx = IndexRange.parse(lastDim, tx.colSel)

                        # Enusre columns within the same data pre-processing node to be applied to the column once only for transformations requiring pre-extractions.
                        if any([(idx in colIdx) for idx in toTxCol]):
                            raise ValueError("Transformation reapplied error.")

                # Push the transformation configuration into this object.
                self.transformations.append(transformationConfig)
            
            def clearTransform(self):
                """
                    Clear all transform data in this @DataPreprocessing.Transformation.Config preprocessing node.   --- UPDATED (Dexter) 20190921
                """
                self.transformations = []

            def _processData(self, data: 'np.ndarray', step: 'DataPreprocessing.Node.StepEnum' = _DataPreprocessingNodeStepEnum.Output) -> 'np.ndarray':
                '''
                    Abstract method of data pre-processing given data generated from @Source.Config objects with output at this data preprocessing node.   --- UPDATED (Dexter) 20190508

                    Parameters
                    ------------------------------

                    data    `np.ndarray`                   - The source data to get pre-processed from the root @Source.Config objects.

                    step    `DataPreprocessing.Node.StepEnum`   - The step of data preprocessing within a @DataPreprocessing.Node.Config object.

                    Returns
                    ------------------------------

                    `np.ndarray`     - The preprocesssed data of this pre-processing node at the requested step.
                '''
                pass
            
            def processData(self, data: Union['np.ndarray','tf.Tensor'], step: 'DataPreprocessing.Node.StepEnum' = _DataPreprocessingNodeStepEnum.Output) -> Union['np.ndarray','tf.Tensor']:
                '''
                    Virtual method to process given data generated from @Source.Config objects with output at this data preprocessing node. Data can be given with an np.ndarray or tf.Tensor types.   --- UPDATED (Dexter) 20190315

                    Parameters
                    ------------------------------

                    data    `np.ndarray|tf.Tensor`              - The source data to get pre-processed from the root @Source.Config objects.

                    step    `DataPreprocessing.Node.StepEnum`   - The step of data preprocessing within a @DataPreprocessing.Node.Config object.

                    Returns
                    ------------------------------

                    `np.ndarray|tf.Tensor`     - The preprocesssed data of this pre-processing node at the requested step.
                '''
                if (isinstance(data, np.ndarray)):
                    return self._processData(data, step)
                elif (isinstance(data, tf.Tensor)):
                    return self.processDataTensor(data, step)
                else:
                    raise ValueError("Data type not supported.")

            def processDataNPArray(self, data: 'np.ndarray', step: 'DataPreprocessing.Node.StepEnum' = _DataPreprocessingNodeStepEnum.Output) -> 'np.ndarray':
                '''
                    Virtual method to process given data generated from @Source.Config objects with output at this data preprocessing node. Data should be an np.ndarray object.   --- UPDATED (Dexter) 20190312

                    Parameters
                    ------------------------------

                    data    `np.ndarray`                                 - The source data to get pre-processed from the root @Source.Config objects.

                    step    `DataPreprocessing.Node.StepEnum`   - The step of data preprocessing within a @DataPreprocessing.Node.Config object.

                    Returns
                    ------------------------------

                    `np.ndarray`     - The preprocesssed data of this pre-processing node at the requested step.
                '''
                return self._processData(data, step)

            def processDataTensor(self, data: 'tf.Tensor', step: 'DataPreprocessing.Node.StepEnum' = _DataPreprocessingNodeStepEnum.Output) -> 'tf.Tensor':
                '''
                    Virtual method to process given data generated from @Source.Config objects with output at this data preprocessing node. Data should be an tf.Tensor object.   --- UPDATED (Dexter) 20190312

                    Parameters
                    ------------------------------

                    data    `tf.Tensor`                                 - The source data to get pre-processed from the root @Source.Config objects.

                    step    `DataPreprocessing.Node.StepEnum`   - The step of data preprocessing within a @DataPreprocessing.Node.Config object.

                    Returns
                    ------------------------------

                    `tf.Tensor`     - The preprocesssed data of this pre-processing node at the requested step.
                '''
                return tf.convert_to_tensor(self._processData(np.array(data), step))

            def getProcessedData(self, rootData: 'Train.RootData', step: 'DataPreprocessing.Node.StepEnum' = _DataPreprocessingNodeStepEnum.Output) -> 'np.ndarray':
                """
                    Virtual method to get the processed data based on a given root data.   --- UPDATED (Dexter) 20190922

                    Parameters
                    ------------------------------

                    rootData    `Train.RootData`    - Root data of the @Train object.
                """
                # Return the cached data if needed.
                if (self.key in rootData.dppNodes):
                    if (step != DataPreprocessing.Node.StepEnum.Output):
                        raise ValueError("When data is preprocessed elementally, it cannot be received using intercepted steps.")
                    return rootData.dppNodes[self.key]
                
                # Get the incoming data.
                incomingData = (rootData.sources[self.source] if self.outputset is None else rootData.sources[self.source][self.outputset]) if isinstance(self.source, int) else self.train.dppNodes[self.source].getProcessedData(rootData)

                # Process the data.
                return self.processData(incomingData, step = step)

            def recoverToRawData(self, items: 'np.ndarray|list[*+]', selCols: str = "None:None") -> 'np.ndarray':
                '''
                    Abstract method for recovering data items to raw-data format of the predicted data, like undo normalization or transformations, etc. No action taken, and a sub-class should be used.   --- UPDATED (Dexter) 20190510

                    Parameters
                    ------------------------------

                    items       `np.ndarray|list[*+]`   - A list of data items.
                     
                    selCols     `str`                   - The represented selected features (feature columns/channels) of the given items from the original output column of this column config.

                    Returns
                    ------------------------------

                    `np.ndarray`   - Recovered data items.
                '''
                pass

            def getTensor(self) -> 'tf.Tensor':
                '''
                    Get the source placeholder tensors of this training data source.   --- UPDATED (Dexter) 20190508

                    Returns
                    ------------------------------

                    `tf.Tensor`     - A TensorFlow placeholder tensor object with available source outputs.
                '''
                return tf.compat.v1.placeholder(tf.float32 if self.dtype is None else self.dtype, shape=([None, *self.getShape()[1:]]))
            
        class Columns(Config):
            '''
                    Class representing a data column configuration, handling data tables with column-based features.   --- UPDATED (Dexter) 20180622
            '''
            class StepEnum(_DataPreprocessingNodeStepEnum):
                '''
                    Enumeration defining the step of data preprocessing within a @DataPreprocessing.Node.Config object.   --- UPDATED (Dexter) 20190129
                '''
                # Data just after circular data definitions.
                CircularDataDefined = 2
            
            def __init__(self, sourceCol: str = "None:None", dtype: 'tf.DType' = None, source: str = None, order: int = 0):
                '''
                    Create a configuration of a data column.   --- UPDATED (Dexter) 20190522

                    Parameters
                    ------------------------------

                    sourceCol   `str`               - An @IndexRange parsable string specifying the column selection (data feature) to be selected from the source data preprocessing node.

                    dtype       `tf.DType`          - The TensorFlow data type of this source column config. If `None`, it will be automatically determined during data pre-processing, and converted to `tf.float32` if it's using as an input of data model.

                    source      `str`               - The source column config of where this config comes from.   --- DEPRECATED --- MAXVER 1904

                    order       `int`               - Feedforward (Topological) order of aligning all @ConConfig in a data source.   --- DEPRECATED --- MAXVER 1904
                '''
                super().__init__(instanceClass = DataPreprocessing.Node.Types.Columns, dtype = dtype, sourceCol = sourceCol)
                # `dict<int,set<str>>` - The column selections on which columns to convert to one hot encoding.
                self.oneHotColumns: Dict[int, Set[str]] = {}
                # `list<DataPreprocessing.Transformation.Columns.Config>` - The transformation definition information in action order.
                self.transformations: List[DataPreprocessing.Transformation.Columns.Config] = []
                # `list<DataPreprocessing.CircularConfig>` - The cicular definition information in action order.
                self.circular: List[DataPreprocessing.CircularConfig] = []
                # `dict<int,int>` - A map from column index to circular config.
                self._colToCircular: Dict[int, int] = {}
                # `bool` - Whether to preprocess the data in batch. `False` if it's preprocessed using Dataset API in an element-wise manner.   --- RESERVED
                self.preprocessInBatch: bool = True
                # `int` - The class count of all values in this column data preprocessing node.
                self._classCount: int = None

            def parseJSON(self, obj: Dict[str, Any], train: 'Train'):
                '''
                    Parse a previously saved object into this @DataPreprocessing.Node.Config object.   --- UPDATED (Dexter) 20200312

                    Parameters
                    ------------------------------

                    obj     `dict{str, *}`      - @DataPreprocessing.Node.Config -like object in NOM object

                    train   `Train`             - The train object this @DataPreprocessing.Node.Config object is attached in.
                '''
                for k,v in obj.items():
                    if (k == "source"):
                        setattr(self, k, v)
                        # It's a number if it's a source; a string if it's another data preprocessing node.
                        self._train = train
                        self._fromNode = train.sources[v] if isinstance(v, int) else train.dppNodes[v]
                    elif (k == "oneHotColumns"):
                        # One Hot Columns is a Map object with index key and categorical value set.
                        for (oneHotIdx, oneHotList) in v:
                            self.oneHotColumns[oneHotIdx] = set(oneHotList)
                    elif (k == "transformations"):
                        # If there is .instanceClass property, use DataPreprocessing.Transformation to parse; otherwise, it's a legacy normal object.
                        setattr(self, k, [(DataPreprocessing.Transformation.createFromJSON(tx) if "_instanceClass" in tx else tx) for tx in v])
                    elif (k == "circular"):
                        # Create the circular config object.
                        setattr(self, k, [DataPreprocessing.CircularConfig.createFromJSON(ci) for ci in v])

                        # Create a cached map between column index and circular configs.
                        self.updateColToCircular()
                    elif (k == "dtype"):
                        # Get Python data type.
                        if (v is not None):
                            self.dtype = Source.Config.getDataType(v)
                    elif (k == "_getDataMode"):
                        setattr(self, k, DataPreprocessing.Node.GetDataMode[v])
                    elif (k == "crops"):
                        self.crops = {idx: crop for idx, crop in enumerate(v) if crop is not None}
                    elif (k not in ["train", "fromNode", "_instanceClass"]):
                        setattr(self, k, v)
            
            def copy(self, node: 'DataPreprocessing.Node.Columns') -> 'DataPreprocessing.Node.Columns':
                '''
                    Copy the configuration of this @DataPreprocessing.Node.Columns object to another @DataPreprocessing.Node.Columns object   --- UPDATED (Dexter) 20190522

                    Parameters
                    ------------------------------

                    node        `DataPreprocessing.Node.Columns`    - Another @DataPreprocessing.Node.Columns object.

                    Returns
                    ------------------------------

                    `DataPreprocessing.Node.Columns` - The new data preprocessing node.
                '''
                super().copy(node)
                node.oneHotColumns = {idx: set([*oneHotSet]) for idx,oneHotSet in self.oneHotColumns.items()}
                node.circular = [cir.copy() for cir in self.circular]
            
            @property
            def colToCircular(self) -> Dict[int, int]:
                """
                    Get the map from column index to circular config.   --- UPDATED (Dexter) 20190917

                    Returns
                    ------------------------------

                    `dict<int, int>` - The map from column index to circular config.
                """
                return self._colToCircular

            @property
            def hasData(self) -> bool:
                """
                    Determine if this @DataPreprocessing.Node.Columns object has data derivable.   --- UPDATED (Dexter) 20200105
                    
                    Returns
                    ------------------------------

                    `bool`  - Whether this @DataPreprocessing.Node.Columns object has data derivable.
                """
                for s in self.getRootSources():
                    if isinstance(s, Source.Table):
                        s: Source.Table
                        if not(s.hasData()):
                            return False
                    else:
                        return False
                return True

            def refreshItemShape(self):
                '''
                    Refresh the item shape, typically after updates of datashape.   --- UPDATED (Dexter) 20191013
                '''
                # Get the shape after column selection.
                inputShape = self.getInputShape()[1:]
                selShape = [*inputShape[:-1], len(Source.Table.getColList(inputShape[-1], self.sourceCol))]

                # Update the shape if there is onehot columns added.
                if len(self.oneHotColumns):
                    selShape[-1] += sum([max([1, len(oneHotSet)]) for oneHotSet in self.oneHotColumns.values()]) - len(self.oneHotColumns)
                
                # Set the item shape.
                self.setItemShape([*selShape])
            
            def setClassCount(self, classCount: int):
                """
                    Set or cache the class count of all values in this column data preprocessing node.   --- UPDATED (Dexter) 20190921

                    Parameters
                    ------------------------------

                    classCount `int` - The class count of all values in this column data preprocessing node.
                """
                self._classCount = classCount

            def getClassCount(self) -> int:
                """
                    Get the class count of all values in this column data preprocessing node.   --- UPDATED (Dexter) 20190921

                    Returns
                    ------------------------------

                    `int`    - The class count of all values in this column data preprocessing node.
                """
                # Return cached class count if needed.
                if (self._classCount is not None):
                    return self._classCount

                # Get the data.
                colData = self.getData(sourceDataset = DataGenerator.Dataset.Types.Train)

                # Ensure the data is in format of array and there are some data existed.
                if ((isinstance(colData, list) or isinstance(colData, np.ndarray)) and len(colData) > 0):
                    classCount = Source.Table.getDataClassCount(colData)
                    self.setClassCount(classCount)
                    return classCount
                else:
                    raise ValueError("There is error in getting a valid preprocessing data.")

            def setCircularOutput(self, cols: str = "None:None", minV: float = 0, maxV: float = 360):
                """
                    Define any circular data columns in a particular column config.   --- UPDATED (Dexter) 20190917

                    Parameters
                    ------------------------------

                    cols    `str`  - An @IndexRange parsable string for selecting the columns on the data preprocessing node.

                    minV    `float` - The minimum value of the circular range (inclusive).
                    
                    maxV    `float` - The maximum value of the circular range (exclusive).
                """
                # Get the column config and find out the column indexes that are actually referring to.
                selCols = Source.Table.getColList(self.getInputShape()[-1], self.sourceCol)
                toColList = Source.Table.getColList(len(selCols), cols)

                # Create a set to store previously defined column indexes.
                allPrevIdx = set()
                for cc in self.circular:
                    for idx in IndexRange.parse(len(selCols), cc.colSel):
                        allPrevIdx.add(idx)
                
                if (any([(idx in allPrevIdx) for idx in toColList])):
                    raise ValueError("There has been circular definition on some of the requested columns.")
                elif (not len(IndexRange.parse(len(selCols), cols))):
                    raise ValueError("here must be some columns selected.")
                else:
                    # Another basic object storing the parametric info will also be stored in an Array for easier access in frontend environment
                    self.circular.append(DataPreprocessing.CircularConfig(cols, minV, maxV))
            
            def updateColToCircular(self):
                """
                    Update the map between column index and circular info index.   --- UPDATED (Dexter) 20200104
                """
                # Get the column config and find out the column indexes that are actually referring to.
                selCols = Source.Table.getColList(self.getInputShape()[-1], self.sourceCol)

                # Create a set to store previously defined column indexes.
                allPrevIdx = dict()
                for (cidx, cc) in enumerate(self.circular):
                    for idx in IndexRange.parse(len(selCols), cc.colSel):
                        allPrevIdx[idx] = cidx
                self._colToCircular = allPrevIdx

            def setOneHot(self, cols: str = "None:None"):
                """
                    Define the specific columns on this @DataPreprocessing.Node.Columns object to have a one-hot encoding.   --- UDPATED (Dexter) 20190917

                    Parameters
                    ------------------------------

                    cols    `str`   - The array of columns for selecting the columns from this @DataPreprocessing.Node.Columns .
                """
                # Get the column config and find out the column indexes that are actually referring to.
                selCols = Source.Table.getColList(self.getInputShape()[-1], self.sourceCol)
                toColList = Source.Table.getColList(len(selCols), cols)

                # Clear the one hot columns.
                tempOldOneHot = self.oneHotColumns
                self.oneHotColumns = {}

                # Set up the one hot columns.
                oriData = self.getData(step = DataPreprocessing.Node.Columns.StepEnum.Transformed)
                dataSize = len(oriData)
                failIdxs = []

                # Loop each selected columns and get all possible values
                for idx in toColList:
                    toDict = set(oriData[:,idx])
                    if (len(toDict) > dataSize / 2):
                        failIdxs.append(idx)
                    self.oneHotColumns[idx] = toDict
                
                # Avoid too many indices.
                if (len(failIdxs)):
                    raise ValueError("The requested column(s) will incur too many categories. Please double-check if you have select the right columns with categorical values.")
                
                # Update the item shape.
                self.refreshItemShape()

            def getHeader(self, step = StepEnum.Output) -> List[str]:
                """
                    Get the header names of this data preprocessing columns node.   --- UPDATED (Dexter) 20190509

                    Parameters
                    ------------------------------

                    step    `DataPreprocessing.Node.Columns.StepEnum.Output`    - The step of data preprocessing within a @DataPreprocessing.Node.Columns object.
                """
                # Get the source header.
                incomingHeader = self.fromNode.getHeader() if isinstance(self.fromNode, DataPreprocessing.Node.Config) else self.fromNode.getHeader(self.outputset)

                # Select header of specific columns.
                toHeader = Source.Table.arraySlice([incomingHeader], self.sourceCol)[0].tolist()

                # Returns if it needs to check through the data preprocessing.
                if (step not in [DataPreprocessing.Node.Columns.StepEnum.Output]):
                    return toHeader
                
                # Update the header list based on additional columns from one-hot encoding.
                oneHotIdxs = self.oneHotColumns
                if (len(oneHotIdxs) > 0):
                    finalHeader = []
                    for idx, headerName in enumerate(toHeader):
                        if (idx in oneHotIdxs):
                            # If the particular index of header is within the specification in one-hot columns, expand the headers with a postfix.
                            for oneHotName in oneHotIdxs[idx]:
                                finalHeader.append(headerName + " (" + oneHotName + ") ")
                        else:
                            finalHeader.append(headerName)
                    return finalHeader
                else:
                    return toHeader

            def _processData(self, data: 'np.ndarray<int|float|str>', step: 'DataPreprocessing.Node.Columns.StepEnum' = StepEnum.Output) -> 'np.ndarray<int|float|str>':
                '''
                    Data pre-processing given data generated from @Source.Config objects with output at this data preprocessing node.   --- UPDATED (Dexter) 20200308

                    Parameters
                    ------------------------------

                    data    `np.ndarray<int|float|str>`           - The source data to get pre-processed from the root @Source.Config objects.

                    step    `DataPreprocessing.Node.Columns.StepEnum`   - The step of data preprocessing within a @DataPreprocessing.Node.Columns object.

                    Returns
                    ------------------------------

                    `np.ndarray<int|float|str>`             - The preprocesssed data of this pre-processing node at the requested step.
                '''
                # Select specific columns.
                selCol = Source.Table.arraySlice(data, self.sourceCol)
                shape: List[int] = np.shape(data)
                selCol: np.ndarray = data[(slice(None, None),
                            *((IndexRange.toIndexer(self.crops[idx]) if idx in self.crops else slice(None, None)) for idx, v in enumerate(shape[1:-1])),
                            IndexRange.toIndexer(self.sourceCol))]

                # Returns incoming data if the step request is input.
                if (step == DataPreprocessing.Node.Columns.StepEnum.Input):
                    return selCol

                # Prefetch columns.
                preFetchCols = None

                # If there is transformation, apply each transformation function sequentially on each of the applied column indexes.
                for tx in self.transformations:
                    # Check if there is a need to pre-fetchh columns of the entire dataset.
                    if (tx.requirePreExtraction and not tx.cachedPrefetch):
                        # If it's needed, check if the columns have been extracted. If not, get all the data.
                        if (preFetchCols is None):
                            preFetchCols = self.train.dppNodes[self.key].getData(step = DataPreprocessing.Node.Columns.StepEnum.Input, sourceDataset = DataGenerator.Dataset.Types.Train)
                        
                        # Prefetch necessary information. for this transformation.
                        tx.preExtract(preFetchCols)
                    
                    # Transform the data.
                    selCol[..., IndexRange.toIndexer(tx.colSel)] = np.vectorize(tx.transformTo)(selCol[..., IndexRange.toIndexer(tx.colSel)])
                
                if (step == DataPreprocessing.Node.Columns.StepEnum.Transformed):
                    return selCol

                # Perform circular data range transformations on specific columns.
                for cc in self.circular:
                    minV, maxV, rangeV = cc.min, cc.max, cc.range
                    cirSlicer = IndexRange.toIndexer(cc.colSel)
                    selCol[..., cirSlicer] = np.mod((selCol[..., cirSlicer].astype(float) - minV), rangeV)
                
                if (step == DataPreprocessing.Node.Columns.StepEnum.CircularDataDefined):
                    return selCol

                # Handle the one-hot columns data.
                if (len(self.oneHotColumns) > 0):
                    # List all columns, perform transformation if needed.
                    allCols = []

                    for idx in range(0, len(selCol[0])):
                        if (idx in self.oneHotColumns):
                            newCol = selCol[:,[idx]]
                            newCol = Source.Table.convertCatToNumbers(newCol, True, {v: idx for idx,v in enumerate(self.oneHotColumns[idx])})["data"]
                            allCols.extend(newCol)
                        else:
                            allCols.append(selCol[:,[idx]])
                    
                    # Concatenate all columns.
                    selCol = np.column_stack(allCols)

                # Cast the data type if needed.
                if (self.dtype is not None):
                    selCol = selCol.astype(self.dtype.as_numpy_dtype)
                else:
                    selCol = selCol.astype(float)

                # Return the transformed data.
                return selCol
            
            def recoverToRawData(self, items: 'np.ndarray|list[*+]', revealedIdxs: List[int] = None) -> 'np.ndarray':
                """
                    Recover data items to raw-data format of the predicted data, like undo normalization or transformations, etc. No action taken, and a sub-class should be used.   --- UPDATED (Dexter) 20191024

                    Parameters
                    ------------------------------

                    items       `np.ndarray|list[*+]`   - A list of data items.

                    revealedIdxs    `list<int>`         - The incoming indexes of items.

                    Returns
                    ------------------------------

                    `np.ndarray`   - Recovered data items.
                """
                # Convert to np array for the predicted data.
                items = np.array(items)

                # Get the items index with respect to the input index.
                if revealedIdxs is None:
                    revealedIdxs = [*range(0, items.shape[-1])]
                
                # Get the actual columns selected and displayed in items.
                relativeCols = []

                # Recover from one-hot encoding.
                if (len(self.oneHotColumns) > 0):
                    # Sort all one hot column keys.
                    oneHotKeys = [*self.oneHotColumns.keys()]
                    oneHotKeys.sort()

                    # List all columns, perform transformation if needed.
                    allCols = []
                    pushIdx = 0
                    oriIdx = 0

                    for idx in range(0, self.getShape()[-1]):
                        if (idx == pushIdx):
                            if (idx in oneHotKeys):
                                # Determine the original data.
                                catSet = self.oneHotColumns[idx]
                                catSize = len(catSet)
                                pushIdx += catSize
                                oriIdx += 1

                                if all([idx in revealedIdxs for i in range(idx, idx+catSize)]):
                                    # Ensure all the one-hot indices have been selected.
                                    catCols = items[...,idx:idx+catSize]
                                    catIdx = np.argmax(catCols, axis=-1)
                                    catDict = {i: v for i,v in enumerate(catSet)}
                                    allCols.append(np.reshape(np.frompyfunc(lambda i: catDict[i], 1, 1)(catIdx), [*catCols.shape[:-1], 1]))
                                    relativeCols.append(oriIdx - 1)
                                else:
                                    raise ValueError("Column selection not covered all one-hot indices cannot perform recovering data.")
                            else:
                                pushIdx += 1
                                oriIdx += 1
                                if (idx in revealedIdxs):
                                    allCols.append(items[:,[idx]])
                                    relativeCols.append(oriIdx - 1)

                    # Concatenate all columns.
                    items = np.column_stack(allCols)

                else:
                    # If there is no one hot columns, the relative column index is just revealed from the shape.
                    relativeCols = [*range(0, self.getShape()[-1])]

                # Get the selected columns.
                inputShape = self.getInputShape()
                nextRevealedIdx = IndexRange.parse(inputShape[-1], self.sourceCol)

                # Ensure the predicted data is within circular definition range.
                for cc in self.circular:
                    minV, maxV, rangeV = cc.min, cc.max, cc.range
                    cirSlicer = [relativeCols.index(i) for i in IndexRange.parse(len(nextRevealedIdx), cc.colSel) if i in relativeCols]
                    items[..., cirSlicer] = np.mod((items[..., cirSlicer].astype(float) - minV), rangeV)
                
                # Transform the data back to the original distribution.
                if len(self.transformations):
                    # If there is transformation, reverse each transformation function sequentially on each of the applied column indexes.
                    for tx in reversed(self.transformations):
                        # Transform the data.
                        txSlicer = [relativeCols.index(i) for i in IndexRange.parse(len(nextRevealedIdx), tx.colSel) if i in relativeCols]
                        recoveredCols = np.vectorize(tx.transformFrom)(items[..., txSlicer])

                        # Convert the data type.
                        if (items.dtype != recoveredCols.dtype):
                            if (len(txSlicer) != items.shape[-1]):
                                items = items.astype(np.dtype("O"))
                            else:
                                items = items.astype(recoveredCols.dtype)

                        # Replace the data.
                        items[..., txSlicer] = recoveredCols

                # Recursively get previously recovered data.
                return items if isinstance(self.fromNode, Source.Config) else self.fromNode.recoverToRawData(items, nextRevealedIdx)
            
            def getPrintableItems(self, items, recovered = True):
                '''
                    Print some items into an array, with some further formatting.   --- UPDATED (Dexter) 20190509

                    Parameters
                    ------------------------------

                    items   `list[*+]|np.array[*+]`  - A list of items typically with original input format.

                    recovered   `bool`  - Whether the data is recovered from transformation or not.

                    Returns
                    ------------------------------

                    `list[[str,*]+]`    - A list of items in a single data column with prefix data type column.
                '''
                # Recover the data to a before-transformation state.
                recoveredData = (escapeNaNNPAry(items).tolist() if isinstance(items, np.ndarray) else items) if recovered else self.recoverToRawData(items).tolist()

                # Check if every item is a list.
                isNDAry = isinstance(recoveredData[0], list)

                if (not isNDAry):
                    # If not, it's a single value.
                    dataType = "Value"
                elif len(recoveredData[0]) == 1:
                    # If it is, but there is only one element of each item, it's still a single value.
                    dataType = "Value"
                    recoveredData = [c[0] for c in recoveredData]
                else:
                    # Otherwise, it's a table.
                    dataType = "Table"
                
                # Return the recovered ddata as a pair of datatype and JSON-stringified object.
                return [[dataType, json.dumps(i)] for i in recoveredData]

        class Image(Config):
            '''
                Class representing a image preprocessing node, i.e. preprocessing node of on a image-like source (like @Source.Image object).   --- UPDATED (Dexter) 20190130
            '''
            class StepEnum(_DataPreprocessingNodeStepEnum):
                '''
                    Enumeration defining the step of data preprocessing within a @DataPreprocessing.Node.Config object.   --- UPDATED (Dexter) 20190129
                '''
                pass

            def __init__(self, dtype: 'tf.DType' = None, sourceCol: str= "None:None"):
                '''
                    Creates a image preprocessing node.   --- UPDATED (Dexter) 20190505

                    Parameters
                    ------------------------------

                    dtype           `str`       - The data type of the data output. If `null`, it will be automatically determined during data pre-processing, and converted to `tf.float32` if it's using as an input of data model.

                    sourceCol       `str`       - An @IndexRange parsable string specifying the column selection (color channel) to be selected from the source data preprocessing node.
                '''
                super().__init__(instanceClass = DataPreprocessing.Node.Types.Image, dtype = dtype, sourceCol = sourceCol)
                # `bool` - Whether to preprocess the data in batch. `False` if it's preprocessed using Dataset API in an element-wise manner. 
                self.preprocessInBatch = False

            def refreshItemShape(self):
                '''
                    Refresh the item shape, typically after updates of datashape.   --- UPDATED (Dexter) 20190508
                '''
                # Get the shape from the node input.
                selShape = self.getInputShape()[1:]

                # Update the shape if there is transformation - crop, applied.
                # TODO:
                filterTxs = [tx for tx in self.transformations if isinstance(tx, DataPreprocessing.Transformation.ImageAugmentation.Crop)]
                if (len(filterTxs)):
                    selShape[-3] = filterTxs[-1].height
                    selShape[-2] = filterTxs[-1].width
                
                # Set the item shape.
                self.setItemShape([*selShape])
            
            def _processData(self, data: 'tf.Tensor', step: 'DataPreprocessing.Node.Image.StepEnum' = StepEnum.Output) -> 'tf.Tensor':
                '''
                    Data pre-processing given data generated from @Source.Config objects with output at this data preprocessing node.   --- UPDATED (Dexter) 20190508

                    Parameters
                    ------------------------------

                    data    `tf.Tensor`   - The source data to get pre-processed from the root @Source.Config objects.

                    step    `DataPreprocessing.Node.Image.StepEnum`   - The step of data preprocessing within a @DataPreprocessing.Node.Image object.

                    Returns
                    ------------------------------

                    `tf.Tensor`     - The preprocesssed data of this pre-processing node at the requested step.
                '''
                # Select specific channel.
                selectedData = Source.Table.tensorSlice(data, self.sourceCol)

                # Returns incoming data if the step request is input.
                if (step == DataPreprocessing.Node.Image.StepEnum.Input):
                    return selectedData
                
                # If there is transformation, apply each transformation function sequentially on each of the applied column indexes.
                for tx in self.transformations:
                    if isinstance(tx, DataPreprocessing.Transformation.ImageAugmentation.Config) and (self.train.currentSourceDataset in [DataGenerator.Dataset.Types.Train, DataGenerator.Dataset.Types.ValidationTrain]):
                        selectedData = tx.augmentTo(selectedData)
                    elif hasattr(tx, "transformTo"):
                        # Transform the data.
                        selectedData = tx.transformTo(selectedData)
                
                return selectedData

            def processData(self, data: Union['np.ndarray','tf.Tensor'], step: 'DataPreprocessing.Node.Image.StepEnum' = StepEnum.Output) -> Union['np.ndarray','tf.Tensor']:
                '''
                    Process given data generated from @Source.Config objects with output at this data preprocessing node. Data can be given with an np.ndarray or tf.Tensor types.   --- UPDATED (Dexter) 20190315

                    Parameters
                    ------------------------------

                    data    `np.ndarray|tf.Tensor`                                 - The source data to get pre-processed from the root @Source.Config objects.

                    step    `DataPreprocessing.Node.Image.StepEnum`   - The step of data preprocessing within a @DataPreprocessing.Node.Image object.

                    Returns
                    ------------------------------

                    `np.ndarray|tf.Tensor`     - The preprocesssed data of this pre-processing node at the requested step.
                '''
                if (isinstance(data, np.ndarray)):
                    return self.processDataNPArray(data, step)
                elif (isinstance(data, tf.Tensor)):
                    return self._processData(data,step)
                else:
                    raise ValueError("Data type not supported.")

            def processDataNPArray(self, data: 'np.ndarray', step: 'DataPreprocessing.Node.Image.StepEnum' = StepEnum.Output) -> 'np.ndarray':
                '''
                    Process given data generated from @Source.Config objects with output at this data preprocessing node. Data should be an np.ndarray object.   --- UPDATED (Dexter) 20190315

                    Parameters
                    ------------------------------

                    data    `np.ndarray`                                 - The source data to get pre-processed from the root @Source.Config objects.

                    step    `DataPreprocessing.Node.Image.StepEnum`   - The step of data preprocessing within a @DataPreprocessing.Node.Image object.

                    Returns
                    ------------------------------

                    `np.ndarray`     - The preprocesssed data of this pre-processing node at the requested step.
                '''
                return np.array(self._processData(tf.convert_to_tensor(data), step))

            def processDataTensor(self, data: 'tf.Tensor', step: 'DataPreprocessing.Node.Image.StepEnum' = StepEnum.Output) -> 'tf.Tensor':
                '''
                    Process given data generated from @Source.Config objects with output at this data preprocessing node. Data should be an tf.Tensor object.   --- UPDATED (Dexter) 20190315

                    Parameters
                    ------------------------------

                    data    `tf.Tensor`                                 - The source data to get pre-processed from the root @Source.Config objects.

                    step    `DataPreprocessing.Node.Image.StepEnum`   - The step of data preprocessing within a @DataPreprocessing.Node.Image object.

                    Returns
                    ------------------------------

                    `tf.Tensor`     - The preprocesssed data of this pre-processing node at the requested step.
                '''
                return self._processData(data)
            
            def recoverToRawData(self, items):
                '''
                    Recover data items to raw-data format of the predicted data, like undo normalization.   --- UPDATED (Dexter) 20191024

                    Parameters
                    ------------------------------

                    items               `np.ndarray|list[*+]`   - A list of data items.

                    Returns
                    ------------------------------

                    `np.ndarray|list`    - A list of recovered data items.
                '''
                # Convert to np array for the predicted data.
                items = np.array(items)

                # Transform the data back to the original distribution.
                if len(self.transformations):
                    # If there is transformation, reverse each transformation function sequentially on each of the applied column indexes.
                    for tx in reversed(self.transformations):
                        if hasattr(tx, "transformFrom"):
                            items = tx.transformFrom(items)

                # Recursively get previously recovered data.
                return items if isinstance(self.fromNode, Source.Config) else self.fromNode.recoverToRawData(items)

            def getPrintableItems(self, items, recovered = True):
                '''
                    Print some items into an array, with some further formatting.   --- UPDATED (Dexter) 20190922

                    Parameters
                    ------------------------------

                    items   `list[*+]`  - A list of items typically with original input format.

                    recovered   `bool`  - Whether the data is recovered from transformation or not.

                    Return
                    ------------------------------

                    `list[[str,*]+]`    - A list of items in a single data column with prefix data type column.
                '''
                return [["Image", json.dumps(i.tolist())] for i in (items if recovered else self.recoverToRawData(items))]

        class SourceLike(Config):
            """
                Abstract class of a source-like data-preprocessing node config.   --- UPDATED (Dexter) 20191015
            """
            class StepEnum(_DataPreprocessingNodeStepEnum):
                '''
                    Enumeration defining the step of data preprocessing within a @DataPreprocessing.Node.SourceLike object.   --- UPDATED (Dexter) 20200223
                '''
                # `DataPreprocessing.Node.SourceLike.StepEnum` - The concatenated data from all the from nodes of this data preprocessing node object.
                RawInput = -1
                # `DataPreprocessing.Node.SourceLike.StepEnum` - The raw data received from previous root data or @DataPreprocessing.Node.Config object.
                Input = 0
                # `DataPreprocessing.Node.SourceLike.StepEnum` - Data generated from this source like data preprocessing node.
                DataGenerated = 1
                # `DataPreprocessing.Node.SourceLike.StepEnum` - Data just after transformations.
                Transformed = 2
                # `DataPreprocessing.Node.SourceLike.StepEnum` - Data just after one-hot encoding column expansion. Also as the output of the data preproccessing.
                Output = 10
            
            def __init__(self, instanceClass: 'DataPreprocessing.Node.Types' = None, sourceCol: str = "None:None", dtype: 'tf.DType' = None):
                '''
                    Create a configuration of a source-like data preprocessing node.   --- UPDATED (Dexter) 20191019

                    Parameters
                    ------------------------------

                    instanceClass       `DataPreprocessing.Node.Types`   - The instnace class, as defined in @DataPreprocessing.Node.Types .

                    sourceCol   `str`               - An @IndexRange parsable string specifying the column selection (data feature) to be selected from the source data preprocessing node.

                    dtype       `tf.DType`          - The TensorFlow data type of this source column config. If `None`, it will be automatically determined during data pre-processing, and converted to `tf.float32` if it's using as an input of data model.
                '''
                super().__init__(instanceClass = (instanceClass or DataPreprocessing.Node.Types.SourceLike), dtype = dtype, sourceCol = sourceCol)
                # `bool` - Whether the data preprocessing source is splittable, for the purpose of partitioning validation data.
                self.splittable: bool = False
                # `DataPreprocessing.Node.GetDataMode` - The method for getting data from previous nodes or by itself.
                self._getDataMode: DataPreprocessing.Node.GetDataMode = DataPreprocessing.Node.GetDataMode.Current
                # `DataGenerator.Controller` - The data generator controller for this data preprocessing node.
                self.generatorController: DataGenerator.Controller = None

            @property
            def batchSize(self) -> int:
                """
                    Batch size when using batched training.   --- UPDATED (Dexter) 20200308

                    Returns
                    ------------------------------

                    `int`   - Batch size when using batched training.
                """
                if (self.train is None):
                    raise ValueError("This source is not attached to a Train object yet. No batch size can be determined.")

                return self.train.buildConfigs[self.train.buildNo].batchSize

            @property
            def batchCountPerEpoch(self) -> int:
                """
                    Get the number of batches included in one epoch of the current set.   --- UPDATED (Dexter) 20200308

                    Returns
                    ------------------------------

                    `bool` - The number of batches included in one epoch.
                """
                return self.generatorController[self.train.currentSourceDataset].batchCountPerEpoch

            @property
            def epochSize(self) -> int:
                """
                    The epoch size from this source like data preprocessing node.   --- UPDATED (Dexter) 20200310

                    Returns
                    ------------------------------

                    `int` - The epoch size from this source like data preprocessing node.
                """
                return self.generatorController[self.train.currentSourceDataset].epochSize

            @property
            def testRatio(self) -> int:
                """
                    The test data proportion.   --- UPDATED (Dexter) 20200308

                    Returns
                    ------------------------------

                    `int`   - The test data proportion.
                """
                if (self.train is None):
                    raise ValueError("This source is not attached to a Train object yet. No test ratio can be determined.")

                return self.train.testRatio

            @property
            def shuffle(self) -> bool:
                """
                    Whether the data source will be shuffled on training.   --- UPDATED (Dexter) 20200308

                    Returns
                    ------------------------------

                    `bool`   - Whether the data source will be shuffled on training.
                """
                if (self.train is None):
                    raise ValueError("This source is not attached to a Train object yet. No shuffle can be determined.")

                return self.train.buildConfigs[self.train.buildNo].shuffle

            def parseJSON(self, obj: Dict[str, Any], train: 'Train'):
                """Parse a previously saved object into this @DataPreprocessing.Node.TimeSeries object.   --- UPDATED (Dexter) 20200308

                    Parameters
                    ------------------------------

                    obj     `dict<str,*>`   - JSON object from Project file.
                    
                    train   `Train`         - The train object this @DataPreprocessing.Node.TimeSeries object is attached in.
                """
                for k,v in obj.items():
                    if (k == "generatorController"):
                        # Parse generator controllers.
                        self.generatorController.parseFromJSON(v)
                    elif (k == "source"):
                        setattr(self, k, v)
                        # It's a number if it's a source; a string if it's another data preprocessing node.
                        self._train = train
                        self._fromNode = train.sources[v] if isinstance(v, int) else train.dppNodes[v]
                    elif (k == "transformations"):
                        # If there is .instanceClass property, use DataPreprocessing.Transformation to parse; otherwise, it's a legacy normal object.
                        setattr(self, k, [(DataPreprocessing.Transformation.createFromJSON(tx) if "_instanceClass" in tx else tx) for tx in v])
                    elif (k == "dtype"):
                        # Get Python data type.
                        if (v is not None):
                            self.dtype = Source.Config.getDataType(v)
                    elif (k == "_getDataMode"):
                        setattr(self, k, DataPreprocessing.Node.GetDataMode[v])
                    elif (k == "crops"):
                        self.crops = {idx: crop for idx, crop in enumerate(v) if crop is not None}
                    elif (k not in ["train", "fromNode", "_instanceClass"]):
                        setattr(self, k, v)
                        
            def getData(self, step: 'DataPreprocessing.Node.SourceLike.StepEnum' = StepEnum.Output, 
                        sourceDataset: 'DataGenerator.Dataset.Types' = None,
                        start: int = None, end: int = None) -> List[List[int]]:
                """Virtual method to get the output data of this data preprocessing node, with specification of the range of the batch.   --- UPDATED (Dexter) 20200501
                
                Parameters
                ------------------------------

                step `DataPreprocessing.Node.SourceLike.StepEnum` - The step of data preprocessing within this @DataPreprocessingNodeConfig object.

                sourceDataset `DataGenerator.Dataset.Types` - The type of dataset of a source that's this data is referencing, as defined in @DataGenerator.Dataset.Types .

                start `int` - Batch starting index (inclusive). If `"None"`, it notates `0`.

                end `int` - Batch ending index (exclusive). If `"None"`, it notes the column count.

                Returns
                ------------------------------

                `list<list<int>>` - A data array after the requested processing step at this data preprocessing node.

                """
                rootData: Train.RootData
                if step == DataPreprocessing.Node.SourceLike.StepEnum.RawInput:
                    # Get the incoming data.
                    rootSources: Train.RootSources = Train.RootSources(self, self.getRootSources(), sourceDataset)

                    # Ensure all of the sources are tables.
                    rootData = rootSources.getData(start, end)
                else:
                    rootData = self.generatorController.get(sourceDataset or self.train.currentSourceDataset).getData(start, end)
                
                # Return the transformed data.
                return self.getProcessedData(rootData, step)
            
            def getProcessedData(self, rootData: 'Train.RootData|list<*>', step: 'DataPreprocessing.Node.SourceLike.StepEnum' = StepEnum.Output) -> 'list<*>':
                """Virtual method to get the processed data based on a given root data.   --- UPDATED (Dexter) 20200311

                Parameters
                ------------------------------

                rootData `Train.RootData|list<*>` - Root data of the @Train object, or data generated from this source like node.

                step `DataPreprocessing.Node.SourceLike.StepEnum` - The step of data preprocessing within this @DataPreprocessing.Node.SourceLike object.
                
                Returns
                ------------------------------
                
                `list<*>` - The proccessed data.
                """
                # Get raw input data.
                if step == DataPreprocessing.Node.SourceLike.StepEnum.RawInput:
                    if isinstance(rootData, list):
                        raise ValueError("When raw input is requested for source like data preprocessing node, root data must not root data of the train object.")
                    if self.key in [*rootData.dppNodes.keys()]:
                        raise ValueError("When raw input is requested for source like data preprocessing node, root data must not include itself.")

                    # Get the incoming data.
                    incomingData = (rootData.sources[self.source] if self.outputset is None else rootData.sources[self.source][self.outputset])\
                                        if isinstance(self.source, int) else self.train.dppNodes[self.source].getProccessedData(rootData)
                    return incomingData
                
                if isinstance(rootData, Train.RootData):
                    # Return the cached data if needed.
                    if self.key in [*rootData.dppNodes.keys()]:
                        if step == DataPreprocessing.Node.SourceLike.StepEnum.RawInput:
                            return rootData.dppNodes[self.key]
                        elif step != DataPreprocessing.Node.SourceLike.StepEnum.Output:
                            raise ValueError("When data is preprocessed elementally, it cannot be received using intercepted steps.")
                        else:
                            rootData = rootData.dppNodes[self.key]
                    else:
                        raise ValueError("Root Data cannot be found for data preprocessing")

                # Process the data.
                return self.processData(rootData, step)

            def _processData(self, data: 'np.ndarray', step: 'DataPreprocessing.Node.StepEnum' = _DataPreprocessingNodeStepEnum.Output) -> 'np.ndarray':
                '''
                    Abstract method of data pre-processing given data generated from @Source.Config objects with output at this data preprocessing node.   --- UPDATED (Dexter) 20190508

                    Parameters
                    ------------------------------

                    data    `np.ndarray`                   - The source data to get pre-processed from the root @Source.Config objects.

                    step    `DataPreprocessing.Node.StepEnum`   - The step of data preprocessing within a @DataPreprocessing.Node.Config object.

                    Returns
                    ------------------------------

                    `np.ndarray`     - The preprocesssed data of this pre-processing node at the requested step.
                '''
                pass
            
            def processData(self, data: Union['np.ndarray','tf.Tensor'], step: 'DataPreprocessing.Node.StepEnum' = _DataPreprocessingNodeStepEnum.Output) -> Union['np.ndarray','tf.Tensor']:
                '''
                    Virtual method to process given data generated from @Source.Config objects with output at this data preprocessing node. Data can be given with an np.ndarray or tf.Tensor types.   --- UPDATED (Dexter) 20190315

                    Parameters
                    ------------------------------

                    data    `np.ndarray|tf.Tensor`              - The source data to get pre-processed from the root @Source.Config objects.

                    step    `DataPreprocessing.Node.StepEnum`   - The step of data preprocessing within a @DataPreprocessing.Node.Config object.

                    Returns
                    ------------------------------

                    `np.ndarray|tf.Tensor`     - The preprocesssed data of this pre-processing node at the requested step.
                '''
                if (isinstance(data, np.ndarray)):
                    return self._processData(data, step)
                elif (isinstance(data, tf.Tensor)):
                    return self.processDataTensor(data, step)
                else:
                    raise ValueError("Data type not supported.")

            def processDataNPArray(self, data: 'np.ndarray', step: 'DataPreprocessing.Node.StepEnum' = _DataPreprocessingNodeStepEnum.Output) -> 'np.ndarray':
                '''
                    Virtual method to process given data generated from @Source.Config objects with output at this data preprocessing node. Data should be an np.ndarray object.   --- UPDATED (Dexter) 20190312

                    Parameters
                    ------------------------------

                    data    `np.ndarray`                                 - The source data to get pre-processed from the root @Source.Config objects.

                    step    `DataPreprocessing.Node.StepEnum`   - The step of data preprocessing within a @DataPreprocessing.Node.Config object.

                    Returns
                    ------------------------------

                    `np.ndarray`     - The preprocesssed data of this pre-processing node at the requested step.
                '''
                return self._processData(data, step)

            def processDataTensor(self, data: 'tf.Tensor', step: 'DataPreprocessing.Node.StepEnum' = _DataPreprocessingNodeStepEnum.Output) -> 'tf.Tensor':
                '''
                    Virtual method to process given data generated from @Source.Config objects with output at this data preprocessing node. Data should be an tf.Tensor object.   --- UPDATED (Dexter) 20190312

                    Parameters
                    ------------------------------

                    data    `tf.Tensor`                                 - The source data to get pre-processed from the root @Source.Config objects.

                    step    `DataPreprocessing.Node.StepEnum`   - The step of data preprocessing within a @DataPreprocessing.Node.Config object.

                    Returns
                    ------------------------------

                    `tf.Tensor`     - The preprocesssed data of this pre-processing node at the requested step.
                '''
                return tf.convert_to_tensor(self._processData(np.array(data), step))

            def getNextBatch(self, sourceDataset: 'DataGenerator.Dataset.Types' = None) -> Any:
                '''
                    Generate the next batch of data of the required source dataset.   --- UPDATED (Dexter) 20200308

                    Parameters
                    ------------------------------

                    sourceDataset   `DataGenerator.Dataset.Types`  - The type of dataset of a source that's this data is referencing.

                    Returns
                    ------------------------------

                    `*`    - Next batch of data.
                '''
                return self.generatorController[sourceDataset or self.train.currentSourceDataset].getNextBatch()
            
            def getRandItems(self, sourceDataset: 'DataGenerator.Dataset.Types' = None, randomSeed: int = None, randomCount: int = None):
                '''
                    Get random items of the required source dataset.   --- UPDATED (Dexter) 20200308

                    Parameters
                    ------------------------------

                    sourceDataset   `DataGenerator.Dataset.Types`  - The type of dataset of a source that's this data is referencing.

                    randomSeed      `int`       - Random seed.

                    randomCount     `int`       - The count of random items to collect.

                    Returns
                    ------------------------------

                    `*`    - Random batch of data.
                '''
                return self.generatorController[sourceDataset or self.train.currentSourceDataset].getRandItems(randomSeed, randomCount)
            
        class TimeSeries(SourceLike):
            """
                Class representing a series data preprocessing node configuration.   --- UPDATED (CYK, Dexter) 20190505
            """
            class StepEnum(_DataPreprocessingNodeStepEnum):
                '''
                    Enumeration defining the step of data preprocessing within a @DataPreprocessing.Node.Config object.   --- UPDATED (Dexter) 20190129
                '''
                # Data just after series conversion.
                SeriesConverted = 3
            
            class GeneratorDetail(DataGenerator.Detail):
                """A generator detail for time series data preprocessing node.   --- UPDATED (CYK, Dexter) 20191022"""
                def __init__(self, controller: 'DataPreprocessing.Node.TimeSeries.GeneratorController'):
                    """Create a @DataPreprocessing.Node.TimeSeries.GeneratorDetail object for generating data of series table sources.   --- UPDATED (CYK, Dexter) 20200308

                    Parameters
                    ------------------------------

                    controller `DataPreprocessing.Node.TimeSeries.GeneratorController` - The dataset controller maintaining this data generator detail.
                    """
                    super().__init__(controller)
                    # `list<int>` - The reference indexes of the full incoming data.
                    self._referenceIndexes: List[int] = []
                    # `np.ndarray<*>` - The cached input data for previous node/sources.
                    self._cacheInput: List[Any] = []
                
                @property
                def epochSize(self) -> int:
                    """
                        Epoch size of the data, i.e. total number of records in the training dataset.   --- UPDATED (Dexter) 20200308

                        Returns
                        ------------------------------

                        `int`   - Epoch size of the data, i.e. total number of records in the training dataset.
                    """
                    return (len(self._referenceIndexes) if len(self._referenceIndexes) else None) if self._epochSize is None else self._epochSize

                def getEpochSize(self, refresh: bool = False) -> int:
                    """
                        Get the epoch size of this data preprocessing node using the current source dataset.   --- UPDATED (Dexter) 20200310

                        Parameters
                        ------------------------------

                        refresh `bool` - Whether to refresh cached epoch size.

                        Returns
                        ------------------------------

                        `int` - The epoch size of this data preprocessing node using the current source dataset.
                    """
                    return self.epochSize

                def copy(self) -> 'DataPreprocessing.Node.TimeSeries.GeneratorDetail':
                    """Copy this generator detail.   --- UPDATED (Dexter) 20200308

                    Returns
                    ------------------------------

                    `DataPreprocessing.Node.TimeSeries.GeneratorDetail` - The copied generator detail object.
                    """
                    # Create a new detail object.
                    newDetail: DataPreprocessing.Node.TimeSeries.GeneratorDetail = DataPreprocessing.Node.TimeSeries.GeneratorDetail(self.controller)

                    # Copy all information.
                    for k in ["_epochSize", "_oriShape", "_controller", "_batchSize", "_shuffle", "_referenceIndexes"]:
                        setattr(newDetail, k, getattr(self,k))
                    
                    return newDetail
                                    
                def initialize(self):
                    """Intitiate this generator loop.   --- TRAIN --- UPDATED (CYK, Dexter) 20200310
                    """
                    super().initialize()                    

                    # Initialize for reference indexes.
                    if len(self._referenceIndexes) == 0:
                        # 'DataPreprocessing.Node.TimeSeries' - The dpp node of this generator detail.
                        dppNode: DataPreprocessing.Node.TimeSeries = self.controller.attachObject
                        
                        # Return if there is no data
                        prevEpochSize: int = dppNode.getRootSources()[0].epochSize
                        if prevEpochSize is None:
                            return
                        
                        # Set the reference indexes.
                        self._referenceIndexes = [i for i in range(0, prevEpochSize - (dppNode.seriesLen - 1))]

                        # If there is some specific series label, sort the reference indexes.
                        if dppNode.seriesLabel is not None:
                            # Get the root source.
                            labelIndex: int = dppNode.seriesLabel
                            rootSources: List[DataPreprocessing.Node.Config, Source.Config] = dppNode.getRootSources()

                            # If the root source can be read in memory.
                            if rootSources[0].splittable:
                                # Get all data.
                                data: List[Any] = rootSources[0].getData()

                                # Sort the reference index for different time parser or numerical parser.
                                if dppNode.seriesLabelParser == "n":
                                    self._referenceIndexes.sort(key = lambda dataRowIndex: float(data[dataRowIndex][labelIndex]))
                                else:
                                    self._referenceIndexes.sort(key = lambda dataRowIndex: data[dataRowIndex][labelIndex])
                        
                        # Cache the data if the from node is another data preprocessing node.
                        if isinstance(dppNode.fromNode, DataPreprocessing.Node.Config):
                            # Get the root source.
                            rootSources: List[DataPreprocessing.Node.Config, Source.Config] = dppNode.getRootSources()

                            # If the root source can be read in memory.
                            if rootSources[0].splittable:
                                # Get all data.
                                data: np.ndarray = rootSources[0].getData()
                            
                                # Preprocess the data.
                                self._cacheInput = dppNode.fromNode.processData(data)
                        
                        # Reverse the order if needed.
                        if dppNode.reversed:
                            self._referenceIndexes = [index + dppNode.seriesLen - 1 for index in self._referenceIndexes]                            
                
                def getCacheInput(self, start: int = None, end: int = None, indexes: List[int] = None) -> 'np.ndarray<*>':
                    """Get the cache input data of with specific indexes.   --- UPDATED (Dexter) 20200229

                    Parameters
                    ------------------------------

                    start `int` - Batch starting index (inclusive). If `None`, it notates `0`.

                    end `int` - Batch ending index (exclusive). If `None`, it notes the epoch size.

                    indexes `list<int>` - A list of indexes of data rows.

                    Returns
                    ------------------------------

                    `np.ndarray<*>` - The requested slice of data of the cache input data.
                    """
                    # Translate String "None" into null.
                    if start == "None":
                        start = None
                    if end == "None":
                        end = None
                    
                    # Check the batch start and end to get the data.
                    if start is not None or end is not None:
                        return self._cacheInput[start:end]
                    elif indexes is not None:
                        if any([(idx >= len(self._cacheInput) or idx < -len(self._cacheInput)) for idx in indexes]):
                            raise ValueError("Some of the requested indexes are out of range.")
                        return self._cacheInput[indexes]
                    else:
                        return self._cacheInput[:]
                
                def prepareIteration(self):
                    """Prepare a new round of data source iteration.   --- UPDATED (Dexter) 20200307
                    """
                    # 1. Confirm this data generator detail is initialized before any generator yields.
                    if (not self.initialized):
                        self.initialize()

                    # 2. Reset the iteration index.
                    self.batchIdxReset()

                    # 3. Reshuffle the data if needed.
                    if (self.shuffle):
                        self._shuffleData()
                
                def _shuffleData(self):
                    """Shuffle the data.   --- UPDATED (CYK, Dexter) 20200307
                    """
                    # Shuffle the data in-place.
                    np.random.shuffle(self._referenceIndexes)
                
                def setData(self, inputArray: Union[list] = [], outputArray: None = None, hasHeader: None = None):
                    """Read the data table and set the raw data of this source.   --- UPDATED (Dexter) 20200308

                    Parameters
                    ------------------------------
                    
                    inputArray  `list[list[float|int|str]]` - An input data table (training set).

                    outputArray `list[list[float|int|str]]` - An output data table (training set).

                    hasHeader   `bool`  - Whether this table source includes a heading.
                    """
                    # Convert input array into list.
                    referenceIndexes = list(inputArray)

                    # Ensure the reference index is of 1 dimension.
                    if len(np.shape(referenceIndexes)) != 1:
                        raise ValueError("Input Shape must be a 1-dimensional list.")
                    
                    # Ensure there is at least some data.
                    if len(referenceIndexes) == 0:
                        raise ValueError("Some data should be set instead of `None` or array with zero length")
                        
                    # Set the reference indexes.
                    self._referenceIndexes = referenceIndexes

                def getData(self, start: int = None, end: int = None, indexes: int = None) -> 'np.ndarray<*>':
                    """Get the data of with specific indexes.   --- UPDATED (CYK, Dexter) 20200308

                    Parameters
                    ------------------------------

                    start `int` - Batch starting index (inclusive). If `None`, it notates `0`.

                    end `int` - Batch ending index (exclusive). If `None`, it notes the epoch size.

                    indexes `list<int>` - A list of indexes of data rows.

                    Returns
                    ------------------------------

                    `np.ndarray<*>` - The requested slice of data of the generator detail.
                    """
                    # Confirm this data generator detail is initialized before any generator yields.
                    if not self.initialized:
                        self.initialize()
                    
                    # 'DataPreprocessing.Node.TimeSeries' - The dpp node of this generator detail.
                    dppNode: DataPreprocessing.Node.TimeSeries = self.controller.attachObject

                    # Return if there is no data.
                    if dppNode.getEpochSize(True) is None:
                        return []
                    
                    # Get the root source of this node.
                    rootSources: List[DataPreprocessing.Node.Config, Source.Config] = dppNode.getRootSources()

                    # Translate String "None" into null.
                    if start == "None" or start is None:
                        start = 0
                    if end == "None" or end is None:
                        end = len(self._referenceIndexes)
                    
                    # Get the selected reference indexes.
                    batchIndexes: List[int]
                    if start is not None or end is not None:
                        batchIndexes = self._referenceIndexes[start: end]
                    elif indexes is not None:
                        if any([(idx >= end or idx < -end) for idx in indexes]):
                            raise ValueError("Some of the requested indexes are out of range.")
                        indexes = [(idx if idx >= 0 else (len(self._referenceIndexes) + idx)) for idx in indexes]
                        
                        returnAry: List[Any] = []
                        for index in indexes:
                            returnAry.append(self._referenceIndexes[index])
                        batchIndexes = returnAry
                    else:
                        batchIndexes = self._referenceIndexes[:]
                    
                    # If it's a splittable source (All data can be read within memory).
                    if not rootSources[0].splittable:
                        raise ValueError("Time Series data preprocessing node currenly only supports for splittable data sources.")
                    
                    # Ensure the from node is not an array.
                    if isinstance(dppNode.fromNode, list):
                        raise ValueError("Multiple from node is detected from a time series data source")
                        
                    # Collect the data for eacth time step.
                    stepsData: List[np.ndarray] = []
                    oneStepShape: List[int] = None
                    increment: int = -1 if dppNode.reversed else 1

                    for t in range(0, dppNode.seriesLen):
                        oneStepData: np.ndarray
                        # If there is cache, just collect from it.
                        if self._cacheInput and len(self._cacheInput):
                            oneStepData = self.getCacheInput(indexes = batchIndexes)
                        # Otherwise, collect from the source config.
                        else:
                            oneStepData = dppNode.fromNode.getData(indexes = batchIndexes)
                        
                        # Select specific dimensions.
                        oneStepShape = np.shape(oneStepData) if oneStepShape is None else oneStepShape
                        selectedData: np.ndarray = oneStepData[(slice(None, None),
                            *((IndexRange.toIndexer(dppNode.crops[idx]) if idx in dppNode.crops else slice(None, None)) for idx, v in enumerate(oneStepShape[1:-1])),
                            IndexRange.toIndexer(dppNode.sourceCol))]
                        stepsData.append(selectedData[:, np.newaxis])
                        batchIndexes = [(batchIndex + increment) for batchIndex in batchIndexes]
                    
                    # Return the concatenated matrix.
                    return np.concatenate(stepsData, axis = 1)
                
                def getNextBatch(self) -> 'np.ndarray':
                    """Generate the next batch of data rows of the root source.   --- UPDATED (Dexter) 20200308

                    Returns
                    ------------------------------

                    'np.ndarray' - The next batch data. In the future, the data will be output as `tf.Tensor` objects.
                    """
                    # Confirm this data generator detail is initialized before any generator yields.
                    if not self.initialized:
                        self.initialize()

                    # Initiate batch index if needed.
                    if self.batchIdx is None:
                        self.prepareIteration()
                    
                    # Get an index and batch size.
                    i: int = self.batchIdx
                    batchSize: int = self.batchSize

                    # If this batch has touched the end of the epoch, regenerate the data .
                    if ((self.dropRemainder and (i+1)*batchSize > self.epochSize) or ((not self.dropRemainder) and i*batchSize >= self.epochSize)):
                        self.prepareIteration()
                        i = self.batchIdx
                    
                    # Increment the index and return all the data.
                    returnData: np.ndarray = self.getData(start = i*batchSize, end = (i+1)*batchSize)

                    # Get the batch data.
                    self.batchIdxIncrement()

                    return returnData
                
                def getRandItems(self, randomSeed: int = None, randomCount: int = None) -> 'np.ndarray':
                    """Generate the random items of data. No action taken, and a sub-class should be used.   --- UPDATED (Dexter) 20200308

                    Parameters
                    ------------------------------

                    randomSeed      `int`       - Random seed.

                    randomCount     `int`       - The count of random items to collect.

                    Returns
                    ------------------------------

                    'np.ndarray' - Random items of data. In the future, the data will be output as `tf.Tensor` objects.
                    """
                    # Ensure there is original data.
                    if (len(self._referenceIndexes) == 0):
                        return np.array([])

                    # Get random indexing.
                    randIdx: np.ndarray = np.arange(len(self._referenceIndexes))
                    np.random.seed(randomSeed)
                    np.random.shuffle(randIdx)
                    np.random.seed()
                    
                    # Increment the index and return all the data.
                    returnData: np.ndarray = self.getData(indexes=randIdx)

                    # 4. Copy the randomly-indexed items.
                    return returnData.copy()
                
                def partition(self, prop: float = 0.2, shuffle: float = False) -> Tuple['np.ndarray','np.ndarray']:
                    '''
                        Partition current data into 2 parts.   --- UPDATED (Dexter) 20200308

                        Parameters
                        ------------------------------

                        prop            `float`     - Proportion of original dataset to be the secondary dataset.

                        shuffle         `bool`      - Whether to shuffle before data splitting.

                        Returns
                        ------------------------------

                        `tuple<'np.ndarray','np.ndarray'>` - Primary and secondary datasets.
                    '''
                    # Confirm this data generator detail is initialized before any generator yields.
                    if not self.initialized:
                        self.initialize()

                    # Check total dataset size
                    dataCount = len(self._referenceIndexes)
                    portion = math.floor(dataCount*prop)
                    indexes = self._referenceIndexes[:]

                    # If needed, shuffle the data.
                    if (shuffle):
                        np.random.shuffle(indexes)

                    # Split the dataset
                    primaryIndexes = indexes[portion:]
                    secondaryIndexes = indexes[:portion]

                    # Assign the splitted datasets
                    return (primaryIndexes, secondaryIndexes)
                
            class GeneratorController(DataGenerator.Controller):
                """
                    Class representing a data generator controller for series data preprocessing nodes.   --- UPDATED (Dexter) 20190713
                """
                def __init__(self, attachObject: 'DataPreprocessing.Node.TimeSeries'):
                    """
                        Create a @DataGenerator.Controller object for controlling data generator of a series data preprocessing node-.   --- UPDATED (Dexter) 20191019

                        Parameters
                        ------------------------------

                        attachObject `DataPreprocessing.Node.TimeSeries` - The object this data generator controller is embedded with.
                    """
                    super().__init__(attachObject = attachObject)
                
                @property
                def detailType(self) -> Callable[..., 'DataPreprocessing.Node.TimeSeries.GeneratorDetail']:
                    """
                        Get the data generator detail type.   --- UPDATED (Dexter) 20191019

                        Returns
                        ------------------------------

                        `Callable<*, DataPreprocessing.Node.TimeSeries.GeneratorDetail>`    - The type of the generator detail.
                    """
                    return DataPreprocessing.Node.TimeSeries.GeneratorDetail

                def splitValidationDataset(self, validation: float = 0.1, randomFold: bool = False):
                    """Split current data into training and validation datasets.   --- UPDATED (Dexter) 20200310
                    
                    Parameters
                    ------------------------------

                    validation `float` - Proportion of original dataset to be as the validation dataset.

                    randomFold `bool` - Whether to perform random sub-sampling validation.
                    """
                    # 0. Check total dataset size
                    trainset = self[DataGenerator.Dataset.Types.Train]
                    rowCount = trainset.epochSize

                    # 1. Consider if random sub-sampling or k-fold is required.
                    # 1A. If random sub-sampling is needed, subsample with random partition.
                    if randomFold:
                        (trainIndexes, valIndexes) = trainset.partition(prop = validation, shuffle = True)
                    
                    # 1B. If k-fold is required, cut the suitable part of the dataset.
                    else:
                        # 1B-1. Determine no. of validation count required.
                        validationCount = math.floor(1/validation)
                        i = self.validationTime % validationCount
                        data = trainset.oriData.copy()[1:] if trainset.hasHeader else trainset.oriData.copy() 

                        # 1B-2. If it's the first time to split and this dataset needs shuffling, shuffle the dataset.
                        if (i == 0 and trainset.shuffle): 
                            np.random.shuffle(data)

                        # 1B-3: Split the dataset by the index.
                        valPortion = math.ceil(rowCount/validationCount)
                        startIdx = i*valPortion
                        endIdx = (i+1)*valPortion
                        trainIndexes = np.vstack((data[:startIdx], data[endIdx:]))
                        valIndexes = data[startIdx: endIdx]
                    
                    # 2. Assign the sliced dataset to the training and validation part
                    self[DataGenerator.Dataset.Types.ValidationTrain].setData(inputArray = trainIndexes)
                    self[DataGenerator.Dataset.Types.Validation].setData(inputArray = valIndexes)

                    # Assign the reference of root sources.
                    rootSources = self.attachObject.getRootSources()
                    for rootSource in rootSources:
                        rootSource.generatorController[DataGenerator.Dataset.Types.Validation] = rootSource.generatorController[DataGenerator.Dataset.Types.Train]
                        rootSource.generatorController[DataGenerator.Dataset.Types.ValidationTrain] = rootSource.generatorController[DataGenerator.Dataset.Types.Train]
                    
                    # 3. Increment the validation time by one
                    self.validationTime += 1
                
                def splitTestDataset(self, test: float = 0.2, shuffle: float = False):
                    """Split original train data into train and test sets.   --- UPDATED (Dexter) 20200310

                    Parameters
                    ------------------------------

                    test            `float`     - Proportion of original dataset to be as the test dataset.

                    shuffle         `bool`      - Whether to shuffle before data splitting.
                    """
                    # Get the partiitoned datasets.
                    trainset = self[DataGenerator.Dataset.Types.Train]
                    (trainIndexes, testIndexes) = trainset.partition(prop = test, shuffle = shuffle)
                    
                    # Create the test dataset.
                    self[DataGenerator.Dataset.Types.Train].setData(inputArray = trainIndexes)
                    self[DataGenerator.Dataset.Types.Test].setData(inputArray = testIndexes)

                    # Assign the reference of root sources.
                    rootSources = self.attachObject.getRootSources()
                    for rootSource in rootSources:
                        rootSource.generatorController[DataGenerator.Dataset.Types.Test] = rootSource.generatorController[DataGenerator.Dataset.Types.Train]
                    
            
            def __init__(self, seriesLen: int = 2, sourceCol: str = "None:None", dtype: 'tf.DType' = None):
                '''
                    Create a configuration of a series data.   --- UPDATED (CYK, Dexter) 20191019

                    Parameters
                    ------------------------------

                    sourceCol       `str`                - An @IndexRange parsable string specifying the column selection from the source.

                    seriesLen       `int`                - The length of series.

                    dtype           `tf.DType`           - The TensorFlow data type of this source column config. If `None`, it will be automatically determined during data pre-processing, and converted to `tf.float32` if it's using as an input of data model.

                '''
                super().__init__(instanceClass = DataPreprocessing.Node.Types.TimeSeries, sourceCol = sourceCol, dtype = dtype)
                # `int` - The series sorting index. `-1` refers to sorting by original dataset order.
                self.seriesSort: int = -1
                # `int` - The length of time series.
                self.seriesLen: int = seriesLen

                # `dict<int,set<str>>` - The column selections on which columns to convert to one hot encoding.
                self.oneHotColumns: Dict[int, Set[str]] = {}
                # `list<DataPreprocessing.Transformation.Columns.Config>` - The transformation definition information in action order.
                self.transformations: List[DataPreprocessing.Transformation.Columns.Config] = []
                # `list<DataPreprocessing.CircularConfig>` - The cicular definition information in action order.
                self.circular: List[DataPreprocessing.CircularConfig] = []
                # `dict<int,int>` - A map from column index to circular config.
                self._colToCircular: Dict[int, int] = {}
                # `bool` - Whether to preprocess the data in batch. `False` if it's preprocessed using Dataset API in an element-wise manner.   --- RESERVED
                self.preprocessInBatch: bool = True
                # `int` - The class count of all values in this column data preprocessing node.
                self._classCount: int = None

                # `DataPreprocessing.Node.TimeSeries.GeneratorController` - The data generator controller for this data preprocessing node.
                self.generatorController: DataPreprocessing.Node.TimeSeries.GeneratorController = DataPreprocessing.Node.TimeSeries.GeneratorController(self)

            def appendOn(self, source: Union['Source.Config', 'DataPreprocessing.Node.Config', List['DataPreprocessing.Node.Config']], key: str, outputset: str = None) -> 'DataPreprocessing.Node.TimeSeries':
                """
                    Append this data preprocessing node on a @Source.Config or @DataPreprocessing.Node.Config.   --- UPDATED (Dexter, CYK) 20200126

                    Parameters
                    ------------------------------

                    source      `Source.Config | DataPreprocessing.Node.Config | list<DataPreprocessing.Node.Config>`     - The previous @Source.Config or @DataPreprocessing.Node.Config this data preprocessing node is linked to.

                    key         `str`       - The key for accessing this data preprocessing node.

                    outputset   `str`       - The dataset key within the data source. Applicable only when appending on a multi-outputset data source.

                    Returns
                    ------------------------------

                    `DataPreprocessing.Node.TimeSeries`     - This object if it has been successfully appended.
                """
                # Disable the source to be a list.
                if isinstance(source, list):
                    raise ValueError("Time Series Data Preprocessing Node can only append on a single Source.Config or DataPreprocessing.Node.Config object.")
                elif isinstance(source, DataPreprocessing.Node.Config):
                    # Ensure there is only one root source.
                    rootSource = source.getRootSources()
                    if (len(rootSource) > 0):
                        raise ValueError("Time Series Data Preprocessing Node can only make reference to one root source.")
                
                    # Check if there are any serires dpp nodes before.
                    prevNodes = [source]

                    # Check if the previous source is a series data preprocessing node.
                    if isinstance(source, DataPreprocessing.Node.TimeSeries):
                        raise ValueError("There should be no series data preprocessing node before any series data preprocessing node.")
                    
                    # Iteratively scan the previous path of data preprocessing nodes.
                    while (len(prevNodes)):
                        newNodes = set()
                        for s in prevNodes:
                            if isinstance(s.fromNode, list):
                                for n in s.fromNode:
                                    if isinstance(n, DataPreprocessing.Node.TimeSeries):
                                        raise ValueError("There should be no series data preprocessing node before any series data preprocessing node.")
                                    elif isinstance(n, DataPreprocessing.Node.Config):
                                        newNodes.add(n)
                            if isinstance(s.fromNode, DataPreprocessing.Node.TimeSeries):
                                raise ValueError("There should be no series data preprocessing node before any series data preprocessing node.")
                            elif isinstance(s.fromNode, DataPreprocessing.Node.Config):
                                newNodes.add(s.fromNode)
                        prevNodes = newNodes
                
                # Call the parent method.
                return super().appendOn(source, key, outputset = outputset)

                # Because the root source dataset, we can determine the epoch size, and so initialize the dataset generator of this dpp node.
                # TODO: initialize ._timeSeriesGenerator

            def getEpochSize(self, refresh: bool = False) -> int:
                """
                    Get the epoch size of this data preprocessing node using the current source dataset.   --- UPDATED (Dexter) 20200310

                    Parameters
                    ------------------------------

                    refresh `bool` - Whether to refresh cached epoch size.

                    Returns
                    ------------------------------

                    `int` - The epoch size of this data preprocessing node using the current source dataset.
                """
                return self.generatorController[self.train.currentSourceDataset].getEpochSize(refresh = refresh)
                
            def getNextBatch(self, sourceDataset: 'DataGenerator.Dataset.Types' = None) -> 'np.ndarray':
                '''
                    Generate the next batch of data.  --- UPDATED (Dexter) 20200308

                    Parameters
                    ------------------------------

                    sourceDataset   `DataGenerator.Dataset.Types`  - The type of dataset of a source that's this data is referencing.
                '''
                return self.generatorController.getNextBatch(datasetType = (sourceDataset or self.train.currentSourceDataset))

            def copy(self, node: 'DataPreprocessing.Node.TimeSeries') -> 'DataPreprocessing.Node.TimeSeries':
                """
                    Copy the configuration of this @DataPreprocessing.Node.TimeSeries object to another @DataPreprocessing.Node.TimeSeries object.   --- UPDATED (CYK, Dexter) 20200308

                    Parameters
                    ------------------------------

                    node        `DataPreprocessing.Node.TimeSeries`    - Another @DataPreprocessing.Node.TimeSeries object.
                """
                super().copy(node)
                node.seriesSort = self.seriesSort
                node.seriesLen = self.seriesLen

            def refreshItemShape(self):    
                '''
                    Refresh the item shape, typically after updates of datashape.   --- UPDATED (Dexter) 20190822
                '''
                # Get the shape after column selection.
                inputShape = self.getInputShape()[1:]
                selShape = [self.seriesLen, *inputShape[1:-1], len(Source.Table.getColList(inputShape[-1], self.sourceCol))]

                # Set the item shape.
                self.setItemShape([*selShape])
            
            def _processData(self, data: 'np.ndarray<int|float|str>', step: 'DataPreprocessing.Node.Columns.StepEnum' = StepEnum.Output) -> 'np.ndarray<int|float|str>':
                '''
                    Data pre-processing given data generated from @Source.Config objects with output at this data preprocessing node.   --- UPDATED (Dexter) 20190522

                    Parameters
                    ------------------------------

                    data    `np.ndarray<int|float|str>`           - The source data to get pre-processed from the root @Source.Config objects.

                    step    `DataPreprocessing.Node.Columns.StepEnum`   - The step of data preprocessing within a @DataPreprocessing.Node.Columns object.

                    Returns
                    ------------------------------

                    `np.ndarray<int|float|str>`             - The preprocesssed data of this pre-processing node at the requested step.
                '''
                # Select specific columns.
                selCol = Source.Table.arraySlice(data, self.sourceCol)

                # Returns incoming data if the step request is input.
                if (step == DataPreprocessing.Node.Columns.StepEnum.Input):
                    return selCol

                # Prefetch columns.
                preFetchCols = None

                # If there is transformation, apply each transformation function sequentially on each of the applied column indexes.
                for tx in self.transformations:
                    # Check if there is a need to pre-fetchh columns of the entire dataset.
                    if (tx.requirePreExtraction and not tx.cachedPrefetch):
                        # If it's needed, check if the columns have been extracted. If not, get all the data.
                        if (preFetchCols is None):
                            preFetchCols = self.train.dppNodes[self.key].getData(step = DataPreprocessing.Node.Columns.StepEnum.Input, sourceDataset = DataGenerator.Dataset.Types.Train)
                        
                        # Prefetch necessary information. for this transformation.
                        tx.preExtract(preFetchCols)
                    
                    # Transform the data.
                    selCol[..., IndexRange.toIndexer(tx.colSel)] = np.vectorize(tx.transformTo)(selCol[..., IndexRange.toIndexer(tx.colSel)])
                
                if (step == DataPreprocessing.Node.Columns.StepEnum.Transformed):
                    return selCol

                # Perform circular data range transformations on specific columns.
                for cc in self.circular:
                    minV, maxV, rangeV = cc.min, cc.max, cc.range
                    cirSlicer = IndexRange.toIndexer(cc.colSel)
                    selCol[..., cirSlicer] = np.mod((selCol[..., cirSlicer].astype(float) - minV), rangeV)
                
                if (step == DataPreprocessing.Node.Columns.StepEnum.CircularDataDefined):
                    return selCol

                # Handle the one-hot columns data.
                if (len(self.oneHotColumns) > 0):
                    # List all columns, perform transformation if needed.
                    allCols = []

                    for idx in range(0, len(selCol[0])):
                        if (idx in self.oneHotColumns):
                            newCol = selCol[:,[idx]]
                            newCol = Source.Table.convertCatToNumbers(newCol, True, {v: idx for idx,v in enumerate(self.oneHotColumns[idx])})["data"]
                            allCols.extend(newCol)
                        else:
                            allCols.append(selCol[:,[idx]])
                    
                    # Concatenate all columns.
                    selCol = np.column_stack(allCols)

                # Cast the data type if needed.
                if (self.dtype is not None):
                    selCol = selCol.astype(self.dtype.as_numpy_dtype)
                else:
                    selCol = selCol.astype(float)

                # Return the transformed data.
                return selCol
            
            def recoverToRawData(self, items: 'np.ndarray|list[*+]', revealedIdxs: List[int] = None) -> 'np.ndarray':
                """
                    Recover data items to raw-data format of the predicted data, like undo normalization or transformations, etc. No action taken, and a sub-class should be used.   --- UPDATED (Dexter) 20191024

                    Parameters
                    ------------------------------

                    items       `np.ndarray|list[*+]`   - A list of data items.

                    revealedIdxs    `list<int>`         - The incoming indexes of items.

                    Returns
                    ------------------------------

                    `np.ndarray`   - Recovered data items.
                """
                # Convert to np array for the predicted data.
                items = np.array(items)

                # Get the items index with respect to the input index.
                if revealedIdxs is None:
                    revealedIdxs = [*range(0, items.shape[-1])]
                
                # Get the actual columns selected and displayed in items.
                relativeCols = []

                # Recover from one-hot encoding.
                if (len(self.oneHotColumns) > 0):
                    # Sort all one hot column keys.
                    oneHotKeys = [*self.oneHotColumns.keys()]
                    oneHotKeys.sort()

                    # List all columns, perform transformation if needed.
                    allCols = []
                    pushIdx = 0
                    oriIdx = 0

                    for idx in range(0, len(self.getShape())):
                        if (idx == pushIdx):
                            if (idx in oneHotKeys):
                                # Determine the original data.
                                catSet = self.oneHotColumns[idx]
                                catSize = len(catSet)
                                pushIdx += catSize
                                oriIdx += 1

                                if all([idx in revealedIdxs for i in range(idx, idx+catSize)]):
                                    # Ensure all the one-hot indices have been selected.
                                    catCols = items[...,idx:idx+catSize]
                                    catIdx = np.argmax(catCols, axis=-1)
                                    catDict = {i: v for i,v in enumerate(catSet)}
                                    allCols.append(np.reshape(np.frompyfunc(lambda i: catDict[i], 1, 1)(catIdx), [*catCols.shape[:-1], 1]))
                                    relativeCols.append(oriIdx - 1)
                                else:
                                    raise ValueError("Column selection not covered all one-hot indices cannot perform recovering data.")
                            else:
                                pushIdx += 1
                                oriIdx += 1
                                if (idx in revealedIdxs):
                                    allCols.append(items[:,[idx]])
                                    relativeCols.append(oriIdx - 1)

                    # Concatenate all columns.
                    items = np.column_stack(allCols)

                # Get the selected columns.
                inputShape = self.getInputShape()
                nextRevealedIdx = IndexRange.parse(inputShape[-1], self.sourceCol)

                # Ensure the predicted data is within circular definition range.
                for cc in self.circular:
                    minV, maxV, rangeV = cc.min, cc.max, cc.range
                    cirSlicer = [relativeCols.index(i) for i in IndexRange.parse(len(nextRevealedIdx), cc.colSel) if i in relativeCols]
                    items[..., cirSlicer] = np.mod((items[..., cirSlicer].astype(float) - minV), rangeV)
                
                # Transform the data back to the original distribution.
                if len(self.transformations):
                    # If there is transformation, reverse each transformation function sequentially on each of the applied column indexes.
                    for tx in reversed(self.transformations):
                        # Transform the data.
                        txSlicer = [relativeCols.index(i) for i in IndexRange.parse(len(nextRevealedIdx), tx.colSel) if i in relativeCols]
                        items[..., txSlicer] = np.vectorize(tx.transformFrom)(items[..., txSlicer])
                
                # Recursively get previously recovered data.
                return items if isinstance(self.fromNode, Source.Config) else self.fromNode.recoverToRawData(items, nextRevealedIdx)
            
            def getPrintableItems(self, items, recovered = True):
                '''
                    Print some items into an array, with some further formatting.   --- UPDATED (Dexter) 20190509

                    Parameters
                    ------------------------------

                    items   `list[*+]|np.array[*+]`  - A list of items typically with original input format.

                    recovered   `bool`  - Whether the data is recovered from transformation or not.

                    Returns
                    ------------------------------

                    `list[[str,*]+]`    - A list of items in a single data column with prefix data type column.
                '''
                # Recover the data to a before-transformation state.
                recoveredData = (escapeNaNNPAry(items).tolist() if isinstance(items, np.ndarray) else items) if recovered else self.recoverToRawData(items).tolist()

                # Check if every item is a list.
                isNDAry = isinstance(recoveredData[0], list)

                if (not isNDAry):
                    # If not, it's a single value.
                    dataType = "Value"
                elif len(recoveredData[0]) == 1:
                    # If it is, but there is only one element of each item, it's still a single value.
                    dataType = "Value"
                    recoveredData = [c[0] for c in recoveredData]
                else:
                    # Otherwise, it's a table.
                    dataType = "Table"
                
                # Return the recovered ddata as a pair of datatype and JSON-stringified object.
                return [[dataType, json.dumps(i)] for i in recoveredData]

        class Noise(SourceLike):
            """
                Class representing a series data preprocessing node configuration.   --- UPDATED (CYK, Dexter) 20190505
            """
            class StepEnum(_DataPreprocessingNodeStepEnum):
                '''
                    Enumeration defining the step of data preprocessing within a @DataPreprocessing.Node.Config object.   --- UPDATED (Dexter) 20190129
                '''
                # Data just after series conversion.
                SeriesConverted = 3
 
            class GeneratorDetail(DataGenerator.Detail):
                """
                    Class representing a data generator detail for noise sources.   --- UPDATED (Dexter) 20190713
                """
                def __init__(self, controller: 'Source.Noise.GeneratorController'):
                    """
                        Create a @DataGenerator.Detail object for generating data of csv sources.   --- UPDATED (Dexter) 20190713
                        
                        Parameters
                        ------------------------------

                        controller `Source.Noise.GeneratorController` - The dataset controller maintaining this data generator detail.
                    """
                    super().__init__(controller)
                    # `list<int>` - The shape of the data.
                    self._noiseShape = []
                    # `dict<str, Source.OutputsetInfo>` - Dataset information specifying root information for this dataset.
                    self.outputsetInfo = {}

                def copy(self) -> 'Source.Noise.GeneratorDetail':
                    """
                        Copy this generator detail   --- UPDATED (Dexter) 20191022

                        Returns
                        ------------------------------

                        `Source.Noise.GeneratorDetail` - The copied generator detail object.
                    """
                    # Create a new detail object.
                    newDetail: Source.Noise.GeneratorDetail = Source.Noise.GeneratorDetail(self.controller)

                    # Copy all information.
                    for k in ["_noiseShape", "outputsetInfo"]:
                        setattr(newDetail, k, getattr(self, k))
                    
                    return newDetail

                def getData(self, batchSize: int = 1) -> 'np.ndarray':
                    """
                        Get the data of with specific indexes.   --- UPDATED (Dexter) 20191024

                        Parameters
                        ------------------------------

                        start       `int`       - Batch starting index (inclusive). If `None`, it notates `0`.

                        end         `int`       - Batch ending index (exclusive). If `None`, it notes the epoch size.

                        indexes     `list[int]` - A list of indexes of data rows.

                        Returns
                        ------------------------------

                        `np.ndarray` - The requested source data of this generator detail.
                    """
                    shape = self._noiseShape
                    shape[0] = batchSize
                    return tf.convert_to_tensor(np.random.random(size=shape), dtype=tf.float32)

                def getNextBatch(self) -> 'np.ndarray':
                    """
                        Generate the next batch of data rows of the root source.   --- UPDATED (Dexter) 20190918

                        Returns
                        ------------------------------

                        'np.ndarray' - The next batch data. In the future, the data will be output as `tf.Tensor` objects.
                    """
                    # 1. Get the batch size.
                    batchSize = self.batchSize

                    # 2. Get the batch data.
                    returnData = self.getData(batchSize = batchSize)

                    return returnData

                    class GeneratorController(DataGenerator.Controller):
                        """
                            Class representing a data generator controller for data table sources.   --- UPDATED (Dexter) 20190713
                        """
                        def __init__(self, attachObject: Union['Source.Noise', 'DataPreprocessing.Node.SourceLike']):
                            """
                                Create a @Source.Noise.GeneratorController object for controlling data generator of a table source.   --- UPDATED (Dexter) 20190915

                                Parameters
                                ------------------------------

                                attachObject `(Source.Config|ModelNode.Config)` - The object this data generator controller is embedded with.
                            """
                            super().__init__(attachObject = attachObject)
                        
                        @property
                        def detailType(self) -> Callable[..., 'Source.Noise.GeneratorDetail']:
                            """
                                Get the data generator detail type.   --- UPDATED (Dexter) 20190713

                                Returns
                                ------------------------------

                                `Callable<*, Source.Noise.GeneratorDetail>`    - The type of the generator detail.
                            """
                            return Source.Noise.GeneratorDetail

                    def __init__(self, sourceType = "noise", noiseShape = [None, 1], name: str = ""):
                        '''
                            Create an @Source.Image object, a type of @TrainingSource handling the image data sources.   --- UPDATED (Dexter) 20190814
                            
                            Parameters
                            ------------------------------
                            
                            sourceType      `str`           - The type of image source, only `cifar10`, `cifar100`, `fashion-mnist`, `mnist` is supported currently.

                            coreDataDir     `str`           - The directory of the data folder places.

                            flattenImg      `bool`          - Whether to flatten into 1D when generating the images from the source.
                            
                            name        `str`   - Name of this @Source.Table object.
                        '''
                        # Create the Noise source config.
                        super().__init__(instanceClass = DataPreprocessing.Node.Types.TimeSeries, dtype = tf.float32)
                        # `Source.Types` - The instance class, as defined in @Source.Types .
                        self._instanceClass = Source.Types.Noise
                        # `Source.Noise.GeneratorController` - The data generator controller for this source.
                        self.generatorController = Source.Noise.GeneratorController(self)
                        # `list<int>` - The shape of the data.
                        self._noiseShape = noiseShape
                        
                        # set noise shape
                        self.generatorController[DataGenerator.Dataset.Types.Train]._noiseShape = self._noiseShape
                        self.generatorController[DataGenerator.Dataset.Types.Test]._noiseShape = self._noiseShape

                    def parseJSON(self, obj: Dict[str, Any], train: 'Train'):
                        """
                            Parse a previously saved object into this @Source.Config object.   --- UPDATED (Dexter) 20190713

                            Parameters
                            ------------------------------

                            obj     `dict<str,*>`   - JSON object from Project file.
                            
                            train   `Train`         - The train object this @Source.Config object is attached in.
                        """
                        super().parseJSON(obj, train)
                        
                    def getNextBatch(self, sourceDataset: 'DataGenerator.Dataset.Types' = None) -> 'np.ndarray':
                        '''
                            Generate the next batch of data.  --- UPDATED (Dexter) 20200308

                            Parameters
                            ------------------------------

                            sourceDataset   `DataGenerator.Dataset.Types`  - The type of dataset of a source that's this data is referencing.
                        '''
                        return self.generatorController.getNextBatch(datasetType = (sourceDataset or self.train.currentSourceDataset))

                def getRandItems(self, randomSeed: int = None, randomCount: int = None) -> 'np.ndarray':
                    """Generate the random items of data. No action taken, and a sub-class should be used.   --- UPDATED (Dexter) 20200308

                    Parameters
                    ------------------------------

                    randomSeed      `int`       - Random seed.

                    randomCount     `int`       - The count of random items to collect.

                    Returns
                    ------------------------------

                    'np.ndarray' - Random items of data. In the future, the data will be output as `tf.Tensor` objects.
                    """

                    # 1. Get the batch size.
                    batchSize = self.batchSize

                    # 2. Get the random data.
                    returnData: np.ndarray = self.getData(batchSize = batchSize)

                    # 4. Copy the randomly-indexed items.
                    return returnData#.copy()

            def __init__(self, instanceClass: 'DataPreprocessing.Node.Types' = None, sourceCol: str = "None:None", dtype: 'tf.DType' = None):
                '''
                    Create a configuration of a source-like data preprocessing node.   --- UPDATED (Dexter) 20191019

                    Parameters
                    ------------------------------

                    instanceClass       `DataPreprocessing.Node.Types`   - The instnace class, as defined in @DataPreprocessing.Node.Types .

                    sourceCol   `str`               - An @IndexRange parsable string specifying the column selection (data feature) to be selected from the source data preprocessing node.

                    dtype       `tf.DType`          - The TensorFlow data type of this source column config. If `None`, it will be automatically determined during data pre-processing, and converted to `tf.float32` if it's using as an input of data model.
                '''
                super().__init__(instanceClass = (instanceClass or DataPreprocessing.Node.Types.SourceLike), dtype = dtype, sourceCol = sourceCol)
                # `bool` - Whether the data preprocessing source is splittable, for the purpose of partitioning validation data.
                self.splittable: bool = False
                # `DataPreprocessing.Node.GetDataMode` - The method for getting data from previous nodes or by itself.
                self._getDataMode: DataPreprocessing.Node.GetDataMode = DataPreprocessing.Node.GetDataMode.Current
                # `DataGenerator.Controller` - The data generator controller for this data preprocessing node.
                self.generatorController: DataGenerator.Controller = None

            def getProcessedData(self, rootData: 'Train.RootData|list<*>', step: 'DataPreprocessing.Node.SourceLike.StepEnum' = StepEnum.Output) -> 'list<*>':
                """
                    Virtual method to get the processed data based on a given root data.   --- UPDATED (Dexter) 20200311

                    Parameters
                    ------------------------------

                    rootData `Train.RootData|list<*>` - Root data of the @Train object, or data generated from this source like node.

                    step `DataPreprocessing.Node.SourceLike.StepEnum` - The step of data preprocessing within this @DataPreprocessing.Node.SourceLike object.
                    
                    Returns
                    ------------------------------
                    
                    `list<*>` - The proccessed data.
                """
                # Get raw input data.
                if step == DataPreprocessing.Node.SourceLike.StepEnum.RawInput:
                    return rootData
                
                if isinstance(rootData, Train.RootData):
                    # Return the cached data if needed.
                    if self.key in [*rootData.dppNodes.keys()]:
                        if step == DataPreprocessing.Node.SourceLike.StepEnum.RawInput:
                            return rootData.dppNodes[self.key]
                        elif step != DataPreprocessing.Node.SourceLike.StepEnum.Output:
                            raise ValueError("When data is preprocessed elementally, it cannot be received using intercepted steps.")
                        else:
                            rootData = rootData.dppNodes[self.key]
                    else:
                        raise ValueError("Root Data cannot be found for data preprocessing")

                # Process the data.
                # return self.processData(rootData, step)
                return rootData   # self.processData(rootData, step)

            def getPrintableItems(self, items, recovered = True):
                '''
                    Print some items into an array, with some further formatting.   --- UPDATED (Dexter) 20190922

                    Parameters
                    ------------------------------

                    items   `list[*+]`  - A list of items typically with original input format.

                    recovered   `bool`  - Whether the data is recovered from transformation or not.

                    Return
                    ------------------------------

                    `list[[str,*]+]`    - A list of items in a single data column with prefix data type column.
                '''
                return [["Image", json.dumps(i.tolist())] for i in items]

        class BERT(Config):
            """
                Class representing a BERT preprocessing node configuration.   --- UPDATED (Trista) 20200813
            """
            class StepEnum(_DataPreprocessingNodeStepEnum):
                '''
                    Enumeration defining the step of data preprocessing within a @DataPreprocessing.Node.Config object.   --- UPDATED (Dexter) 20190129
                '''
                # Data just after series conversion.
                SeriesConverted = 3

            class GeneratorDetail(DataGenerator.Detail):
                """A generator detail for time BERT preprocessing node.   --- UPDATED (Trista) 20200813"""
                def __init__(self, controller: 'DataPreprocessing.Node.BERT.GeneratorController'):
                    """
                        Create a @DataPreprocessing.Node.BERT.GeneratorDetail object for generating data of series table sources.   --- UPDATED (CYK, Dexter) 20200308

                        Parameters
                        ------------------------------

                        controller `DataPreprocessing.Node.BERT.GeneratorController` - The dataset controller maintaining this data generator detail.
                    """
                    super().__init__(controller)
                    # `list<int>` - The reference indexes of the full incoming data.
                    self._referenceIndexes: List[int] = []
                    # `np.ndarray<*>` - The cached input data for previous node/sources.
                    self._cacheInput: List[Any] = []
                
                @property
                def epochSize(self) -> int:
                    """
                        Epoch size of the data, i.e. total number of records in the training dataset.   --- UPDATED (Trista) 20200813

                        Returns
                        ------------------------------

                        `int`   - Epoch size of the data, i.e. total number of records in the training dataset.
                    """
                    return (len(self._referenceIndexes) if len(self._referenceIndexes) else None) if self._epochSize is None else self._epochSize

                def getEpochSize(self, refresh: bool = False) -> int:
                    """
                        Get the epoch size of this data preprocessing node using the current source dataset.   --- UPDATED (Trista) 20200813

                        Parameters
                        ------------------------------

                        refresh `bool` - Whether to refresh cached epoch size.

                        Returns
                        ------------------------------

                        `int` - The epoch size of this data preprocessing node using the current source dataset.
                    """
                    return self.epochSize

                def copy(self) -> 'DataPreprocessing.Node.BERT.GeneratorDetail':
                    """
                        Copy this generator detail.   --- (Trista) 20200813

                        Returns
                        ------------------------------

                        `DataPreprocessing.Node.BERT.GeneratorDetail` - The copied generator detail object.
                    """
                    # Create a new detail object.
                    newDetail: DataPreprocessing.Node.BERT.GeneratorDetail = DataPreprocessing.Node.BERT.GeneratorDetail(self.controller)

                    # Copy all information.
                    for k in ["_epochSize", "_oriShape", "_controller", "_batchSize", "_shuffle", "_referenceIndexes"]:
                        setattr(newDetail, k, getattr(self,k))
                    
                    return newDetail
                                    
                def initialize(self):
                    """
                        Intitiate this generator loop.   --- TRAIN --- UPDATED (Trista) 20200813
                    """
                    super().initialize()                    

                    # Initialize for reference indexes.
                    if len(self._referenceIndexes) == 0:
                        # 'DataPreprocessing.Node.BERT' - The dpp node of this generator detail.
                        dppNode: DataPreprocessing.Node.BERT = self.controller.attachObject
                        
                        # Return if there is no data
                        prevEpochSize: int = dppNode.getRootSources()[0].epochSize
                        if prevEpochSize is None:
                            return
                        
                        # Set the reference indexes.
                        self._referenceIndexes = [i for i in range(0, prevEpochSize - (dppNode.seriesLen - 1))]

                        # If there is some specific series label, sort the reference indexes.
                        if dppNode.seriesLabel is not None:
                            # Get the root source.
                            labelIndex: int = dppNode.seriesLabel
                            rootSources: List[DataPreprocessing.Node.Config, Source.Config] = dppNode.getRootSources()

                            # If the root source can be read in memory.
                            if rootSources[0].splittable:
                                # Get all data.
                                data: List[Any] = rootSources[0].getData()

                                # Sort the reference index for different time parser or numerical parser.
                                if dppNode.seriesLabelParser == "n":
                                    self._referenceIndexes.sort(key = lambda dataRowIndex: float(data[dataRowIndex][labelIndex]))
                                else:
                                    self._referenceIndexes.sort(key = lambda dataRowIndex: data[dataRowIndex][labelIndex])
                        
                        # Cache the data if the from node is another data preprocessing node.
                        if isinstance(dppNode.fromNode, DataPreprocessing.Node.Config):
                            # Get the root source.
                            rootSources: List[DataPreprocessing.Node.Config, Source.Config] = dppNode.getRootSources()

                            # If the root source can be read in memory.
                            if rootSources[0].splittable:
                                # Get all data.
                                data: np.ndarray = rootSources[0].getData()
                            
                                # Preprocess the data.
                                self._cacheInput = dppNode.fromNode.processData(data)
                        
                        # Reverse the order if needed.
                        if dppNode.reversed:
                            self._referenceIndexes = [index + dppNode.seriesLen - 1 for index in self._referenceIndexes]                            
                
                def getCacheInput(self, start: int = None, end: int = None, indexes: List[int] = None) -> 'np.ndarray<*>':
                    """
                        Get the cache input data of with specific indexes.   --- (Trista) 20200813

                        Parameters
                        ------------------------------

                        start `int` - Batch starting index (inclusive). If `None`, it notates `0`.

                        end `int` - Batch ending index (exclusive). If `None`, it notes the epoch size.

                        indexes `list<int>` - A list of indexes of data rows.

                        Returns
                        ------------------------------

                        `np.ndarray<*>` - The requested slice of data of the cache input data.
                    """
                    # Translate String "None" into null.
                    if start == "None":
                        start = None
                    if end == "None":
                        end = None
                    
                    # Check the batch start and end to get the data.
                    if start is not None or end is not None:
                        return self._cacheInput[start:end]
                    elif indexes is not None:
                        if any([(idx >= len(self._cacheInput) or idx < -len(self._cacheInput)) for idx in indexes]):
                            raise ValueError("Some of the requested indexes are out of range.")
                        return self._cacheInput[indexes]
                    else:
                        return self._cacheInput[:]
                
                def prepareIteration(self):
                    """
                        Prepare a new round of data source iteration.   --- UPDATED (Trista) 20200813
                    """
                    # 1. Confirm this data generator detail is initialized before any generator yields.
                    if (not self.initialized):
                        self.initialize()

                    # 2. Reset the iteration index.
                    self.batchIdxReset()

                    # 3. Reshuffle the data if needed.
                    if (self.shuffle):
                        self._shuffleData()
                
                def _shuffleData(self):
                    """
                        Shuffle the data.   --- UPDATED (Trista) 20200813
                    """
                    # Shuffle the data in-place.
                    np.random.shuffle(self._referenceIndexes)
                
                def setData(self, inputArray: Union[list] = [], outputArray: None = None, hasHeader: None = None):
                    """
                        Read the data table and set the raw data of this source.   --- UPDATED (Trista) 20200813

                        Parameters
                        ------------------------------
                        
                        inputArray  `list[list[float|int|str]]` - An input data table (training set).

                        outputArray `list[list[float|int|str]]` - An output data table (training set).

                        hasHeader   `bool`  - Whether this table source includes a heading.
                    """
                    # Convert input array into list.
                    referenceIndexes = list(inputArray)

                    # Ensure the reference index is of 1 dimension.
                    if len(np.shape(referenceIndexes)) != 1:
                        raise ValueError("Input Shape must be a 1-dimensional list.")
                    
                    # Ensure there is at least some data.
                    if len(referenceIndexes) == 0:
                        raise ValueError("Some data should be set instead of `None` or array with zero length")
                        
                    # Set the reference indexes.
                    self._referenceIndexes = referenceIndexes

                def getData(self, start: int = None, end: int = None, indexes: int = None) -> 'np.ndarray<*>':
                    """
                        Get the data of with specific indexes.   --- UPDATED (Trista) 20200813

                        Parameters
                        ------------------------------

                        start `int` - Batch starting index (inclusive). If `None`, it notates `0`.

                        end `int` - Batch ending index (exclusive). If `None`, it notes the epoch size.

                        indexes `list<int>` - A list of indexes of data rows.

                        Returns
                        ------------------------------

                        `np.ndarray<*>` - The requested slice of data of the generator detail.
                    """
                    # Confirm this data generator detail is initialized before any generator yields.
                    if not self.initialized:
                        self.initialize()
                    
                    # 'DataPreprocessing.Node.BERT' - The dpp node of this generator detail.
                    dppNode: DataPreprocessing.Node.BERT = self.controller.attachObject

                    # Return if there is no data.
                    if dppNode.getEpochSize(True) is None:
                        return []
                    
                    # Get the root source of this node.
                    rootSources: List[DataPreprocessing.Node.Config, Source.Config] = dppNode.getRootSources()

                    # Translate String "None" into null.
                    if start == "None" or start is None:
                        start = 0
                    if end == "None" or end is None:
                        end = len(self._referenceIndexes)
                    
                    # Get the selected reference indexes.
                    batchIndexes: List[int]
                    if start is not None or end is not None:
                        batchIndexes = self._referenceIndexes[start: end]
                    elif indexes is not None:
                        if any([(idx >= end or idx < -end) for idx in indexes]):
                            raise ValueError("Some of the requested indexes are out of range.")
                        indexes = [(idx if idx >= 0 else (len(self._referenceIndexes) + idx)) for idx in indexes]
                        
                        returnAry: List[Any] = []
                        for index in indexes:
                            returnAry.append(self._referenceIndexes[index])
                        batchIndexes = returnAry
                    else:
                        batchIndexes = self._referenceIndexes[:]
                    
                    # If it's a splittable source (All data can be read within memory).
                    if not rootSources[0].splittable:
                        raise ValueError("Time Series data preprocessing node currenly only supports for splittable data sources.")
                    
                    # Ensure the from node is not an array.
                    if isinstance(dppNode.fromNode, list):
                        raise ValueError("Multiple from node is detected from a time series data source")
                        
                    # Collect the data for eacth time step.
                    stepsData: List[np.ndarray] = []
                    oneStepShape: List[int] = None
                    increment: int = -1 if dppNode.reversed else 1

                    for t in range(0, dppNode.seriesLen):
                        oneStepData: np.ndarray
                        # If there is cache, just collect from it.
                        if self._cacheInput and len(self._cacheInput):
                            oneStepData = self.getCacheInput(indexes = batchIndexes)
                        # Otherwise, collect from the source config.
                        else:
                            oneStepData = dppNode.fromNode.getData(indexes = batchIndexes)
                        
                        # Select specific dimensions.
                        oneStepShape = np.shape(oneStepData) if oneStepShape is None else oneStepShape
                        selectedData: np.ndarray = oneStepData[(slice(None, None),
                            *((IndexRange.toIndexer(dppNode.crops[idx]) if idx in dppNode.crops else slice(None, None)) for idx, v in enumerate(oneStepShape[1:-1])),
                            IndexRange.toIndexer(dppNode.sourceCol))]
                        stepsData.append(selectedData[:, np.newaxis])
                        batchIndexes = [(batchIndex + increment) for batchIndex in batchIndexes]
                    
                    # Return the concatenated matrix.
                    return np.concatenate(stepsData, axis = 1)
                
                def getNextBatch(self) -> 'np.ndarray':
                    """
                        Generate the next batch of data rows of the root source.   --- UPDATED (Trista) 20200813

                        Returns
                        ------------------------------

                        'np.ndarray' - The next batch data. In the future, the data will be output as `tf.Tensor` objects.
                    """
                    # Confirm this data generator detail is initialized before any generator yields.
                    if not self.initialized:
                        self.initialize()

                    # Initiate batch index if needed.
                    if self.batchIdx is None:
                        self.prepareIteration()
                    
                    # Get an index and batch size.
                    i: int = self.batchIdx
                    batchSize: int = self.batchSize

                    # If this batch has touched the end of the epoch, regenerate the data .
                    if ((self.dropRemainder and (i+1)*batchSize > self.epochSize) or ((not self.dropRemainder) and i*batchSize >= self.epochSize)):
                        self.prepareIteration()
                        i = self.batchIdx
                    
                    # Increment the index and return all the data.
                    returnData: np.ndarray = self.getData(start = i*batchSize, end = (i+1)*batchSize)

                    # Get the batch data.
                    self.batchIdxIncrement()

                    return returnData
                
                def getRandItems(self, randomSeed: int = None, randomCount: int = None) -> 'np.ndarray':
                    """
                        Generate the random items of data. No action taken, and a sub-class should be used.   --- (Trista) 20200813

                        Parameters
                        ------------------------------

                        randomSeed      `int`       - Random seed.

                        randomCount     `int`       - The count of random items to collect.

                        Returns
                        ------------------------------

                        'np.ndarray' - Random items of data. In the future, the data will be output as `tf.Tensor` objects.
                    """
                    # Ensure there is original data.
                    if (len(self._referenceIndexes) == 0):
                        return np.array([])

                    # Get random indexing.
                    randIdx: np.ndarray = np.arange(len(self._referenceIndexes))
                    np.random.seed(randomSeed)
                    np.random.shuffle(randIdx)
                    np.random.seed()
                    
                    # Increment the index and return all the data.
                    returnData: np.ndarray = self.getData(indexes=randIdx)

                    # 4. Copy the randomly-indexed items.
                    return returnData.copy()
                
                def partition(self, prop: float = 0.2, shuffle: float = False) -> Tuple['np.ndarray','np.ndarray']:
                    '''
                        Partition current data into 2 parts.   --- UPDATED (Trista) 20200813

                        Parameters
                        ------------------------------

                        prop            `float`     - Proportion of original dataset to be the secondary dataset.

                        shuffle         `bool`      - Whether to shuffle before data splitting.

                        Returns
                        ------------------------------

                        `tuple<'np.ndarray','np.ndarray'>` - Primary and secondary datasets.
                    '''
                    # Confirm this data generator detail is initialized before any generator yields.
                    if not self.initialized:
                        self.initialize()

                    # Check total dataset size
                    dataCount = len(self._referenceIndexes)
                    portion = math.floor(dataCount*prop)
                    indexes = self._referenceIndexes[:]

                    # If needed, shuffle the data.
                    if (shuffle):
                        np.random.shuffle(indexes)

                    # Split the dataset
                    primaryIndexes = indexes[portion:]
                    secondaryIndexes = indexes[:portion]

                    # Assign the splitted datasets
                    return (primaryIndexes, secondaryIndexes)
                
            class GeneratorController(DataGenerator.Controller):
                """
                    Class representing a data generator controller for series data preprocessing nodes.   --- UPDATED (Trista) 20200813
                """
                def __init__(self, attachObject: 'DataPreprocessing.Node.BERT'):
                    """
                        Create a @DataGenerator.Controller object for controlling data generator of a series data preprocessing node-.   --- UPDATED (Dexter) 20191019

                        Parameters
                        ------------------------------

                        attachObject `DataPreprocessing.Node.BERT` - The object this data generator controller is embedded with.
                    """
                    super().__init__(attachObject = attachObject)
                
                @property
                def detailType(self) -> Callable[..., 'DataPreprocessing.Node.BERT.GeneratorDetail']:
                    """
                        Get the data generator detail type.   --- UPDATED (Trista) 20200813

                        Returns
                        ------------------------------

                        `Callable<*, DataPreprocessing.Node.BERT.GeneratorDetail>`    - The type of the generator detail.
                    """
                    return DataPreprocessing.Node.BERT.GeneratorDetail

                def splitValidationDataset(self, validation: float = 0.1, randomFold: bool = False):
                    """
                        Split current data into training and validation datasets.   --- UPDATED (Trista) 20200813
                        
                        Parameters
                        ------------------------------

                        validation `float` - Proportion of original dataset to be as the validation dataset.

                        randomFold `bool` - Whether to perform random sub-sampling validation.
                    """
                    # 0. Check total dataset size
                    trainset = self[DataGenerator.Dataset.Types.Train]
                    rowCount = trainset.epochSize

                    # 1. Consider if random sub-sampling or k-fold is required.
                    # 1A. If random sub-sampling is needed, subsample with random partition.
                    if randomFold:
                        (trainIndexes, valIndexes) = trainset.partition(prop = validation, shuffle = True)
                    
                    # 1B. If k-fold is required, cut the suitable part of the dataset.
                    else:
                        # 1B-1. Determine no. of validation count required.
                        validationCount = math.floor(1/validation)
                        i = self.validationTime % validationCount
                        data = trainset.oriData.copy()[1:] if trainset.hasHeader else trainset.oriData.copy() 

                        # 1B-2. If it's the first time to split and this dataset needs shuffling, shuffle the dataset.
                        if (i == 0 and trainset.shuffle): 
                            np.random.shuffle(data)

                        # 1B-3: Split the dataset by the index.
                        valPortion = math.ceil(rowCount/validationCount)
                        startIdx = i*valPortion
                        endIdx = (i+1)*valPortion
                        trainIndexes = np.vstack((data[:startIdx], data[endIdx:]))
                        valIndexes = data[startIdx: endIdx]
                    
                    # 2. Assign the sliced dataset to the training and validation part
                    self[DataGenerator.Dataset.Types.ValidationTrain].setData(inputArray = trainIndexes)
                    self[DataGenerator.Dataset.Types.Validation].setData(inputArray = valIndexes)

                    # Assign the reference of root sources.
                    rootSources = self.attachObject.getRootSources()
                    for rootSource in rootSources:
                        rootSource.generatorController[DataGenerator.Dataset.Types.Validation] = rootSource.generatorController[DataGenerator.Dataset.Types.Train]
                        rootSource.generatorController[DataGenerator.Dataset.Types.ValidationTrain] = rootSource.generatorController[DataGenerator.Dataset.Types.Train]
                    
                    # 3. Increment the validation time by one
                    self.validationTime += 1
                
                def splitTestDataset(self, test: float = 0.2, shuffle: float = False):
                    """
                        Split original train data into train and test sets.   --- UPDATED (Trista) 20200813

                        Parameters
                        ------------------------------

                        test            `float`     - Proportion of original dataset to be as the test dataset.

                        shuffle         `bool`      - Whether to shuffle before data splitting.
                    """
                    # Get the partiitoned datasets.
                    trainset = self[DataGenerator.Dataset.Types.Train]
                    (trainIndexes, testIndexes) = trainset.partition(prop = test, shuffle = shuffle)
                    
                    # Create the test dataset.
                    self[DataGenerator.Dataset.Types.Train].setData(inputArray = trainIndexes)
                    self[DataGenerator.Dataset.Types.Test].setData(inputArray = testIndexes)

                    # Assign the reference of root sources.
                    rootSources = self.attachObject.getRootSources()
                    for rootSource in rootSources:
                        rootSource.generatorController[DataGenerator.Dataset.Types.Test] = rootSource.generatorController[DataGenerator.Dataset.Types.Train]

            def __init__(self, sourceCol: str = "None:None", dtype: 'tf.DType' = None, source: str = None, order: int = 0):
                '''
                    Create a configuration of a data column.   --- UPDATED (Dexter) 20190522

                    Parameters
                    ------------------------------

                    sourceCol   `str`               - An @IndexRange parsable string specifying the column selection (data feature) to be selected from the source data preprocessing node.

                    dtype       `tf.DType`          - The TensorFlow data type of this source column config. If `None`, it will be automatically determined during data pre-processing, and converted to `tf.float32` if it's using as an input of data model.

                    source      `str`               - The source column config of where this config comes from.   --- DEPRECATED --- MAXVER 1904

                    order       `int`               - Feedforward (Topological) order of aligning all @ConConfig in a data source.   --- DEPRECATED --- MAXVER 1904
                '''
                super().__init__(instanceClass = DataPreprocessing.Node.Types.BERT, sourceCol = sourceCol, dtype = dtype)
                # `dict<int,set<str>>` - The column selections on which columns to convert to one hot encoding.
                self.oneHotColumns: Dict[int, Set[str]] = {}
                # `list<DataPreprocessing.Transformation.Columns.Config>` - The transformation definition information in action order.
                self.transformations: List[DataPreprocessing.Transformation.Columns.Config] = []
                # `list<DataPreprocessing.CircularConfig>` - The cicular definition information in action order.
                self.circular: List[DataPreprocessing.CircularConfig] = []
                # `dict<int,int>` - A map from column index to circular config.
                self._colToCircular: Dict[int, int] = {}
                # `bool` - Whether to preprocess the data in batch. `False` if it's preprocessed using Dataset API in an element-wise manner.   --- RESERVED
                self.preprocessInBatch: bool = True
                # `int` - The class count of all values in this column data preprocessing node.
                self._classCount: int = None
                # `str` - The separator for data string.
                self.separator: str = None
                # `str` - The url of BERT source.
                self.tokenSource: str = None
                # `str` - The url of BERT preprocessing.
                self.preprocessSource: str = None

                # `DataPreprocessing.Node.BERT.GeneratorController` - The data generator controller for this data preprocessing node.
                self.generatorController: DataPreprocessing.Node.BERT.GeneratorController = DataPreprocessing.Node.BERT.GeneratorController(self)

            def getEpochSize(self, refresh: bool = False) -> int:
                """
                    Get the epoch size of this data preprocessing node using the current source dataset.   --- UPDATED (Trista) 20200813

                    Parameters
                    ------------------------------

                    refresh `bool` - Whether to refresh cached epoch size.

                    Returns
                    ------------------------------

                    `int` - The epoch size of this data preprocessing node using the current source dataset.
                """
                return self.generatorController[self.train.currentSourceDataset].getEpochSize(refresh = refresh)

            def getNextBatch(self, sourceDataset: 'DataGenerator.Dataset.Types' = None) -> 'np.ndarray':
                '''
                    Generate the next batch of data.  --- UPDATED (Dexter) 20200308

                    Parameters
                    ------------------------------

                    sourceDataset   `DataGenerator.Dataset.Types`  - The type of dataset of a source that's this data is referencing.
                '''
                return self.generatorController.getNextBatch(datasetType = (sourceDataset or self.train.currentSourceDataset))

            def copy(self, node: 'DataPreprocessing.Node.BERT') -> 'DataPreprocessing.Node.BERT':
                """
                    Copy the configuration of this @DataPreprocessing.Node.BERT object to another @DataPreprocessing.Node.BERT object.   --- UPDATED (Trista) 20200813

                    Parameters
                    ------------------------------

                    node        `DataPreprocessing.Node.BERT`    - Another @DataPreprocessing.Node.BERT object.
                """
                super().copy(node)
                node.oneHotColumns = {idx: set([*oneHotSet]) for idx,oneHotSet in self.oneHotColumns.items()}

            def refreshItemShape(self):    
                '''
                    Refresh the item shape, typically after updates of datashape.   --- UPDATED (Trista) 20200813
                '''
                # Get the shape after column selection.
                # Get the shape after column selection.
                inputShape = self.getInputShape()[1:]
                selShape = [*inputShape[:-1], len(Source.Table.getColList(inputShape[-1], self.sourceCol))]

                # Update the shape if there is onehot columns added.
                if len(self.oneHotColumns):
                    selShape[-1] += sum([max([1, len(oneHotSet)]) for oneHotSet in self.oneHotColumns.values()]) - len(self.oneHotColumns)
                
                # Set the item shape.
                self.setItemShape([*selShape])
            
            def recoverToRawData(self, items: 'np.ndarray|list[*+]', revealedIdxs: List[int] = None) -> 'np.ndarray':
                """
                    Recover data items to raw-data format of the predicted data, like undo normalization or transformations, etc. No action taken, and a sub-class should be used.   --- UPDATED (Trista) 20200813

                    Parameters
                    ------------------------------

                    items       `np.ndarray|list[*+]`   - A list of data items.

                    revealedIdxs    `list<int>`         - The incoming indexes of items.

                    Returns
                    ------------------------------

                    `np.ndarray`   - Recovered data items.
                """
                # Convert to np array for the predicted data.
                items = np.array(items)

                # Get the items index with respect to the input index.
                if revealedIdxs is None:
                    revealedIdxs = [*range(0, items.shape[-1])]
                
                # Get the actual columns selected and displayed in items.
                relativeCols = []

                # Recover from one-hot encoding.
                if (len(self.oneHotColumns) > 0):
                    # Sort all one hot column keys.
                    oneHotKeys = [*self.oneHotColumns.keys()]
                    oneHotKeys.sort()

                    # List all columns, perform transformation if needed.
                    allCols = []
                    pushIdx = 0
                    oriIdx = 0

                    for idx in range(0, len(self.getShape())):
                        if (idx == pushIdx):
                            if (idx in oneHotKeys):
                                # Determine the original data.
                                catSet = self.oneHotColumns[idx]
                                catSize = len(catSet)
                                pushIdx += catSize
                                oriIdx += 1

                                if all([idx in revealedIdxs for i in range(idx, idx+catSize)]):
                                    # Ensure all the one-hot indices have been selected.
                                    catCols = items[...,idx:idx+catSize]
                                    catIdx = np.argmax(catCols, axis=-1)
                                    catDict = {i: v for i,v in enumerate(catSet)}
                                    allCols.append(np.reshape(np.frompyfunc(lambda i: catDict[i], 1, 1)(catIdx), [*catCols.shape[:-1], 1]))
                                    relativeCols.append(oriIdx - 1)
                                else:
                                    raise ValueError("Column selection not covered all one-hot indices cannot perform recovering data.")
                            else:
                                pushIdx += 1
                                oriIdx += 1
                                if (idx in revealedIdxs):
                                    allCols.append(items[:,[idx]])
                                    relativeCols.append(oriIdx - 1)

                    # Concatenate all columns.
                    items = np.column_stack(allCols)

                # Get the selected columns.
                inputShape = self.getInputShape()
                nextRevealedIdx = IndexRange.parse(inputShape[-1], self.sourceCol)

                # Ensure the predicted data is within circular definition range.
                for cc in self.circular:
                    minV, maxV, rangeV = cc.min, cc.max, cc.range
                    cirSlicer = [relativeCols.index(i) for i in IndexRange.parse(len(nextRevealedIdx), cc.colSel) if i in relativeCols]
                    items[..., cirSlicer] = np.mod((items[..., cirSlicer].astype(float) - minV), rangeV)
                
                # Transform the data back to the original distribution.
                if len(self.transformations):
                    # If there is transformation, reverse each transformation function sequentially on each of the applied column indexes.
                    for tx in reversed(self.transformations):
                        # Transform the data.
                        txSlicer = [relativeCols.index(i) for i in IndexRange.parse(len(nextRevealedIdx), tx.colSel) if i in relativeCols]
                        items[..., txSlicer] = np.vectorize(tx.transformFrom)(items[..., txSlicer])
                
                # Recursively get previously recovered data.
                return items if isinstance(self.fromNode, Source.Config) else self.fromNode.recoverToRawData(items, nextRevealedIdx)
            
            def getPrintableItems(self, items, recovered = True):
                '''
                    Print some items into an array, with some further formatting.   --- UPDATED (Trista) 20200813

                    Parameters
                    ------------------------------

                    items   `list[*+]|np.array[*+]`  - A list of items typically with original input format.

                    recovered   `bool`  - Whether the data is recovered from transformation or not.

                    Returns
                    ------------------------------

                    `list[[str,*]+]`    - A list of items in a single data column with prefix data type column.
                '''
                # Recover the data to a before-transformation state.
                recoveredData = (escapeNaNNPAry(items).tolist() if isinstance(items, np.ndarray) else items) if recovered else self.recoverToRawData(items).tolist()

                # Check if every item is a list.
                isNDAry = isinstance(recoveredData[0], list)

                if (not isNDAry):
                    # If not, it's a single value.
                    dataType = "Value"
                elif len(recoveredData[0]) == 1:
                    # If it is, but there is only one element of each item, it's still a single value.
                    dataType = "Value"
                    recoveredData = [c[0] for c in recoveredData]
                else:
                    # Otherwise, it's a table.
                    dataType = "Table"
                
                # Return the recovered ddata as a pair of datatype and JSON-stringified object.
                return [[dataType, json.dumps(i)] for i in recoveredData]

            def getProcessedData(self, rootData: 'Train.RootData', step: 'DataPreprocessing.Node.StepEnum' = _DataPreprocessingNodeStepEnum.Output) -> 'np.ndarray':
                """
                    Virtual method to get the processed data based on a given root data.   --- UPDATED (Trista) 20200813

                    Parameters
                    ------------------------------

                    rootData    `Train.RootData`    - Root data of the @Train object.
                """
                # Return the cached data if needed.
                if (self.key in rootData.dppNodes):
                    if (step != DataPreprocessing.Node.StepEnum.Output):
                        raise ValueError("When data is preprocessed elementally, it cannot be received using intercepted steps.")
                    return rootData.dppNodes[self.key]
                
                # Get the incoming data.
                incomingData = (rootData.sources[self.source] if self.outputset is None else rootData.sources[self.source][self.outputset]) if isinstance(self.source, int) else self.train.dppNodes[self.source].getProcessedData(rootData)

                # Process the data.
                return self.processData(incomingData, step = step)

            def _processData_old(self, data: 'np.ndarray<int|float|str>', step: 'DataPreprocessing.Node.Columns.StepEnum' = StepEnum.Output) -> 'np.ndarray<int|float|str>':
                '''
                    Data pre-processing given data generated from @Source.Config objects with output at this data preprocessing node.   --- UPDATED (Trista) 20200813

                    Parameters
                    ------------------------------

                    data    `np.ndarray<int|float|str>`           - The source data to get pre-processed from the root @Source.Config objects.

                    step    `DataPreprocessing.Node.Columns.StepEnum`   - The step of data preprocessing within a @DataPreprocessing.Node.Columns object.

                    Returns
                    ------------------------------

                    `np.ndarray<int|float|str>`             - The preprocesssed data of this pre-processing node at the requested step.
                '''
                # Select specific columns.
                selCol = Source.Table.arraySlice(data, self.sourceCol)
                shape: List[int] = np.shape(data)
                selCol: np.ndarray = data[(slice(None, None),
                            *((IndexRange.toIndexer(self.crops[idx]) if idx in self.crops else slice(None, None)) for idx, v in enumerate(shape[1:-1])),
                            IndexRange.toIndexer(self.sourceCol))]

                # Returns incoming data if the step request is input.
                if (step == DataPreprocessing.Node.Columns.StepEnum.Input):
                    return selCol

                # Prefetch columns.
                preFetchCols = None

                # If there is transformation, apply each transformation function sequentially on each of the applied column indexes.
                for tx in self.transformations:
                    # Check if there is a need to pre-fetchh columns of the entire dataset.
                    if (tx.requirePreExtraction and not tx.cachedPrefetch):
                        # If it's needed, check if the columns have been extracted. If not, get all the data.
                        if (preFetchCols is None):
                            preFetchCols = self.train.dppNodes[self.key].getData(step = DataPreprocessing.Node.Columns.StepEnum.Input, sourceDataset = DataGenerator.Dataset.Types.Train)
                        
                        # Prefetch necessary information. for this transformation.
                        tx.preExtract(preFetchCols)
                    
                    # Transform the data.
                    selCol[..., IndexRange.toIndexer(tx.colSel)] = np.vectorize(tx.transformTo)(selCol[..., IndexRange.toIndexer(tx.colSel)])
                
                if (step == DataPreprocessing.Node.Columns.StepEnum.Transformed):
                    return selCol

                # sentence list
                separator: str = self.separator
                url: str = self.tokenSource
                sentence_list = []
                text = ''
                if (separator is not None):
                    for idx in range(len(selCol)):
                        # text = separator.join(selCol[idx])
                        sentence_list.append((separator.join(selCol[idx])).split(separator))
                else:
                    sentence_list = list(selCol)

                # token
                import tensorflow_hub as hub
                import tokenization
                max_len = 512
                bert_layer = hub.KerasLayer(url, trainable=True)
                vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()
                do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()
                tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)
                token_ids = []
                tk_ids = []
                masks = []
                encoding = np.zeros(max_len, dtype=np.int32)
                segments = []
                for data in sentence_list:
                    # Convert to tokens
                    tk_ids = ['[CLS]'] + sum([tokenizer.tokenize(sent) + ['[SEP]'] for sent in data], [])
                    tk_ids = tk_ids[:max_len-1] + ['[SEP]'] if len(tk_ids) > max_len else tk_ids
                    # Add masks
                    encoding = np.zeros(max_len, dtype=np.int32)
                    encoding[0:len(tk_ids)] = 1
                    masks.append(encoding)
                    # Add segment
                    encoding = np.zeros(max_len, dtype=np.int32)
                    encoding[tk_ids.index('[SEP]'):len(tk_ids)] = 1
                    segments.append(encoding)
                    # Encode sentence token
                    tk_ids = tokenizer.convert_tokens_to_ids(tk_ids)
                    tk_ids = tk_ids + [0] * (max_len-len(tk_ids))
                    token_ids.append(tk_ids)
                return [np.array(token_ids, dtype=np.int32), np.array(masks, dtype=np.int32), np.array(segments, dtype=np.int32)]   # np.array([token_ids, masks, segments])

                # Cast the data type if needed.
                if (self.dtype is not None):
                    selCol = selCol.astype(self.dtype.as_numpy_dtype)
                else:
                    selCol = selCol.astype(float)

                # Return the transformed data.
                return selCol

            def _processData(self, data: 'np.ndarray<int|float|str>', step: 'DataPreprocessing.Node.Columns.StepEnum' = StepEnum.Output) -> 'np.ndarray<int|float|str>':
                '''
                    Data pre-processing given data generated from @Source.Config objects with output at this data preprocessing node.   --- UPDATED (Trista) 20210906

                    Parameters
                    ------------------------------

                    data    `np.ndarray<int|float|str>`           - The source data to get pre-processed from the root @Source.Config objects.

                    step    `DataPreprocessing.Node.Columns.StepEnum`   - The step of data preprocessing within a @DataPreprocessing.Node.Columns object.

                    Returns
                    ------------------------------

                    `np.ndarray<int|float|str>`             - The preprocesssed data of this pre-processing node at the requested step.
                '''
                # Select specific columns.
                selCol = Source.Table.arraySlice(data, self.sourceCol)
                shape: List[int] = np.shape(data)
                selCol: np.ndarray = data[(slice(None, None),
                            *((IndexRange.toIndexer(self.crops[idx]) if idx in self.crops else slice(None, None)) for idx, v in enumerate(shape[1:-1])),
                            IndexRange.toIndexer(self.sourceCol))]

                # Returns incoming data if the step request is input.
                if (step == DataPreprocessing.Node.Columns.StepEnum.Input):
                    return selCol

                # Prefetch columns.
                preFetchCols = None

                # If there is transformation, apply each transformation function sequentially on each of the applied column indexes.
                for tx in self.transformations:
                    # Check if there is a need to pre-fetchh columns of the entire dataset.
                    if (tx.requirePreExtraction and not tx.cachedPrefetch):
                        # If it's needed, check if the columns have been extracted. If not, get all the data.
                        if (preFetchCols is None):
                            preFetchCols = self.train.dppNodes[self.key].getData(step = DataPreprocessing.Node.Columns.StepEnum.Input, sourceDataset = DataGenerator.Dataset.Types.Train)
                        
                        # Prefetch necessary information. for this transformation.
                        tx.preExtract(preFetchCols)
                    
                    # Transform the data.
                    selCol[..., IndexRange.toIndexer(tx.colSel)] = np.vectorize(tx.transformTo)(selCol[..., IndexRange.toIndexer(tx.colSel)])
                
                if (step == DataPreprocessing.Node.Columns.StepEnum.Transformed):
                    return selCol

                # sentence list
                separator: str = self.separator
                url: str = self.tokenSource
                preprocessUrl: str = self.preprocessSource
                batch_input = []
                # full_text = ''
                for idx in range(len(selCol)):
                    if (separator is not None):
                        batch_input.append((separator.join(selCol[idx])).replace(separator, ' '))
                    else:
                        batch_input.append(' '.join(selCol[idx]))

                # token
                import tensorflow_hub as hub
                import tensorflow_text as text
                # import tokenization
                # max_len = 512
                preprocessor = hub.KerasLayer(preprocessUrl)
                encoder_inputs = preprocessor(batch_input)
                encoder = hub.KerasLayer(url, trainable=True)
                outputs = encoder(encoder_inputs)

                # return [np.array(token_ids, dtype=np.int32), np.array(masks, dtype=np.int32), np.array(segments, dtype=np.int32)]   # np.array([token_ids, masks, segments])
                return outputs["pooled_output"]

    class Transformation:
        '''
            Sub-class containing methods and instance classes on transformation configurations on columns from a @DataPreprocessing.Node.Config object.   --- RESERVED --- UPDATED (Dexter) 20190130
        '''
        @staticmethod
        def createFromJSON(obj:Dict[str, Any]) -> 'DataPreprocessing.Transformation.Config':
            '''
                Parse a previously saved object into a new @DataPreprocessing.Transformation.Config object. This will auto-determine the sub-class of the object, and pass the JSON object to the inner method to continue to parse.   --- UPDATED (Dexter) 20190508
                
                Parameters
                ------------------------------

                obj     `dict<str, *>`   - JSON object.

                Returns
                ------------------------------

                `DataPreprocessing.Transformation.Config`    - A @DataPreprocessing.Transformation.Config object.
            '''
            # Parse this DataPreprocessing.Transformation.Config object. If there is instanceClass attribute, parse the JSON object. Otherwise, it's a legacy DataPreprocessing.Node.Config object.
            if "_instanceClass" in obj:
                instanceClass = getattr(DataPreprocessing.Transformation,DataPreprocessing.Transformation.Types.getName(obj["_instanceClass"]))
                dataTransformationConfig = getattr(instanceClass, instanceClass.Types(obj["_method"]).name)()
                dataTransformationConfig.parseJSON(obj)
                return dataTransformationConfig
            else:
                return obj
        
        class Types(Enumeration):
            '''
                Enumeration defining the instance / sub-class type of a @DataPreprocessing.Transformation.Config object.   --- UPDATED (Dexter) 20190326
            '''
            # `int` - Class representing a column configuration, i.e. preprocessing node of on a table-like source (like @Source.Table object) (Ref: @DataPreprocessing.Transformation.Columns).
            Columns = 1
            # `int` - Class representing an image configuration, i.e. preprocessing node of on a image-like source (like @Source.Image object) (Ref: @DataPreprocessing.Transformation.Image).
            Image = 2
            # `int` - Class representing an image augmentation configuration, i.e. preprocessing node of on a image-like source (like @Source.Image object) (Ref: @DataPreprocessing.Transformation.ImageAugmentation).
            ImageAugmentation = 3
        
        Config = _DataPreprocessingTransformationConfig

        class Columns:
            '''
                Sub-class containing methods and instance classes on transformation configurations on columns from a @DataPreprocessing.Node.Columns object.   --- RESERVED --- UPDATED (Dexter) 20190130
            '''
            class Types(Enumeration):
                '''
                    Enumeration defining the transformation method type of a @DataPreprocessing.Transformation.Columns.Config object.   --- RESERVED --- UPDATED (Dexter) 20190131
                '''
                # `int`- Class representing a normalization configuration to be transformed on columns preprocessing for @Source.Table objects. (Ref: DataPreprocessing.Transformation.Columns.Normalize )
                Normalize = 1
                # `int`- Class representing a logarithm configuration to be transformed on columns preprocessing for @Source.Table objects. (Ref: DataPreprocessing.Transformation.Columns.Log )
                Log = 2
                # `int` - Class representing a exponential configuration to be transformed on columns preprocessing for @Source.Table objects. (Ref: DataPreprocessing.Transformation.Columns.Exponential )
                Exponential = 3
                # `int` - Class representing a classification configuration to be transformed on columns preprocessing for @Source.Table objects. (Ref: DataPreprocessing.Transformation.Columns.Classify )
                Classify = 4
                # `int` - Class representing a power configuration to be transformed on columns preprocessing for @Source.Table objects. (Ref: DataPreprocessing.Transformation.Columns.Power )
                Power = 5
                # `int` - Class representing a missing data configuration to be transformed on columns preprocessing for @Source.Table objects. (Ref: DataPreprocessing.Transformation.Columns.MissingData )
                MissingData = 6
            
            Config = _DataPreprocessingTransformationColumnsConfig
            Normalize = _DataPreprocessingTransformationColumnsNormalize
            Log = _DataPreprocessingTransformationColumnsLog
            Exponential = _DataPreprocessingTransformationColumnsExponential
            Classify = _DataPreprocessingTransformationColumnsClassify
            Power = _DataPreprocessingTransformationColumnsPower
            MissingData = _DataPreprocessingTransformationColumnsMissingData
        
        class Image:
            '''
                Sub-class containing methods and instance classes on transformation configurations on images from a @DataPreprocessing.Node.Image object.   --- UPDATED (Dexter) 20190131
            '''
            class Types(Enumeration):
                '''
                    Enumeration defining the transformation method type of a @DataPreprocessing.Transformation.Image.Config object.   --- RESERVED --- UPDATED (Dexter) 20190131
                '''
                # `int` - Class representing an image transformation configuration to normalize iamges from [0,255] into [0,1]. (Ref: @DataPreprocessing.Transformation.Image.Normalize )
                Normalize = 1
                # `int` - Class representing an image transformation configuration to resize iamges. (Ref: @DataPreprocessing.Transformation.Image.Resize )
                Resize = 2
                # `int` - Class representing an image transformation configuration to crop iamges from the central. (Ref: @DataPreprocessing.Transformation.Image.CentralCrop )
                CentralCrop = 3
                # `int` - Class representing an image transformation configuration to convert RGB image (3-color channels) into grayscale images (1 channel). (Ref: @DataPreprocessing.Transformation.Image.ToGrayscale )
                ToGrayscale = 4
                # `int` - Class representing an image transformation configuration to convert grayscale images (1 channel) into RGB image (3-color channels). (Ref: @DataPreprocessing.Transformation.Image.ToRGB )
                ToRGB = 5
            
            Normalize = _DataPreprocessingTransformationImageNormalize
            Resize = _DataPreprocessingTransformationImageResize
            CentralCrop = _DataPreprocessingTransformationImageCentralCrop
            ToGrayscale = _DataPreprocessingTransformationImageToGrayscale
            ToRGB = _DataPreprocessingTransformationImageToRGB
        
        class ImageAugmentation:
            '''
                Sub-class containing methods and instance classes on augmentation configurations on images from a @DataPreprocessing.Node.Image object.   --- UPDATED (Dexter) 20190131
            '''
            class Types(Enumeration):
                '''
                    Enumeration defining the augmentation method type of a @DataPreprocessing.Transformation.Image.Config object.   --- RESERVED --- UPDATED (Dexter) 20190131
                '''
                # `int`- Sub-class representing an image augmentation configuration for random cropping the size. (Ref: @DataPreprocessing.Transformation.ImageAugmentation.Crop )
                Crop = 1
                # `int` - Sub-class representing an image augmentation configuration for random flipping. (Ref: @DataPreprocessing.Transformation.ImageAugmentation.Flip )
                Flip = 2
                # `int` - Sub-class representing an image augmentation configuration for random rotation. (Ref: @DataPreprocessing.Transformation.ImageAugmentation.Rotate ) --- RESERVED
                Rotate = 3
                # `int` - Sub-class representing an image augmentation configuration for random hue rotation. (Ref: @DataPreprocessing.Transformation.ImageAugmentation.Hue )
                Hue = 4
                # `int` - Sub-class representing an image augmentation configuration for random contrast adjustment. (Ref: @DataPreprocessing.Transformation.ImageAugmentation.Contrast )
                Contrast = 5
                # `int` - Sub-class representing an image augmentation configuration for random saturation adjustment. (Ref: @DataPreprocessing.Transformation.ImageAugmentation.Saturation )
                Saturation = 6
                # `int` - Sub-class representing an image augmentation configuration for random brightness adjustment. (Ref: @DataPreprocessing.Transformation.ImageAugmentation.Brightness )
                Brightness = 7
            
            Config = _DataPreprocessingTransformationImageAugmentationConfig
            Crop = _DataPreprocessingTransformationImageAugmentationCrop
            Flip = _DataPreprocessingTransformationImageAugmentationFlip
            Rotate = _DataPreprocessingTransformationImageAugmentationRotate
            Hue = _DataPreprocessingTransformationImageAugmentationHue
            Contrast = _DataPreprocessingTransformationImageAugmentationContrast
            Saturation = _DataPreprocessingTransformationImageAugmentationSaturation
            Brightness = _DataPreprocessingTransformationImageAugmentationBrightness

    class CircularConfig:
        """
            Class representing a circular definition.   --- UPDATED (Dexter) 20190505

            Parameters
            ------------------------------

            colSel  `str`   - An @IndexRange parsable string specifying the column selection from the source.

            minV    `float` - The minimum value of the circular range (inclusive).

            maxV    `float` - The maximum value of the circular range (exclusive).
        """
        def __init__(self, colSel: str = "None:None", minV: float = 0, maxV: float = 360):
            # `str` - An @IndexRange parsable string specifying the column selection from the source.
            self.colSel = colSel
            # `float` - The minimum value of the circular range (inclusive).
            self.min = minV
            # `float` - The maximum value of the circular range (exclusive).
            self.max = maxV
        
        @property
        def range(self) -> float:
            """
                The range of the circular config.   --- UPDATED (Dexter) 20190509

                Returns
                ------------------------------

                `float`- The range of the circular config.
            """
            return self.max - self.min
        
        def copy(self) -> 'DataPreprocessing.CircularConfig':
            """
                Copy a circular config.   --- UPDATED (Dexter) 20190509

                Returns
                ------------------------------

                `DataPreprocessing.CircularConfig`- Copied circular config.
            """
            return DataPreprocessing.CircularConfig(colSel = self.colSel, minV = self.min, maxV = self.max)
        
        def parseJSON(self, obj: Dict[str, Any]):
            """
                Parse a previously saved object into this class of @DataPreprocessing.CircularConfig.   --- UPDATED (Dexter) 20190505

                Parameters
                ------------------------------

                obj `dict<str,*>`  - A JSON object from Project file.
            """
            # Iterate the object keys and take actions on mapping back to the Train.BuildConfig Class.
            for k,v in obj.items():
                if (k == "cols"):
                    # Old circular info are having .cols attribute instead of .colSel
                    self.colSel = v
                else:
                    setattr(self, k, v)

        @staticmethod
        def createFromJSON(obj: Dict[str, Any]) -> 'DataPreprocessing.CircularConfig':
            """
                Parse a previously saved object into a @DataPreprocessing.CircularConfig object.   --- UPDATED (Dexter) 20190508

                Parameters
                ------------------------------

                obj `dict<str,*>`  - A JSON object from Project file.

                Returns
                ------------------------------

                `DataPreprocessing.CircularConfig` - The newly created @DataPreprocessing.CircularConfig object.
            """
            circularConfig = DataPreprocessing.CircularConfig()
            circularConfig.parseJSON(obj)
            return circularConfig

# Fallback name for old class name.
ColConfig = DataPreprocessing.Node.Columns
class _SourceConfig:
    '''
			Class representing a centralized interface for handling training data source.   --- UPDATED (Dexter) 20180622
    '''
    def __init__(self, name: str = ""):
        '''
			Create a @Source.Config object.   --- UPDATED (Dexter) 20200312

            Parameters
            ------------------------------

            name                `str`               - Name of this @Source.Config .
        '''
        # `Source.Types` - The instance class, as defined in @Source.Types .
        self._instanceClass: Source.Types = Source.Types.Config
        # `str` - The name of this @Source.Config. 
        self.name: str = name
        # `bool` - Whether the data source is splittable, for the purpose of partitioning validation data.
        self.splittable: bool = False
        # `Train` - The @Train object where this @Source.Config belongs to. If this source is just constructed, it is not belonged to any training instance.
        self._train: Train = None
        # `DataGenerator.Controller` - Whether the training loop has been initialized for data generation. `None` for this base class.
        self.generatorController: DataGenerator.Controller = None
        # `dict<str,Source.OutputsetInfo>` - Dataset information specifying root information for multiple datasets in a training source. By default, a "root" key should be available in a single dataset.
        self.outputsetInfo: Dict[str, Source.OutputsetInfo] = {}
        # `list<str>` - A `set` containing the ending data preprocessing node keys that the data generator will propagate for when generating data.
        self._propagateDppNodes: Set[str] = set()
    
    def parseJSON(self, obj: Dict[str, Any], train: 'Train'):
        """
            Abstract method to parse a previously saved object into this @Source.Config object.   --- UPDATED (Dexter) 20190822

            Parameters
            ------------------------------

            obj     `dict<str,*>`   - JSON object from Project file.
            
            train   `Train`         - The train object this @Source.Config object is attached in.
        """
        pass

    @property
    def instanceClass(self) -> 'Source.Types':
        '''
            The instance class of this @Source.Config node, as defined in @Source.Types .   --- UPDATED (Dexter) 20190406

            Returns
            ------------------------------

            `Source.Types`    The instance class of this @Source.Config node, as defined in @Source.Types .
        '''
        return self._instanceClass

    @property
    def sourceID(self) -> int:
        '''
            Get the source id of this source in the parent @Train object.   --- UPDATED (Dexter) 20190129

            Returns
            ------------------------------

            `int` - The source id of this source in the parent @Train object.
        '''
        return self.train.sources.index(self)
    
    @property
    def oriShape(self) -> List[int]:
        '''
            Get the shape of the original (and primary) data of this @Source.Config object.   --- UPDATED (Dexter) 20190914

            Returns
            ------------------------------

            `list<int>` - The shape of the original (and primary) data of this @Source.Config object.
        '''
        return self.generatorController[DataGenerator.Dataset.Types.Train].oriShape
    
    def __len__(self) -> int:
        '''
			Get the epoch size of the current source dataset.   --- UPDATED (Dexter) 20190606

            Returns
            ------------------------------

            `int` - The epoch size
        '''
        return len(self.generatorController[self.train.currentSourceDataset])
    
    @property
    def epochSize(self) -> int:
        """
            Get the epoch size of the current source dataset.   --- UPDATED (Dexter) 20190606

            Returns
            ------------------------------

            `int` - The epoch size
        """
        return self.generatorController[self.train.currentSourceDataset].epochSize
    
    def getEpochSize(self, sourceDataset: 'DataGenerator.Dataset.Types' = None) -> int:
        """
            Get the epoch size of as specific source dataset.   --- UPDATED (Dexter) 20200126

            Parameters
            ------------------------------

            'DataGenerator.Dataset.Types' - A specific source dataset.

            Returns
            ------------------------------

            `int` - The epoch size
        """
        if sourceDataset is None:
            sourceDataset = self.train.currentSourceDataset
        
        return self.generatorController[sourceDataset].epochSize

    @property
    def shuffle(self) -> bool:
        """
            Whether the data source will be shuffled on training.   --- UPDATED (Dexter) 20190612

            Returns
            ------------------------------

            `bool`   - Whether the data source will be shuffled on training.
        """
        if (self.train is None):
            raise ValueError("This source is not attached to a Train object yet. No shuffle can be determined.")

        return self.train.buildConfigs[self.train.buildNo].shuffle

    @property
    def batchSize(self) -> int:
        """
            Batch size when using batched training.   --- UPDATED (Dexter) 20190508

            Returns
            ------------------------------

            `int`   - Batch size when using batched training.
        """
        if (self.train is None):
            raise ValueError("This source is not attached to a Train object yet. No batch size can be determined.")

        return self.train.buildConfigs[self.train.buildNo].batchSize

    @property
    def batchCountPerEpoch(self) -> int:
        """
            Get the number of batches included in one epoch of the current set.   --- UPDATED (Dexter) 20190915

            Returns
            ------------------------------

            `bool` - The number of batches included in one epoch.
        """
        return self.generatorController[self.train.currentSourceDataset].batchCountPerEpoch

    @property
    def testRatio(self) -> int:
        """
            The test data proportion.   --- UPDATED (Dexter) 20190506

            Returns
            ------------------------------

            `int`   - The test data proportion.
        """
        if (self.train is None):
            raise ValueError("This source is not attached to a Train object yet. No test ratio can be determined.")

        return self.train.testRatio

    @property
    def train(self) -> 'Train':
        """
            The @Train object where this @Source.Config belongs to. If this source is just constructed, it is not belonged to any training instance.   --- UPDATED (Dexter) 20190405

            Returns
            ------------------------------

            `Train` - The @Train object where this @Source.Config belongs to.
        """
        return self._train
    
    @train.setter
    def train(self, train: 'Train') -> 'Train':
        """
            Virtual method for setting the @Train object where this @Source.Config belongs to. This method may be overridden in case some sources may need to auto-connect with data preprocessing nodes once it's attached to the @Train object.   --- UPDATED (Dexter) 20191025

            Parameters
            ------------------------------

            train `Train` - The @Train object where this @Source.Config belongs to.
            
            Returns
            ------------------------------

            `Train` - The @Train object where this @Source.Config belongs to.
        """
        # Ensure all existing source are having splittable as this source.
        if any([s.splittable != self.splittable for s in train.sources]):
            raise ValueError("Sources are incompatible within the Train object. All sources should have consistent splittable nature.")

        # Assign reference to the train object.
        self._train = train

        # Assign this source into the source.
        train.sources.append(self)

        # Update if train test type it is not splittable.
        if (not self.splittable):
            train.testDatasetType = DataGenerator.Dataset.TestSource.Assign
            for buildConfig in train.buildConfigs:
                buildConfig.crossValidationType = None

        return self._train
    
    @property
    def propagateDppNodes(self) -> Dict[str, 'DataPreprocessing.Node.Config']:
        """
            Get a `dict` object containing the ending data preprocessing nodes that the data generator will propagate for when generating data.   --- UPDATED (Dexter) 20190922

            Returns
            ------------------------------

            `dict<str, DataPreprocessing.Node.Config>` - The ending data preprocessing nodes that the data generator will propagate for when generating data.
        """
        return {dppKey: self.train.dppNodes[dppKey] for dppKey in self._propagateDppNodes}

    def addPropagateDppNodes(self, dppKey: str):
        """
            Add a propagate data preprocessing node.   --- UPDATED (Dexter) 20200312

            Parameters
            ------------------------------

            dppKey `str` - The requested data preprocessing key.
        """
        self._propagateDppNodes.add(dppKey)

    def removePropagateDppNodes(self, dppKey: str):
        """
            Remove a propagate data preprocessing node.   --- UPDATED (Dexter) 20190922

            Parameters
            ------------------------------

            dppKey `str` - The requested data preprocessing key.
        """
        if (dppKey in self._propagateDppNodes):
            self._propagateDppNodes.remove(dppKey)

    def setOneWayReferenceToTrain(self, train: 'Train') -> 'Train':
        """
            Virtual method for referencing the @Train object from the @Source.Config object, typically used in mirrored validation / test sources. This will not append the source into the @Train object.  --- UPDATED (Dexter) 20190506

            Parameters
            ------------------------------

            train `Train` - The @Train object where this @Source.Config belongs to.
            
            Returns
            ------------------------------

            `Train` - The @Train object where this @Source.Config belongs to.
        """
        # Assign reference to the train object.
        self._train = train

        # Prepare the iteration of this Source.Config.
        self.prepareIteration()

        return self._train

    def getNextBatch(self, sourceDataset: 'DataGenerator.Dataset.Types' = None) -> Any:
        '''
			Generate the next batch of data of the required source dataset.   --- UPDATED (Dexter) 20190606

            Parameters
            ------------------------------

            sourceDataset   `DataGenerator.Dataset.Types`  - The type of dataset of a source that's this data is referencing.

            Returns
            ------------------------------

            `*`    - Next batch of data.
        '''
        return self.generatorController[sourceDataset or self.train.currentSourceDataset].getNextBatch()
    
    def getData(self, sourceDataset: 'DataGenerator.Dataset.Types' = None, start: int = None, end: int = None, batchIndexes: List[int] = None) -> Any:
        '''
			Get the data with specific indexes of the required source dataset.   --- UPDATED (Dexter) 20190606

            Parameters
            ------------------------------

            sourceDataset   `DataGenerator.Dataset.Types`  - The type of dataset of a source that's this data is referencing.

            start  `int`       - Batch starting index (inclusive). If `None`, it notates `0`.

            end    `int`       - Batch ending index (exclusive). If `None`, it notes the column count.

            batchIndexes    `list[int]` - A list of indexes of data rows.

            Returns
            ------------------------------

            `*`    - Requested data.
        '''
        return self.generatorController[sourceDataset or self.train.currentSourceDataset].getData(start, end, batchIndexes)
    
    def getRandItems(self, sourceDataset: 'DataGenerator.Dataset.Types' = None, randomSeed: int = None, randomCount: int = None):
        '''
			Get random items of the required source dataset.   --- UPDATED (Dexter) 20190606

            Parameters
            ------------------------------

            sourceDataset   `DataGenerator.Dataset.Types`  - The type of dataset of a source that's this data is referencing.

            randomSeed      `int`       - Random seed.

            randomCount     `int`       - The count of random items to collect.

            Returns
            ------------------------------

            `*`    - Random batch of data.
        '''
        return self.generatorController[sourceDataset or self.train.currentSourceDataset].getRandItems(randomSeed, randomCount)
    
    def recoverToRawData(self, items):
        '''
			Abstract method for recovering data items to raw-data format of the predicted data, like undo normalization or transformations, etc. No action taken, and a sub-class should be used.   --- UPDATED (Dexter) 20190509

            Parameters
            ------------------------------

            items       `np.ndarray|list[*+]`   - A list of data items.
        '''
        pass

    def getPrintableItems(self, dppKey, items, recovered = True):
        '''
			Abstract method for printing some items into an array, with some further formatting. No action taken, and a sub-class should be used.   --- UPDATED (Dexter) 20180717

            Parameters
            ------------------------------

            dppKey  `str`       - The key for a column configuration.

            items   `list[*+]`   - A list of items typically with original input format.

            recovered   `bool`  - Whether the data is recovered from transformation or not.
        '''
        pass
    
    def prepareIteration(self, sourceDataset: 'DataGenerator.Dataset.Types' = None):
        '''
			Prepare a new iteration of data of the required source dataset, typically re-initialize the iteration configurations.   --- UPDATED (Dexter) 20190606

            Parameters
            ------------------------------

            sourceDataset   `DataGenerator.Dataset.Types`  - The type of dataset of a source that's this data is referencing.
        '''
        self.generatorController[sourceDataset or self.train.currentSourceDataset].prepareIteration()
    
    def splitValidationDataset(self, validation: float = 0.1, randomFold: float = False):
        '''
			Virtual method for splitting current data into training and validation sets. No action taken, and a sub-class should be used.   --- UPDATED (Dexter) 20200308

            Parameters
            ------------------------------

            validation      `float`     - Proportion of original dataset to be as the validation dataset.

            randomFold         `bool`      - Whether to shuffle before data splitting.
        '''
        # Raise error if this is not a splittable data source.
        if not self.splittable:
            raise ValueError("This source cannot be split. Please consider to use seperate sources if you need to perform actions like cross validation.")
    
    def splitTestDataset(self, test: float = 0.2, shuffle: float = False) -> 'Source.Config':
        '''
			Split current data into training and test sets, with this source as the training set. No action taken, and a sub-class should be used.   --- UPDATED (Dexter) 20200312

            Parameters
            ------------------------------

            test            `float`     - Proportion of original dataset to be as the test dataset.

            shuffle         `bool`      - Whether to shuffle before data splitting.

            Returns
            ------------------------------

            `Source.Config`   - The test set.
        '''
        # Raise error if this is not a splittable data source.
        if not self.splittable:
            raise ValueError("This source cannot be split. Please consider to use seperate sources if you need to perform actions like cross validation.")
        
        self.generatorController.splitTestDataset(test, shuffle)
    
    def getLocations(self) -> List[str]:
        """
            Get all the location that this source will reference for.   --- UPDATED (Dexter) 20190727

            Returns
            ------------------------------

            `list<str>` - A list of paths that this sources is referencing on.
        """
        return self.generatorController.getLocations()
    
    def getShape(self, outputsetKey: str) -> List[int]:
        """ 
            Vitual method to get the output shape of the root data of this source object object.   --- UPDATED (Dexter) 20190515

            Parameters
            ------------------------------

            outputsetKey    `str` - The key for a outputset included in this source object .

            Returns
            ------------------------------

            `list<int>`     - The output shape.
        """
        return self.outputsetInfo[outputsetKey].shape if outputsetKey in self.outputsetInfo else [None, *self.oriShape[1:]]
    
    def getHeader(self, outputsetKey: str) -> List[str]:
        """
            Vitual method to get the header names of this source.   --- UPDATED (Dexter) 20190506

            Parameters
            ------------------------------

            outputsetKey    `str` - The key for a outputset included in this source object .

            Returns
            ------------------------------

            `list<str>`     - A list of header names.
        """
        return self.outputsetInfo[outputsetKey].header if outputsetKey in self.outputsetInfo else []

    def close(self, sourceDataset: 'DataGenerator.Dataset.Types' = None):
        '''
			End a life cycle of using this data source.   --- UPDATED (Dexter) 20190914

            Parameters
            ------------------------------

            sourceDataset   `DataGenerator.Dataset.Types`  - The type of dataset of a source that's this data is referencing.
        '''
        self.generatorController.close(sourceDataset or self.train.currentSourceDataset)
    
    @staticmethod
    def convertDType(npdtype):
        '''
			Convert NumPy data type object into TensorFlow datatype object.   --- UPDATED (Dexter) 20190128

            Parameters
            ------------------------------

            npdtype         `np.dtype`  - A NumPy data type object.

            Returns
            ------------------------------

            `tf.DType`      - A TensorFlow data type object
        '''
        if (npdtype == np.dtype('float64')):
            return tf.float64
        elif (npdtype == np.dtype('float32')):
            return tf.float32
        elif (npdtype == np.dtype('int64')):
            return tf.int64
        elif (npdtype == np.dtype('int32')):
            return tf.int32
        elif (npdtype.kind == "U"):
            return tf.string
        else:
            return tf.variant
    
    @staticmethod
    def getDataType(string: str) -> 'tf.DType':
        '''
			Get a TensorFlow data type from a string.   --- UPDATED (Dexter) 20190128

            Parameters
            ------------------------------

            npdtype         `np.dtype`  - A NumPy data type object.

            Returns
            ------------------------------

            `tf.DType`      - A TensorFlow data type object
        '''
        return {"tf.float32": tf.float32, "tf.int64": tf.int64}[string]

class _SourceTable(_SourceConfig):
    '''
        Class representing a data source in table format, usually rows of data records with attributes as columns.   --- UPDATED (Dexter) 20180622 
    '''
    class GeneratorDetail(DataGenerator.Detail):
        """
            Class representing a data generator detail for data table sources.   --- UPDATED (Dexter) 20190713
        """
        def __init__(self, controller: 'Source.Table.GeneratorController'):
            """
                Create a @Source.Table.GeneratorDetail object for generating data of table sources.   --- UPDATED (Dexter) 20190706
                
                Parameters
                ------------------------------

                controller `Source.Table.GeneratorController` - The dataset controller maintaining this data generator detail.
            """
            super().__init__(controller)
            # `np.ndarray` - The original table data of at least 2 dimensions.
            self._oriData: np.ndarray = None
            # `bool` - Whether the table has heading. To be defined when data is read.
            self._hasHeader: bool = False

        def copy(self) -> 'Source.Table.GeneratorDetail':
            """
                Copy this generator detail.   --- UPDATED (Dexter) 20191022

                Returns
                ------------------------------

                `Source.Table.GeneratorDetail` - The copied generator detail object.
            """
            # Create a new detail object.
            newDetail: Source.Table.GeneratorDetail = Source.Table.GeneratorDetail(self.controller)

            # Copy all information.
            for k in ["_epochSize", "_oriShape", "_controller", "_batchSize", "_shuffle", "_oriData", "_itrCount", "_hasHeader"]:
                setattr(newDetail, k, getattr(self, k))
            
            return newDetail

        @property
        def oriData(self) -> 'np.ndarray':
            """
                Get the original data of this table source.   --- UPDATED (Dexter) 20190606

                Returns
                ------------------------------

                `np.ndarray<np.ndarray<int|float|str>>` - The original source data.
            """
            return self._oriData
        
        @property
        def epochSize(self) -> int:
            """
                Epoch size of the data, i.e. total number of records in the training dataset.   --- UPDATED (Dexter) 20190921

                Returns
                ------------------------------

                `int`   - Epoch size of the data, i.e. total number of records in the training dataset.
            """
            return (self._oriShape[0] if len(self._oriShape) else None) if self._epochSize is None else self._epochSize

        @property
        def hasHeader(self) -> bool:
            '''
                Return whether this table source include a heading.   --- UPDATED (Dexter) 20190612

                Returns
                ------------------------------

                `bool` - Whether this data table has heading.
            '''
            return self._hasHeader
        
        def setData(self, inputArray: Union[list,'np.ndarray'] = [], outputArray: Union[list,'np.ndarray'] = [], hasHeader: bool = True):
            """
                Read the data table and set the raw data of this source.   --- UPDATED (Dexter) 20191024

                Parameters
                ------------------------------
                
                inputArray  `list[list[float|int|str]]` - An input data table (training set).

                outputArray `list[list[float|int|str]]` - An output data table (training set).

                hasHeader   `bool`  - Whether this table source includes a heading.
            """
            # Set the data table.
            self._hasHeader = hasHeader

            # Convert input array into NumPy array.
            inp = np.array(inputArray)
            out = np.array(outputArray)

            # Ensure the input arrray is of 2 dimension.
            if len(inp.shape) != 2:
                raise ValueError("Input Shape must be a 2-dimensional array.")
            elif len(out.shape) != 2 and len(out) != 0:
                raise ValueError("Output Shape must be a 2-dimensional array.")

            # Stack the input and output array.
            allData = np.column_stack((inp, out)) if (len(out) == len(inp) and len(inp) > 0) else inp

            # Get the data shape.
            shape = [*(allData[1:].shape if hasHeader else allData.shape)] if len(allData) else [0]
            self._oriShape = shape 
            
            # Ensure there is at least some data.
            if (len(allData) == 0):
                raise ValueError("Some data should be set instead of `None` or array with zero length")

            # If it's replacing old data, ensure the shape is the same (except the batch dimension).
            elif ((self.oriData is not None and len(self.oriData) > 0) and allData.shape[1:] != self.oriData.shape[1:]):
                raise ValueError("Data cannot be reset with a different shape")

            # Noted `self.epochSize` is overridden such that there is no ._epochSize available for table data sources.
            # `np.ndarray<np.ndarray<int|float|str>>` - The original source data
            self._oriData = allData if len(allData) else np.array([])

            # Auto handling data preprocessing column selections if there is output array given.
            if (len(out)):
                inputColCount = len(inp[0]) if inp.shape[-1] else 0
                outputColCount = len(out[0]) if out.shape[-1] else 0
                inputCol = ("None:" + str(inputColCount)) if inputColCount > 0 else None
                targetCol = (str(inputColCount) + ":None") if outputColCount > 0 else None
                if (len(inp)):
                    inputDppNode = DataPreprocessing.Node.Columns(sourceCol=inputCol)
                    inputDppNode.appendOn(self.attachObject, "input")
                if (len(out)):
                    outputDppNode = DataPreprocessing.Node.Columns(sourceCol=targetCol)
                    outputDppNode.appendOn(self.attachObject, "target")

        def prepareIteration(self):
            '''
			    Prepare a new round of data source iteration.   --- UPDATED (Dexter) 20190713
            '''
            # 1. Confirm this data generator detail is initialized before any generator yields.
            if (not self.initialized):
                self.initialize()

            # 2. Reset the iteration index.
            self.batchIdxReset()

            # 3. Reshuffle the data if needed.
            if (self.shuffle):
                self._shuffleData()
        
        def _shuffleData(self):
            """
                Shuffle the data.   --- UPDATED (Dexter) 20190208
            """
            # Shuffle the data in-place.
            np.random.shuffle(self.oriData[1:] if self.hasHeader else self.oriData)
        
        def getHeader(self, outputsetKey: str = None):
            '''
                Get the header names of this table source.   --- UPDATED (Dexter) 20191024
                
                Paramters
                ------------------------------

                outputsetKey    `str` - The key for a outputset included in this source object .

                Returns
                ------------------------------

                `list<str>`     - A list of header names.
            '''
            if (outputsetKey is not None and outputsetKey != "table"):
                raise ValueError("outputsetKey is not supported for Table Source.")
        
            if not self.hasHeader or self.oriData is None or len(self.oriData) == 0:
                # Fill with default column grid id.
                header = [str(chr(ord('A') + idx)) for idx in range(0, self.oriShape[-1])]
            elif self.hasHeader and (self.oriData is not None and len(self.oriData) > 0):
                # Otherwise, return the first row as the header.
                header = self.oriData[0]

            return header   

        def getData(self, start: int = None, end: int = None, indexes: List[int] = None) -> 'np.ndarray':
            """
                Get the data of with specific indexes.   --- UPDATED (Dexter) 20191024

                Parameters
                ------------------------------

                start       `int`       - Batch starting index (inclusive). If `None`, it notates `0`.

                end         `int`       - Batch ending index (exclusive). If `None`, it notes the epoch size.

                indexes     `list[int]` - A list of indexes of data rows.

                Returns
                ------------------------------

                `np.ndarray` - The requested source data of this generator detail.
            """
            # 1) Ensure there is original data.
            if (self.oriData is None or len(self.oriData) == 0):
                return np.array([])
            
            # 2) Make a copy of the source data.
            sourceData = (self.oriData[1:] if self.hasHeader else self.oriData).copy()

            # 3) Check the batch start and end to get the data.
            if start is not None or end is not None:
                return sourceData[start:end]
            elif indexes is not None:
                if any([(idx >= len(sourceData) or idx < -len(sourceData)) for idx in indexes]):
                    raise ValueError("Some of the requested indexes are out of range.")
                return sourceData[indexes]
            else: 
                return sourceData

        def getNextBatch(self) -> 'np.ndarray':
            """
                Generate the next batch of data rows of the root source.   --- UPDATED (Dexter) 20190918

                Returns
                ------------------------------

                'np.ndarray' - The next batch data. In the future, the data will be output as `tf.Tensor` objects.
            """
            # 1. Confirm this data generator detail is initialized before any generator yields.
            if (not self.initialized):
                self.initialize()
                self.prepareIteration()

            # 2. Get an index and batch size.
            i = self.batchIdx
            batchSize = self.batchSize

            # 3. If this batch has touched the end of the epoch, regenerate the data .
            if ((self.dropRemainder and (i+1)*batchSize > self.epochSize) or ((not self.dropRemainder) and i*batchSize >= self.epochSize)):
                self.prepareIteration()
                i = self.batchIdx
            
            # 4. Increment the index and return all the data.
            returnData = self.getData(start = i*batchSize, end = (i+1)*batchSize)

            # 5. Get the batch data.
            self.batchIdxIncrement()

            return returnData
                
        def getRandItems(self, randomSeed: int = None, randomCount: int = None) -> 'np.ndarray':
            """
                Generate the random items of data. No action taken, and a sub-class should be used.   --- UPDATED (Dexter) 20191024

                Parameters
                ------------------------------

                randomSeed      `int`       - Random seed.

                randomCount     `int`       - The count of random items to collect.

                Returns
                ------------------------------

                'np.ndarray' - Random items of data. In the future, the data will be output as `tf.Tensor` objects.
            """
            # 1. Ensure there is original data.
            if (self.oriData is None or len(self.oriData) == 0):
                return np.array([])

            # 2. Access to the source data.
            sourceData = (self.oriData[1:] if self.hasHeader else self.oriData)

            # 3. Get random indexing.
            randIdx = np.arange(len(sourceData))
            np.random.seed(randomSeed)
            np.random.shuffle(randIdx)
            np.random.seed()
            
            # 4. Copy the randomly-indexed items.
            return sourceData[list(randIdx[:randomCount])].copy()
        
        def partition(self, prop: float = 0.2, shuffle: float = False) -> Tuple['np.ndarray','np.ndarray']:
            '''
                Partition current data into 2 parts.   --- UPDATED (Dexter) 20190708

                Parameters
                ------------------------------

                prop            `float`     - Proportion of original dataset to be the secondary dataset.

                shuffle         `bool`      - Whether to shuffle before data splitting.

                Returns
                ------------------------------

                `tuple<'np.ndarray','np.ndarray'>` - Primary and secondary datasets.
            '''
            # 0. Check total dataset size
            rowCount = self.epochSize
            portion = math.floor(rowCount*prop)
            data = self.oriData.copy()[1:] if self.hasHeader else self.oriData.copy() 

            # 1. If needed, shuffle the data.
            if (shuffle):
                np.random.shuffle(data)

            # 2. Split the dataset
            primaryData = data[portion:]
            secondaryData = data[:portion]

            # 3. Append the heading.
            if (self.hasHeader):
                primaryData = np.vstack((self.oriData[[0]], primaryData))
                secondaryData = np.vstack((self.oriData[[0]], secondaryData))

            # 4. Assign the splitted datasets
            return (primaryData, secondaryData)
            
    class GeneratorController(DataGenerator.Controller):
        """
            Class representing a data generator controller for data table sources.   --- UPDATED (Dexter) 20190713
        """
        def __init__(self, attachObject: Union['Source.Table', 'DataPreprocessing.Node.SourceLike']):
            """
                Create a @DataGenerator.Controller object for controlling data generator of a table source.   --- UPDATED (Dexter) 20190915

                Parameters
                ------------------------------

                attachObject `(Source.Config|DataPreprocessing.Node.SourceLike)` - The object this data generator controller is embedded with.
            """
            super().__init__(attachObject = attachObject)
        
        @property
        def detailType(self) -> Callable[..., 'Source.Table.GeneratorDetail']:
            """
                Get the data generator detail type.   --- UPDATED (Dexter) 20190713

                Returns
                ------------------------------

                `Callable<*, Source.Table.GeneratorDetail>`    - The type of the generator detail.
            """
            return Source.Table.GeneratorDetail

        def splitValidationDataset(self, validation: float = 0.1, randomFold: bool = False):
            """
                Split current data into training and validation datasets.   --- UPDATED (Dexter) 20190918
            
                Parameters
                ------------------------------

                validation      `float`     - Proportion of original dataset to be as the validation dataset.

                randomFold `bool` - Whether to perform random sub-sampling validation.
            """
            # 0. Check total dataset size
            trainset = self[DataGenerator.Dataset.Types.Train]
            rowCount = trainset.epochSize

            # 1. Consider if random sub-sampling or k-fold is required.
            # 1A. If random sub-sampling is needed, subsample with random partition.
            if randomFold:
                (trainData, valData) = trainset.partition(prop = validation, shuffle = True)
            
            # 1B. If k-fold is required, cut the suitable part of the dataset.
            else:
                # 1B-1. Determine no. of validation count required.
                validationCount = math.floor(1/validation)
                i = self.validationTime % validationCount
                data = trainset.oriData.copy()[1:] if trainset.hasHeader else trainset.oriData.copy() 

                # 1B-2. If it's the first time to split and this dataset needs shuffling, shuffle the dataset.
                if (i == 0 and trainset.shuffle): 
                    np.random.shuffle(data)

                # 1B-3: Split the dataset by the index.
                valPortion = math.ceil(rowCount/validationCount)
                startIdx = i*valPortion
                endIdx = (i+1)*valPortion
                trainData = np.vstack((data[:startIdx], data[endIdx:]))
                valData = data[startIdx: endIdx]

                # 1B-4. Append the heading.
                if (trainset.hasHeader):
                    trainData = np.vstack((trainset.oriData[[0]], trainData))
                    valData = np.vstack((trainset.oriData[[0]], valData))
            
            # 2. Assign the sliced dataset to the training and validation part
            self[DataGenerator.Dataset.Types.ValidationTrain].setData(inputArray = trainData, hasHeader = trainset.hasHeader)
            self[DataGenerator.Dataset.Types.Validation].setData(inputArray = valData, hasHeader = trainset.hasHeader)
            
            # 3. Increment the validation time by one
            self.validationTime += 1
        
        def splitTestDataset(self, test: float = 0.2, shuffle: float = False):
            """
                Split original train data into train and test sets.   --- UPDATED (Dexter) 20190708

                Parameters
                ------------------------------

                test            `float`     - Proportion of original dataset to be as the test dataset.

                shuffle         `bool`      - Whether to shuffle before data splitting.
            """
            # Get the partiitoned datasets.
            trainset = self[DataGenerator.Dataset.Types.Train]
            (trainData, testData) = trainset.partition(prop = test, shuffle = shuffle)
            
            # Create the test dataset.
            self[DataGenerator.Dataset.Types.Train].setData(inputArray = trainData, hasHeader = trainset.hasHeader)
            self[DataGenerator.Dataset.Types.Test].setData(inputArray = testData, hasHeader = trainset.hasHeader)
    
    def __init__(self, inputArray: List[List[Union[float, int, str]]] = [], outputArray: List[List[Union[float, int, str]]] = [], hasHeader: bool = False, name: str = ""):
        '''
			Create a @Source.Table object, a two-dimensional table.   --- UPDATED (Dexter) 20190713

            Parameters
            ------------------------------

            inputArray  `list[list[float|int|str]]` - An input data table (training set).

            outputArray `list[list[float|int|str]]` - An output data table (training set).

            hasHeader   `bool`  - Whether the constructor parameter table source include a heading.
            
            name        `str`   - Name of this @Source.Table object.
        '''
        # Create the table source config.
        super().__init__(name = name)
        # `bool` - Whether the data source is splittable, for the purpose of partitioning validation data.
        self.splittable = True
        # `Source.Types` - The instance class, as defined in @Source.Types .
        self._instanceClass = Source.Types.Table
        # `Source.Table.GeneratorController` - Whether the training loop has been initialized for data generation.
        self.generatorController = Source.Table.GeneratorController(self)
        
        # If there is given tables, set it as the trainset.
        if (len(inputArray) or len(outputArray)):
            self.generatorController[DataGenerator.Dataset.Types.Train].setData(inputArray = inputArray, outputArray = outputArray, hasHeader = hasHeader)

    def parseJSON(self, obj: Dict[str, Any], train: 'Train'):
        """
            Parse a previously saved object into this @Source.Config object.   --- UPDATED (Dexter) 20200315

            Parameters
            ------------------------------

            obj     `dict<str,*>`   - JSON object from Project file.
            
            train   `Train`         - The train object this @Source.Config object is attached in.
        """
        # Parsing old style sources.
        if "generatorController" not in obj:
            # Iterate the object keys and take actions on mapping back to the Source.Config Class.
            for k in Train.getPrioritizedKeys(obj, ["encoding", "_hasHeader"]):
                v = obj[k]
                if (k == "_path"):
                    # If there is ._path in this level, it's setting up the train level path.
                    self.generatorController[DataGenerator.Dataset.Types.Train].path = v
                
                elif (k == "_hasHeader"):
                    for detail in self.generatorController.values():
                        # Set the hasHeader to the generator detail level.
                        detail._hasHeader = v
                
                elif (k == "encoding"):
                    for detail in self.generatorController.values():
                        # Set the encoding to the generator detail level.
                        detail.encoding = v
                    
                elif (k == "outputsetInfo"):
                    # Get outputsetInfo as a dictionary.
                    setattr(self, k, {ok: ov for (ok, ov) in v})

                elif (k == "_propagateDppNodes"):
                    setattr(self, k, set(v))

                elif (k not in ["_testRatio", "_hasHeader", "batchSize", "colConfigs", "shuffle", "epochSize", "dppNodes", "_type","_train","train","_instanceClass"]):
                    # For other properties, just directly assign the values.
                    setattr(self, k, v)
        else:
            # Iterate the object keys and take actions on mapping back to the Source.Config Class.
            for k,v in obj.items():
                if (k == "generatorController"):
                    # Parse generator controllers.
                    self.generatorController.parseFromJSON(v)
                
                elif (k == "outputsetInfo"):
                    # Get outputsetInfo as a dictionary.
                    setattr(self, k, {ok: ov for (ok, ov) in v})

                elif (k not in ["_testRatio", "_hasHeader", "batchSize", "colConfigs", "shuffle", "epochSize", "dppNodes", "_type","_train","train","_instanceClass"]):
                    # For other properties, just directly assign the values.
                    setattr(self, k, v)
            
        # Assign the train.
        self.train = train

    @property
    def hasHeader(self) -> bool:
        """
            Get whether the current dataset has heading.   --- UPDATED (Dexter) 20191002

            Returns
            ------------------------------

            `bool` - Whether the current dataset has heading.
        """
        return self.generatorController[self.train.currentSourceDataset].hasHeader

    def setData(self, data: 'np.ndarray', sourceDataset: 'DataGenerator.Dataset.Types' = None, hasHeader: bool = True):
        ''' 
            Set the root data.   --- UPDATED (Dexter) 20190822

            Parameters
            ------------------------------

            data    `np.ndarray<np.ndarray<int|float|str>>` - The original source data

            sourceDataset   `DataGenerator.Dataset.Types`  - The type of dataset of a source that's this data is referencing.
        '''
        # Set the data, update the epoch size and iteration count.
        self.generatorController[sourceDataset or self.train.currentSourceDataset].setData(inputArray = data, hasHeader = hasHeader)

    def hasData(self, dppKey: str = None, sourceDataset: 'DataGenerator.Dataset.Types' = None) -> bool:
        '''
			Check if there is data for one column configuration.    --- UPDATED (Dexter) 20191002

            Parameters
            ------------------------------

            dppKey  `str`   - The key for a column configuration.

            sourceDataset   `DataGenerator.Dataset.Types`  - The type of dataset of a source that's this data is referencing.

            Returns
            ------------------------------

            `bool`  - Whether the requested column has data defined from its root column configuration.
        '''
        # Get the dataset data
        oriData = self.generatorController[sourceDataset or self.train.currentSourceDataset].oriData

        # Ensure there is original data embedded.
        if oriData is None or len(oriData) == 0:
            return False

        if (dppKey is not None):
            # Ensure the requested key is defined, and return if it's relating to a data source.
            if dppKey not in self.train.dppNodes:
                return False
            
            # Check if the node has specified at least one column.
            colConfig = self.train.dppNodes[dppKey]
            colConfigDataIdx = [i for i in range(0, (oriData.shape[-1] if colConfig.source is None else self.train.dppNodes[colConfig.source].getShape()[-1]))]
            colSourceCols = colConfig.sourceCol
            colSourceIdx = Source.Table.arraySlice([colConfigDataIdx], colSourceCols)[0]
            return len(colSourceIdx)
        else:
            return oriData is not None and len(oriData) > 0                

    def getDppNodeData(self, dppKey, step: 'DataPreprocessing.Node.Columns.StepEnum' = DataPreprocessing.Node.Columns.StepEnum.Output, sourceDataset: 'DataGenerator.Dataset.Types' = None, start: int = None, end: int = None) -> 'np.ndarray':
        '''
			Get the output data of a particular data preprocessing node, with specification of the range of the batch.   --- UPDATED (Dexter) 20191024

            Parameters
            ------------------------------

            dppKey  `str`       - The key for a column configuration.

            step    `DataPreprocessing.Node.Columns.StepEnum`     - The step of data preprocessing within a @DataPreprocessing.Node.Columns object.

            start  `int`   - The starting index (inclusive) of the data rows.

            end    `int`   - The ending index (exclusive) of the data rows.

            Returns
            ------------------------------

            `np.ndarray<np.ndarray<int|float|str>>`  - A two-dimensional data array.
        '''
        # Get the dataset data
        oriData = self.generatorController[self.train.currentSourceDataset].oriData

        # Ensure there is original data.
        if (oriData is None or len(oriData) == 0):
            return np.array([])

        # Ensure the column key is defined.
        if dppKey not in self.train.dppNodes:
            raise ValueError("The column key cannot be found.")

        # Get the column config.
        dppNode = self.train.dppNodes[dppKey]

        return dppNode.getData(step, start, end)

    def getHeader(self, outputsetKey: str = None) -> List[str]:
        '''
			Get the header names of this table source.   --- UPDATED (Dexter) 20190509
            
            Paramters
            ------------------------------

            outputsetKey    `str` - The key for a outputset included in this source object .

            Returns
            ------------------------------

            `list<str>`     - A list of header names.
        '''
        # Get the data view.
        return self.generatorController[self.train.currentSourceDataset].getHeader(outputsetKey)

    def getNextBatch(self, sourceDataset: 'DataGenerator.Dataset.Types' = None) -> 'np.ndarray':
        '''
			Generate the next batch of data rows of this data source.   --- UPDATED (Dexter) 20190822

            Parameters
            ------------------------------

            sourceDataset   `DataGenerator.Dataset.Types`  - The type of dataset of a source that's this data is referencing.

            Returns
            ------------------------------

            'np.ndarray' - The next batch data. In the future, the data will be output as `tf.Tensor` objects.
        '''
        # 1. Get the batch data.
        return self.generatorController.getNextBatch(datasetType = (sourceDataset or self.train.currentSourceDataset))
    
    def getData(self, sourceDataset = None, start: int = None, end: int = None, indexes: List[int] = None) -> 'np.ndarray':
        """
            Get the source data.   --- UPDATED (Dexter) 20200308

            Parameters
            ------------------------------

            sourceDataset   `DataGenerator.Dataset.Types`  - The type of dataset of a source that's this data is referencing.

            start  `int`       - Batch starting index (inclusive). If `None`, it notates `0`.

            end    `int`       - Batch ending index (exclusive). If `None`, it notes the column count.

            batchIndexes    `list[int]` - A list of indexes of data rows.

            Returns
            ------------------------------

            `np.ndarray` - The requested source data
        """
        # 1. Get the source data.
        return self.generatorController[sourceDataset or self.train.currentSourceDataset].getData(start = start, end = end, indexes = indexes)

    def getRandItems(self, sourceDataset: 'DataGenerator.Dataset.Types' = None, randomSeed: int = None, randomCount: int = None):
        '''
			Generate the random items of data. No action taken, and a sub-class should be used.   --- UPDATED (Dexter) 20190822

            Parameters
            ------------------------------

            sourceDataset   `DataGenerator.Dataset.Types`  - The type of dataset of a source that's this data is referencing.

            randomSeed      `int`       - Random seed.

            randomCount     `int`       - The count of random items to collect.

            Returnes
            ------------------------------

            `np.ndarray` - The requested random items
        '''
        # 1. Get the random data.
        return self.generatorController[sourceDataset or self.train.currentSourceDataset].getRandItems(randomSeed = randomSeed, randomCount = randomCount)
    
    def recoverToRawData(self, items):
        '''
			Recover data items to raw-data format of the predicted data, like undo normalization or contain by circular definition, etc.   --- UPDATED (Dexter) 20190509

            Parameters
            ------------------------------

            dppKey      `str`                   - The column config of the data.

            items       `np.ndarray|list[*+]`   - A list of data items.

            Returns
            ------------------------------

            `np.ndarray|list`    - A list of recovered data items.
        '''
        return items

    def getPrintableItems(self, items: Union[List, 'np.ndarray'], outputsetKey: str = None, recovered = True):
        '''
			Print some items into an array, with some further formatting.   --- UPDATED (Dexter) 20190512

            Parameters
            ------------------------------

            dppKey  `str`               - The key for a column configuration.

            items   `list|np.ndarray`   - A list of items typically with original input format.

            outputsetKey    `str`       - The key for a outputset included in this source object.

            recovered   `bool`          - Whether the data is recovered from transformation or not.

            Returns
            ------------------------------

            `list[[str,*]+]`    - A list of items in a single data column with prefix data type column.
        '''
        if (outputsetKey is not None):
            raise ValueError("Table Source is not supported for outputsetKey parameter.")

        dataType = ""
        recoveredData = (escapeNaNNPAry(items).tolist() if items.__class__.__name__ == "ndarray" else items)
        is2DAry = recoveredData[0].__class__.__name__ == "list"
        if (not is2DAry) or len(recoveredData[0]) == 1:
            dataType = "Value"
            if (is2DAry):
                recoveredData = [c[0] for c in recoveredData]
        else:
            dataType = "Table"
        return [[dataType, json.dumps(i)] for i in recoveredData]

    @staticmethod
    def convertCatToNumbers(column, oneHot=False, manualList=None):
        '''
			A transformation function for converting data into one-hot encoding.   --- UPDATED (Dexter) 20180622

            Parameters
            ------------------------------

            column      `np.ndarray[str]`         - The input column to be transformed

            oneHot      `bool`              - Whether to use one-hot encoding with multiple columns. Otherwise, the original column is transformed into an interger-id based values.

            manualList  `dict{str: int}`    - A pre-defined key-id map for transforming the data.

            Returns
            ------------------------------
            
            `dict{}`        - A dict object containing necessary information

            `dict{}["data"]`:   `list[list[int]]`       - The transformed data column

            `dict{}["dict"]`:   `dict{str: int}`        - A key-id map for transforming the data.
        '''
        # `column` should be a single column, while a flattening may be used if needed.
        if len(column.shape) > 1:
            column = column.reshape([-1])
        
        # If there is no `manualList`, it will be auto created.
        if manualList is None:
            manualList = {v: idx for idx, v in enumerate({*column})}
        
        # If it is one-hot encoding, split the column into several categorical columns with 1 or 0 as values; otherwise, just convert into categorial integers.
        if oneHot:
            return {"data": [[1 if c==v else 0 for c in column] for v in manualList.keys()], "dict": manualList}
        else:
            return {"data": [[manualList[c] if c in manualList else -1] for c in column], "dict": manualList}
    
    @staticmethod
    def convertNumbersToCat(columns, manualList):
        '''
			A transformation function for converting array with categorical integers to original data.   --- UPDATED (Dexter) 20191024

            Parameters
            ------------------------------

            columns     `list[list[int]]`   - One or multiple columns of indexed data.

            manualList  `dict{str: int}`    - A pre-defined key-id map for transforming the data.

            Results
            ------------------------------

            `np.ndarray[str]`     - A reverted array of the un-indexed data.

        '''
        # Return the look back value of the indexed data.
        return np.array([np.array([k for k,v in manualList.items() if v == r]) for r in columns])
    
    @staticmethod
    def getDataClassCount(array: 'np.ndarray') -> int:
        '''
            Get the categorical class count from an array.   --- UDPATED (Dexter) 20190128

            Parameters
            ------------------------------

            array    `np.ndarray<np.ndarray<(int|float|str)>>`   - An array of data.

            Returns
            ------------------------------

            `int`   - The categorical class count.
        '''
        return len({*np.reshape(array, [-1])})

    @staticmethod
    def shuffleTogether(*columns):
        '''
			Shuffle all the data together.   --- UPDATED (Dexter) 20191024

            Parameters
            ------------------------------

            *columns    `*np.ndarray<(int|float|str)>`   - One or multiple columns of data.

            Retuns
            ------------------------------

            `list<np.ndarray<(int|float|str)>>`     - The shuffled data.
        '''
        # Zip the data and convert to array.
        tempList = np.array(list(zip(*columns)))

        # Shuffle the data.
        np.random.shuffle(tempList)

        # Convert back to several arrays of columns.
        return (np.array(z) for z in zip(*tempList))

    @staticmethod
    def getColList(colCount, rangeStr:str=None)->List[int]:
        '''
			Get the indexes of a column selection.   --- UPDATED (Dexter) 20181210

            Parameters
            ------------------------------

            colCount    `int`   - The original table column count

            rangeStr         `str`   - Index range string for selecting certain columns for the transformation.

            Returns
            ------------------------------

            `list[int]`         - The list of indexes with specified column selection.
        '''
        # If no column specification requested, just return all the indexes. 
        if rangeStr is None:
            return [i for i in range(0, colCount)]

        return IndexRange.parse(colCount, rangeStr)
    
    @staticmethod
    def arraySlice(nparray: 'np.ndarray', rangeStr: str = "None:None") -> 'np.ndarray':
        '''
			Specify certain column data by selecting some columns of an numpy array (or list).   --- UPDATED (Dexter) 20190505

            Parameters
            ------------------------------

            nparray     `np.ndarray`   - Data table to be selected.

            rangeStr    `str`   - Index range string for selecting certain columns for the transformation.

            Returns
            ------------------------------

            `np.ndarray`   - Data table with requested selection range.
        '''
        # Convert to np array if it's a list.
        if isinstance(nparray, list):
            try:
                nparray = np.array(nparray)
            except:
                raise ValueError("The list data is not supported for arraySlice() method. It should be a list convertible to an `np.ndarray` object.")

        # Check if it's numpy array.
        if not isinstance(nparray, np.ndarray):
            raise ValueError("The nparray data is not supported for arraySlice() method. It should be an `np.ndarray` or `list` object.")

        # Validate the index range range.
        colCount = len(nparray[0])
        if (not IndexRange.validate(colCount, rangeStr)):
            raise ValueError("Index range string is not valid.")
            
        # Get the slicer.
        slicer = IndexRange.toIndexer(rangeStr)
        return nparray[..., slicer]

    @staticmethod
    def tensorSlice(tensor: 'tf.Tensor|np.ndarray|list', rangeStr: str = "None:None") -> 'tf.Tensor':
        '''
            Slice certain column data by selecting some columns of a tensor.   --- UPDATED (Dexter) 20190922

            Parameters
            ------------------------------

            tensor      `tf.Tensor|np.ndarray|list`   - Data table to be selected.

            rangeStr    `str`   - Index range string for selecting certain columns for the transformation.

            Returns
            ------------------------------

            `tf.Tensor`   - Data table with requested selection range.
        '''
        # Convert to np array if it's a list.
        if isinstance(tensor, list) or isinstance(tensor, np.ndarray):
            try:
                tensor = tf.convert_to_tensor(tensor)
            except:
                raise ValueError("The list data is not supported for tensorSlice() method. It should be a list convertible to an `tf.Tensor` object.")

        # Check if it's numpy array.
        if not isinstance(tensor, tf.Tensor):
            raise ValueError("The nparray data is not supported for tensorSlice() method. It should be an `tf.Tensor`, `np.ndarray` or `list` object.")

        # Validate the index range range.
        colCount = tensor.shape[-1]
        if (not IndexRange.validate(colCount, rangeStr)):
            raise ValueError("Index range string is not valid.")
        
        # Get the slicer.
        slicer = IndexRange.toIndexer(rangeStr)

        if isinstance(slicer, list):
            # TensorFlow has not supported slicing of list of indices
            # Concatenate all the indexed tensor to become the new tensor
            return tf.concat([tf.reshape(tensor[..., idx], (*(s for s in tensor.shape[:-1]), 1)) for idx in slicer], axis = -1)
        else:
            return tensor[..., slicer]

class _SourceCSV(_SourceTable):
    '''
			Class representing a CSV table data source.   --- UPDATED (Dexter) 20180630
    '''
    class GeneratorDetail(_SourceTable.GeneratorDetail):
        """
            Class representing a data generator detail for CSV table sources.   --- UPDATED (Dexter) 20190713
        """
        def __init__(self, controller: 'Source.CSV.GeneratorController'):
            """
                Create a @DataGenerator.Detail object for generating data of csv sources.   --- UPDATED (Dexter) 20190713
                
                Parameters
                ------------------------------

                controller `Source.Table.GeneratorController` - The dataset controller maintaining this data generator detail.
            """
            super().__init__(controller)
            # `np.ndarray` - The original Source data
            self._oriData: np.ndarray = None
            # `int` - Remember the row count, and find the iteration count for each epoch.
            self._itrCount: int = 0
            # `bool` - Whether the table has heading. To be defined when data is read.
            self._hasHeader: bool = False
            # `str` - The full/relative file path of the CSV file.
            self._path: str = None
            # `str` - The file name of the CSV file.
            self._fileName: str = None
            # `str` - Encoding format of the file.
            self.encoding: str = None
        
        def parseFromJSON(self, obj: Dict[str, Any]):
            """
                Parse a previously saved object into this @Source.CSV.GeneratorDetail object.   --- UPDATED (Dexter) 20200106

                Parameters
                ------------------------------

                obj     `dict<str,*>`   - JSON object from Project file.
            """
            # Parse for each key of dataset detail.
            for k in Train.getPrioritizedKeys(obj, ["_hasHeader", "encoding", "_path"]):
                v = obj[k]
                if (k == "_path"):
                    if (v is not None):
                        self.path = v
                    else:
                        # If there is no path provided, just copy remaining values.
                        for k2 in ["_hasHeader", "encoding", "_path", "_fileName"]:
                            setattr(self, k2, obj[k2])
                elif (k not in ["_controller", "_oriShape", "_oriData", "_epochSize", "_itrCount"]):
                    setattr(self, k, v)

        def copy(self) -> 'Source.CSV.GeneratorDetail':
            """
                Copy this generator detail   --- UPDATED (Dexter) 20191022

                Returns
                ------------------------------

                `Source.CSV.GeneratorDetail` - The copied generator detail object.
            """
            # Create a new detail object.
            newDetail: Source.CSV.GeneratorDetail = Source.CSV.GeneratorDetail(self.controller)

            # Copy all information.
            for k in ["_epochSize", "_oriShape", "_controller", "_batchSize", "_shuffle", "_oriData", "_itrCount", "_hasHeader", "_path", "_fileName", "encoding"]:
                setattr(newDetail, k, getattr(self, k))
            
            return newDetail

        def readFile(self, path="", encoding="utf8", hasHeader = True):
            """
                Read the file to create data of this generator detail.   --- UPDATED (Dexter) 20191024

                Parameters
                ------------------------------

                path        `str`   - The full/relative file path of the CSV file.

                encoding        `str`   - Encoding format of the file.

                hasHeader      `bool`  - Whether the table has heading.
            """
            try:
                # Open the CSV file.
                with open(path, encoding=encoding) as f:
                    data = [*csv.reader(f)]

                # Convert the data into objects.
                inputData = np.vstack([np.array(r, dtype=object) for r in data if len(r)])

                # Save the information.
                self._path = path
                self._fileName = path[max([path.rfind("/"), path.rfind("\\")])+1:]
                self.encoding = encoding
                self._hasHeader = hasHeader

            except:
                raise ValueError("CSV file cannot be correctly read.")

            # Set this generator detail with the required data.
            self.setData(inputArray = inputData, hasHeader = hasHeader)
        
        @property
        def path(self) -> str:
            """
                Get the full/relative file path of the CSV file. reading by this data generator.   --- UPDATED (Dexter) 20190713

                Returns
                ------------------------------

                `str` - The full/relative file path of the CSV file.
            """
            return self._path
        
        @path.setter
        def path(self, val:str):
            """
                Set the full/relative path of the CSV file to read.   --- UPDATED (Dexter) 20190713

                Parameters
                ------------------------------

                val     `str`       - The full/relative path of the CSV file to read.
            """
            self._path = val
            self.readFile(path=self._path, encoding=self.encoding, hasHeader = self.hasHeader)
        
        @property
        def fileName(self) -> str:
            """
                Get the file name of the CSV file reading by this data generator.   --- UPDATED (Dexter) 20190713

                Returns
                ------------------------------

                `str` - The file name of the CSV file.
            """
            return self._fileName

        def getLocations(self) -> List[str]:
            """
                Get all the location that this dataset detail will reference for.   --- UPDATED (Dexter) 20200105

                Returns
                ------------------------------

                `list<str>` - A list of paths that this dataset detail is referencing on.
            """
            return [self.fileName] if self.fileName else []
                                            
    class GeneratorController(_SourceTable.GeneratorController):
        """
            Class representing a data generator controller for data table sources.   --- UPDATED (Dexter) 20190713
        """
        def __init__(self, attachObject: Union['Source.CSV', 'DataPreprocessing.Node.SourceLike']):
            """
                Create a @Source.CSV.GeneratorController object for controlling data generator of a table source.   --- UPDATED (Dexter) 20190915

                Parameters
                ------------------------------

                attachObject `(Source.Config|ModelNode.Config)` - The object this data generator controller is embedded with.
            """
            super().__init__(attachObject = attachObject)
        
        @property
        def detailType(self) -> Callable[..., 'Source.CSV.GeneratorDetail']:
            """
                Get the data generator detail type.   --- UPDATED (Dexter) 20190713

                Returns
                ------------------------------

                `Callable<*, Source.CSV.GeneratorDetail>`    - The type of the generator detail.
            """
            return Source.CSV.GeneratorDetail
             
    def __init__(self, fileName: str = "", encoding: str = "utf8", hasHeader: bool = True, name: str = ""):
        '''
			Create a CSV table data source.   --- UPDATED (Dexter) 20190915

            Parameters
            ------------------------------

            fileName `str` - The file path of the csv file

            encoding `str` - Encoding format of the file

            hasHeader `bool` - Whether the table has heading

            name `str` - Name of this @Source.CSV object.
        '''
        # Create the CSV source config.
        super().__init__(name=name)
        # `Source.Types` - The instance class, as defined in @Source.Types .
        self._instanceClass = Source.Types.CSV
        # `Source.CSV.GeneratorController` - The data generator controller for this source.
        self.generatorController = Source.CSV.GeneratorController(self)

        # If there is given filename, set it as the trainset.
        if (len(fileName)):
            self.generatorController[DataGenerator.Dataset.Types.Train].readFile(fileName = fileName, encoding = encoding, hasHeader = hasHeader)

    def parseJSON(self, obj: Dict[str, Any], train: 'Train'):
        """
            Parse a previously saved object into this @Source.Config object.   --- UPDATED (Dexter) 20190713

            Parameters
            ------------------------------

            obj     `dict<str,*>`   - JSON object from Project file.
            
            train   `Train`         - The train object this @Source.Config object is attached in.
        """
        super().parseJSON(obj, train)

class _SourceMySQL(_SourceTable):
    '''
			Class representing a CSV table data source.   --- UPDATED (Dexter) 20180630
    '''
    class GeneratorDetail(_SourceTable.GeneratorDetail):
        """
            Class representing a data generator detail for CSV table sources.   --- UPDATED (Dexter) 20190713
        """
        def __init__(self, controller: 'Source.MySQL.GeneratorController'):
            """
                Create a @DataGenerator.Detail object for generating data of csv sources.   --- UPDATED (Dexter) 20190713
                
                Parameters
                ------------------------------

                controller `Source.Table.GeneratorController` - The dataset controller maintaining this data generator detail.
            """
            super().__init__(controller)
            # `np.ndarray` - The original Source data
            self._oriData: np.ndarray = None
            # `int` - Remember the row count, and find the iteration count for each epoch.
            self._itrCount: int = 0
            # `bool` - Whether the table has heading. To be defined when data is read.
            self._hasHeader: bool = False
            # `str` - The full/relative file path of the CSV file.
            # self._path: str = None
            # `str` - The file name of the CSV file.
            # self._fileName: str = None
            # `str` - Encoding format of the file.
            self.encoding: str = None

            self._host: str = None
            self._user: str = None
            self._password: str = None
            self._database: str = None
            self._table: str = None
        
        def parseFromJSON(self, obj: Dict[str, Any]):
            """
                Parse a previously saved object into this @Source.MySQL.GeneratorDetail object.   --- UPDATED (Dexter) 20200106

                Parameters
                ------------------------------

                obj     `dict<str,*>`   - JSON object from Project file.
            """

            if obj['_host'] is not None:
                for k in ["_host", "_user", "_password", "_database", "_table"]:
                    setattr(self, k, obj[k])

                self.readFile(host = self._host, user = self._user, password = self._password, database = self._database, table = self._table)


            # Parse for each key of dataset detail.
            # for k in Train.getPrioritizedKeys(obj, ["_host", "_user", "_password", "_database", "_table"]):
                # v = obj[k]
                # for k in []:
                #     setattr(self, k, obj[k])

                # if (k == "_host"):
                #     if (v is not None):
                #         self.host = v
                #     else:
                #         # If there is no path provided, just copy remaining values.
                #         for k2 in ["_hasHeader", "encoding", "_host", "_user", "_password", "_database", "_table"]:
                #             setattr(self, k2, obj[k2])
                # elif (k not in ["_controller", "_oriShape", "_oriData", "_epochSize", "_itrCount"]):
                #     setattr(self, k, v)

        def copy(self) -> 'Source.MySQL.GeneratorDetail':
            """
                Copy this generator detail   --- UPDATED (Dexter) 20191022

                Returns
                ------------------------------

                `Source.MySQL.GeneratorDetail` - The copied generator detail object.
            """
            # Create a new detail object.
            newDetail: Source.MySQL.GeneratorDetail = Source.MySQL.GeneratorDetail(self.controller)

            # Copy all information.
            for k in ["_epochSize", "_oriShape", "_controller", "_batchSize", "_shuffle", "_oriData", "_itrCount", "_hasHeader", "_host", "_user", "_password", "_database", "_table", "encoding"]:
                setattr(newDetail, k, getattr(self, k))
            
            return newDetail

        def readFile(self, host = "", user = "", password = "", database = "", table = "", encoding="utf8", hasHeader = False):
            """
                Read the file to create data of this generator detail.   --- UPDATED (Dexter) 20191024

                Parameters
                ------------------------------

                path        `str`   - The full/relative file path of the CSV file.

                encoding        `str`   - Encoding format of the file.

                hasHeader      `bool`  - Whether the table has heading.
            """
            import mysql.connector
            try:
                print(host, user, password, database, table)
                mydb = mysql.connector.connect(
                    host=host,
                    user=user,
                    passwd=password,
                    database=database
                )

                query = "SELECT * FROM " + table

                mycursor = mydb.cursor()

                mycursor.execute(query)

                myresult = mycursor.fetchall()

                inputData = list(map(list, myresult))

                print(inputData)

                # Save the information.
                self._host = host
                self._user = user
                self._password = password
                self._database = database
                self._table = table
                self.encoding = encoding
                self._hasHeader = False

            except:
                raise ValueError("MySQL Server cannot be correctly connect.")

            # Set this generator detail with the required data.
            self.setData(inputArray = inputData, hasHeader = False)
        

                # @path.setter
        # def path(self, val:str):
        #     """
        #         Set the full/relative path of the CSV file to read.   --- UPDATED (Dexter) 20190713

        #         Parameters
        #         ------------------------------

        #         val     `str`       - The full/relative path of the CSV file to read.
        #     """
        #     self._path = val
        #     self.readFile(path=self._path, encoding=self.encoding, hasHeader = self.hasHeader)

        @property
        def host(self) -> str:
            return self._host

        @host.setter
        def host(self, val: str):
            self._host = val
            self.readFile(host = self._host, user = self._user, password = self._password, database = self._database, table = self._table)

        @property
        def user(self) -> str:
            return self._user

        @property
        def password(self) -> str:
            return self._password

        @property
        def database(self) -> str:
            return self._database

        @property
        def table(self) -> str:
            return self.table

        # @property
        # def path(self) -> str:
        #     """
        #         Get the full/relative file path of the CSV file. reading by this data generator.   --- UPDATED (Dexter) 20190713

        #         Returns
        #         ------------------------------

        #         `str` - The full/relative file path of the CSV file.
        #     """
        #     return self._path
        
        # @path.setter
        # def path(self, val:str):
        #     """
        #         Set the full/relative path of the CSV file to read.   --- UPDATED (Dexter) 20190713

        #         Parameters
        #         ------------------------------

        #         val     `str`       - The full/relative path of the CSV file to read.
        #     """
        #     self._path = val
        #     self.readFile(path=self._path, encoding=self.encoding, hasHeader = self.hasHeader)
        
        # @property
        # def fileName(self) -> str:
        #     """
        #         Get the file name of the CSV file reading by this data generator.   --- UPDATED (Dexter) 20190713

        #         Returns
        #         ------------------------------

        #         `str` - The file name of the CSV file.
        #     """
        #     return self._fileName

        # def getLocations(self) -> List[str]:
        #     """
        #         Get all the location that this dataset detail will reference for.   --- UPDATED (Dexter) 20200105

        #         Returns
        #         ------------------------------

        #         `list<str>` - A list of paths that this dataset detail is referencing on.
        #     """
        #     return [self.fileName] if self.fileName else []
                                            
    class GeneratorController(_SourceTable.GeneratorController):
        """
            Class representing a data generator controller for data table sources.   --- UPDATED (Dexter) 20190713
        """
        def __init__(self, attachObject: Union['Source.MySQL', 'DataPreprocessing.Node.SourceLike']):
            """
                Create a @Source.MySQL.GeneratorController object for controlling data generator of a table source.   --- UPDATED (Dexter) 20190915

                Parameters
                ------------------------------

                attachObject `(Source.Config|ModelNode.Config)` - The object this data generator controller is embedded with.
            """
            super().__init__(attachObject = attachObject)
        
        @property
        def detailType(self) -> Callable[..., 'Source.MySQL.GeneratorDetail']:
            """
                Get the data generator detail type.   --- UPDATED (Dexter) 20190713

                Returns
                ------------------------------

                `Callable<*, Source.MySQL.GeneratorDetail>`    - The type of the generator detail.
            """
            return Source.MySQL.GeneratorDetail
             
    def __init__(self, host: str = "", user: str = "", password: str = "", database: str = "", table: str = "", encoding: str = "utf8", hasHeader: bool = False, name: str = ""):
        '''
			Create a CSV table data source.   --- UPDATED (Dexter) 20190915

            Parameters
            ------------------------------

            fileName `str` - The file path of the csv file

            encoding `str` - Encoding format of the file

            hasHeader `bool` - Whether the table has heading

            name `str` - Name of this @Source.MySQL object.
        '''
        # Create the CSV source config.
        super().__init__(name=name)
        # `Source.Types` - The instance class, as defined in @Source.Types .
        self._instanceClass = Source.Types.MySQL
        # `Source.MySQL.GeneratorController` - The data generator controller for this source.
        self.generatorController = Source.MySQL.GeneratorController(self)

        # If there is given filename, set it as the trainset.
        if (len(host)):
            print('DAMN IT')
            self.generatorController[DataGenerator.Dataset.Types.Train].readFile(host = host, user = user, password = password, database = database, table = table, encoding = encoding, hasHeader = False)

    def parseJSON(self, obj: Dict[str, Any], train: 'Train'):
        """
            Parse a previously saved object into this @Source.Config object.   --- UPDATED (Dexter) 20190713

            Parameters
            ------------------------------

            obj     `dict<str,*>`   - JSON object from Project file.
            
            train   `Train`         - The train object this @Source.Config object is attached in.
        """
        super().parseJSON(obj, train)

class _SourceMSSQL(_SourceTable):
    '''
			Class representing a CSV table data source.   --- UPDATED (Dexter) 20180630
    '''
    class GeneratorDetail(_SourceTable.GeneratorDetail):
        """
            Class representing a data generator detail for CSV table sources.   --- UPDATED (Dexter) 20190713
        """
        def __init__(self, controller: 'Source.CSV.GeneratorController'):
            """
                Create a @DataGenerator.Detail object for generating data of csv sources.   --- UPDATED (Dexter) 20190713
                
                Parameters
                ------------------------------

                controller `Source.Table.GeneratorController` - The dataset controller maintaining this data generator detail.
            """
            super().__init__(controller)
            # `np.ndarray` - The original Source data
            self._oriData: np.ndarray = None
            # `int` - Remember the row count, and find the iteration count for each epoch.
            self._itrCount: int = 0
            # `bool` - Whether the table has heading. To be defined when data is read.
            self._hasHeader: bool = False
            # `str` - The full/relative file path of the CSV file.
            self._path: str = None
            # `str` - The file name of the CSV file.
            self._fileName: str = None
            # `str` - Encoding format of the file.
            self.encoding: str = None
        
        def parseFromJSON(self, obj: Dict[str, Any]):
            """
                Parse a previously saved object into this @Source.CSV.GeneratorDetail object.   --- UPDATED (Dexter) 20200106

                Parameters
                ------------------------------

                obj     `dict<str,*>`   - JSON object from Project file.
            """
            # Parse for each key of dataset detail.
            for k in Train.getPrioritizedKeys(obj, ["_hasHeader", "encoding", "_path"]):
                v = obj[k]
                if (k == "_path"):
                    if (v is not None):
                        self.path = v
                    else:
                        # If there is no path provided, just copy remaining values.
                        for k2 in ["_hasHeader", "encoding", "_path", "_fileName"]:
                            setattr(self, k2, obj[k2])
                elif (k not in ["_controller", "_oriShape", "_oriData", "_epochSize", "_itrCount"]):
                    setattr(self, k, v)

        def copy(self) -> 'Source.CSV.GeneratorDetail':
            """
                Copy this generator detail   --- UPDATED (Dexter) 20191022

                Returns
                ------------------------------

                `Source.CSV.GeneratorDetail` - The copied generator detail object.
            """
            # Create a new detail object.
            newDetail: Source.CSV.GeneratorDetail = Source.CSV.GeneratorDetail(self.controller)

            # Copy all information.
            for k in ["_epochSize", "_oriShape", "_controller", "_batchSize", "_shuffle", "_oriData", "_itrCount", "_hasHeader", "_path", "_fileName", "encoding"]:
                setattr(newDetail, k, getattr(self, k))
            
            return newDetail

        def readFile(self, path="", encoding="utf8", hasHeader = True):
            """
                Read the file to create data of this generator detail.   --- UPDATED (Dexter) 20191024

                Parameters
                ------------------------------

                path        `str`   - The full/relative file path of the CSV file.

                encoding        `str`   - Encoding format of the file.

                hasHeader      `bool`  - Whether the table has heading.
            """
            try:
                # Open the CSV file.
                with open(path, encoding=encoding) as f:
                    data = [*csv.reader(f)]

                # Convert the data into objects.
                inputData = np.vstack([np.array(r, dtype=object) for r in data if len(r)])

                # Save the information.
                self._path = path
                self._fileName = path[max([path.rfind("/"), path.rfind("\\")])+1:]
                self.encoding = encoding
                self._hasHeader = hasHeader

            except:
                raise ValueError("CSV file cannot be correctly read.")

            # Set this generator detail with the required data.
            self.setData(inputArray = inputData, hasHeader = hasHeader)
        
        @property
        def path(self) -> str:
            """
                Get the full/relative file path of the CSV file. reading by this data generator.   --- UPDATED (Dexter) 20190713

                Returns
                ------------------------------

                `str` - The full/relative file path of the CSV file.
            """
            return self._path
        
        @path.setter
        def path(self, val:str):
            """
                Set the full/relative path of the CSV file to read.   --- UPDATED (Dexter) 20190713

                Parameters
                ------------------------------

                val     `str`       - The full/relative path of the CSV file to read.
            """
            self._path = val
            self.readFile(path=self._path, encoding=self.encoding, hasHeader = self.hasHeader)
        
        @property
        def fileName(self) -> str:
            """
                Get the file name of the CSV file reading by this data generator.   --- UPDATED (Dexter) 20190713

                Returns
                ------------------------------

                `str` - The file name of the CSV file.
            """
            return self._fileName

        def getLocations(self) -> List[str]:
            """
                Get all the location that this dataset detail will reference for.   --- UPDATED (Dexter) 20200105

                Returns
                ------------------------------

                `list<str>` - A list of paths that this dataset detail is referencing on.
            """
            return [self.fileName] if self.fileName else []
                                            
    class GeneratorController(_SourceTable.GeneratorController):
        """
            Class representing a data generator controller for data table sources.   --- UPDATED (Dexter) 20190713
        """
        def __init__(self, attachObject: Union['Source.CSV', 'DataPreprocessing.Node.SourceLike']):
            """
                Create a @Source.CSV.GeneratorController object for controlling data generator of a table source.   --- UPDATED (Dexter) 20190915

                Parameters
                ------------------------------

                attachObject `(Source.Config|ModelNode.Config)` - The object this data generator controller is embedded with.
            """
            super().__init__(attachObject = attachObject)
        
        @property
        def detailType(self) -> Callable[..., 'Source.CSV.GeneratorDetail']:
            """
                Get the data generator detail type.   --- UPDATED (Dexter) 20190713

                Returns
                ------------------------------

                `Callable<*, Source.CSV.GeneratorDetail>`    - The type of the generator detail.
            """
            return Source.CSV.GeneratorDetail
             
    def __init__(self, fileName: str = "", encoding: str = "utf8", hasHeader: bool = True, name: str = ""):
        '''
			Create a CSV table data source.   --- UPDATED (Dexter) 20190915

            Parameters
            ------------------------------

            fileName `str` - The file path of the csv file

            encoding `str` - Encoding format of the file

            hasHeader `bool` - Whether the table has heading

            name `str` - Name of this @Source.CSV object.
        '''
        # Create the CSV source config.
        super().__init__(name=name)
        # `Source.Types` - The instance class, as defined in @Source.Types .
        self._instanceClass = Source.Types.CSV
        # `Source.CSV.GeneratorController` - The data generator controller for this source.
        self.generatorController = Source.CSV.GeneratorController(self)

        # If there is given filename, set it as the trainset.
        if (len(fileName)):
            self.generatorController[DataGenerator.Dataset.Types.Train].readFile(fileName = fileName, encoding = encoding, hasHeader = hasHeader)

    def parseJSON(self, obj: Dict[str, Any], train: 'Train'):
        """
            Parse a previously saved object into this @Source.Config object.   --- UPDATED (Dexter) 20190713

            Parameters
            ------------------------------

            obj     `dict<str,*>`   - JSON object from Project file.
            
            train   `Train`         - The train object this @Source.Config object is attached in.
        """
        super().parseJSON(obj, train)

class _SourceImage(_SourceConfig):
    '''
        Class representing a centralized class for different image datasets as an input source.   --- UPDATED (Dexter) 20181209
    '''
    class GeneratorDetail(DataGenerator.Detail):
        """
            Class representing a data generator detail for image sources.   --- UPDATED (Dexter) 20190921
        """
        def __init__(self, controller: 'Source.Image.GeneratorController'):
            """
                Create a @DataGenerator.Detail object for generating data of table sources.   --- UPDATED (Dexter) 20190706
                
                Parameters
                ------------------------------

                controller `Source.Image.GeneratorController` - The dataset controller maintaining this data generator detail.
            """
            super().__init__(controller)
            # `str` - The image source type (the renowned image dataset) of this @Source.Image object. 
            self.sourceType = None
            # `list<int>` - The shape of the label data.
            self._labelShape = []
            # `list<str>|dict<str,list<str>>` - A list of actual data files that the training & testing phase would extract on; it may also be a dictionary of list of file names corresponding to a outputset key.
            self._fileNames: Union[List[str], Dict[str, List[str]]] = None
            # `np.ndarray` - A 4-dimension array of dimensions [batch, image height, image width, color depth], for storing all image data in case the data is pre-extracted instead of getting on-demand by handling through TensorFlow Dataset API.
            self._imgData = np.array([], dtype=float)
            # `np.ndarray` - A 2-dimension array of dimensions [batch, class label (must be 1)], for storing all image-label data in case the data is pre-extracted instead of getting on-demand by handling through TensorFlow Dataset API.
            self._labelData = np.array([], dtype=float)
            # `tf.compat.v1.Session` - The TensorFlow session for handling data extraction through Dataset API.
            self._sess = None
            # `tf.Graph` - The TensorFlow graph for handling data extraction through Dataset API.
            self._imgGraph = None
            # `Iterator` - The TensorFlow Dataset API iterable.
            self._itr = None
            # `tf.Tensor` - A TensorFlow tensor handling the label generation.
            self._nextLabels = None
            # `tf.Tensor` - A TensorFlow tensor handling the image generation.
            self._nextImg = None
            # `list<int>` - The raw image dimensions.
            self._imgRawShape = []
            # `int` - The number of bytes for the images.
            self._imgBytes = 0
            # `int` - The number of bytes for the labels.
            self._targetBytes = 0
            # `dict<str, Source.OutputsetInfo>` - Dataset information specifying root information for this dataset.
            self.outputsetInfo = {}
        
        def copy(self) -> 'Source.Image.Detail':
            """
                Copy this generator detail   --- UPDATED (Dexter) 20200315

                Returns
                ------------------------------

                `Source.Image.Detail` - The copied generator detail object.
            """
            # Create a new detail object.
            newDetail: Source.Image.GeneratorDetail = Source.Image.GeneratorDetail(self.controller)

            # Copy all information.
            for k in ["_epochSize", "_oriShape", "_itrIdx", "_batchSize", "_shuffle", "_imgData", "_labelData", "sourceType", "_labelShape", "_fileNames", "_imgRawShape", "_imgBytes", "_targetBytes", "outputsetInfo"]:
                setattr(newDetail, k, getattr(self, k))

            return newDetail

        @property
        def labelShape(self) -> List[int]:
            '''
                The label shape of this @Source.Image.GeneratorDetail object.   --- UPDATED (Dexter) 20190921

                Returns
                ------------------------------

                `list<int>` - The label shape.
            '''
            return self._labelShape
        
        @property
        def imgData(self) -> 'np.ndarray':
            """
                Get the original image data of this image source.   --- UPDATED (Dexter) 20190921

                Returns
                ------------------------------

                `np.ndarray` - The original image data.
            """
            return self._imgData
        
        @property
        def labelData(self) -> 'np.ndarray':
            """
                Get the original label data of this image source.   --- UPDATED (Dexter) 20190921

                Returns
                ------------------------------

                `np.ndarray` - The original label data.
            """
            return self._labelData
        
        @property
        def hasHeader(self) -> bool:
            '''
                Return whether this image source include a heading.   --- UPDATED (Dexter) 20190921

                Returns
                ------------------------------

                `bool` - Whether this image source has heading.
            '''
            return True
        
        def getHeader(self, outputsetKey: str = "image"):
            '''
                Get the header names of this image source.   --- UPDATED (Dexter) 20200104
                
                Paramters
                ------------------------------

                outputsetKey    `str` - The key for a outputset included in this source object .

                Returns
                ------------------------------

                `list<str>`     - A list of header names.
            '''
            return self.outputsetInfo[outputsetKey].header
        
        def prepareIteration(self):
            '''
			    Create the TensorFlow Graph using TensorFlow Dataset API for image source iteration.   --- UPDATED (Dexter) 20191013
            '''
            sourceType: str = self.sourceType
            imgRawShape: List[int] = self.attachObject.outputsetInfo["input"].shape[1:]
            self._imgRawShape = imgRawShape

            # Check for the existance of every file.
            if (isinstance(self._fileNames, dict)):
                (self._fileNames): dict
                for fileList in self._fileNames.values():
                    for fileName in fileList:
                        # Ensure the data folder exists.
                        if not os.path.exists(fileName):
                            raise ValueError("Data File not found: \"" + fileName + "\"")
            elif (isinstance(self._fileNames, list)):
                (self._fileNames): list
                for fileName in self._fileNames:
                    # Ensure the data folder exists.
                    if not os.path.exists(fileName):
                        raise ValueError("Data File not found: \"" + fileName + "\"")
            
            if (sourceType == "mnist" or sourceType == "fashion-mnist"):
                # 1. Read the image and label files.
                magic: int
                imageCount: int
                with open(self._fileNames["input"][0], "rb") as f:
                    magicNo, imageCount = struct.unpack(">II", f.read(8))
                    shape: int = struct.unpack(">II", f.read(8))
                    self._imgData = np.array(list(f.read())).reshape(self.epochSize,imgRawShape[0],imgRawShape[1],imgRawShape[2])
                
                with open(self._fileNames["target"][0], "rb") as f:
                    magicNo, imageCount = struct.unpack(">II", f.read(8))
                    self._labelData = np.array(list(f.read())).reshape(self.epochSize,1)

                # 2. Reset the iteration index.
                self.batchIdxReset()

                # 3. Reshuffle the data if needed.
                if (self.shuffle):
                    self._imgData, self._labelData = Source.Table.shuffleTogether(self._imgData, self._labelData)
            
            elif (sourceType in ["cifar10", "cifar100", "stl10"]):
                # 1. Prepare a standalone TF Graph for the input pipeline and define necessary variables.
                batchSize: int = self.batchSize

                #with self._imgGraph.as_default():
                # 2. Prepare the data iterator and get data images from the input pipeline.
                self._imgBytes = functools.reduce(lambda a,b: a*b, imgRawShape, 1)
                self._targetBytes = 2 if (sourceType == "cifar100") else 1
                dataset: 'tf.data.FixedLengthRecordDataset' = tf.data.FixedLengthRecordDataset(self._fileNames, self._imgBytes + self._targetBytes)
                dataset = dataset.map(self.mapDatasetImages)
                if (self.shuffle): 
                    dataset = dataset.shuffle(buffer_size=self.epochSize, reshuffle_each_iteration=True)
                dataset = dataset.repeat()
                dataset = dataset.batch(batchSize, drop_remainder = self.dropRemainder)
                self._itr = dataset.__iter__()

        def _mapDatasetImage(self, tfDatasetEle) -> 'Train.RootData':
            """
                Get each image from the dataset.   --- UPDATED (Dexter) 20191001

                Parameters
                ------------------------------

                tfDatasetEle    `*`   - A TensorFlow Dataset element, the bytes for a single image (with its label).

                Returns
                ------------------------------

                `Train.RootData` - A tuple of the image class label, and the image pixel data with dimension (height, width, depth).
            """
            # 1. Get the image from reading the bytes
            imgRawShape = self._imgRawShape
            label = tf.cast(tf.strided_slice(tf.io.decode_raw(tfDatasetEle, tf.uint8), [0], [self._targetBytes]), tf.int32)
            unit8Img = tf.transpose(tf.reshape(tf.strided_slice(tf.io.decode_raw(tfDatasetEle, tf.uint8), [self._targetBytes],[self._targetBytes + self._imgBytes]),[imgRawShape[2], imgRawShape[0], imgRawShape[1]]), [1,2,0])

            # 2. Prepare the Tensor to be float32, as we've to normalize later
            floatImg = tf.cast(unit8Img, tf.float32)
            label.set_shape([self._targetBytes])
            floatImg.set_shape([*imgRawShape])

            return {"target": label, "input": floatImg}

        def mapDatasetImages(self, tfDatasetEle) -> Dict[str, Any]:
            """
                Get the dataset image for the raw root source and the propagated dpp nodes.   --- UPDATED (Dexter) 20190921

                Parameters
                ------------------------------

                tfDatasetEle    `*`   - A TensorFlow Dataset element, the bytes for a single image (with its label).

                Returns
                ------------------------------

                `Source.PreprocessedData` - A tuple of the image class label, and the image pixel data with dimension (height, width, depth).
            """
            # Get the root data.
            rootData = self._mapDatasetImage(tfDatasetEle)

            # Pretend a RootData to be pre-processed by data preprocessed node.
            sourceRoot = {}
            sourceRoot[self.attachObject.sourceID] = rootData
            pretendRootData = Train.RootData(sourceRoot)

            dppData = {dppKey: dppNode.getProcessedData(pretendRootData) for dppKey, dppNode in self.attachObject.propagateDppNodes.items()}
            return Source.PreprocessedData(rootData, dppData).toDict()

        def getNextBatch(self) -> 'np.ndarray|Source.PreprocessedData':
            '''
                Get the next batch of image.   --- UPDATED (Dexter) 20191002

                Returns
                ------------------------------

                `dict<str,*>|Source.PreprocessedData` - Depending on whether labels are needed, the list will contains 2 arrays of labels and images respectively.
            '''
            if (self.sourceType in ["cifar10", "cifar100", "stl10"]):
                # Initialize the data if it is not initialized.
                if (not self._initialized):
                    self.prepareIteration()
                    self.initialize()
                
                # Prepare the next batch of data.
                #return Source.PreprocessedData.fromDict(self._sess.run(self._nextTensor))
                processedData = Source.PreprocessedData.fromDict(next(self._itr))
                processedData.toArray()
                return processedData

            elif (self.sourceType in ["mnist", "fashion-mnist"]):
                # 1. Confirm this data generator detail is initialized before any generator yields.
                if (not self._initialized):
                    self.initialize()
                    self.prepareIteration()

                # 2. Get an index and batch size
                i = self._itrIdx
                batchSize = self.batchSize

                # 3. If this batch has touched the end of the epoch, regenerate the data .
                if ((self.dropRemainder and (i+1)*batchSize > self.epochSize) or ((not self.dropRemainder) and i*batchSize >= self.epochSize)):
                    self.prepareIteration()
                    i = self.batchIdx
                
                # 4. Increment the index and return all the data.
                self.batchIdxIncrement()

                # 4. Get the data as according to the default or specially indexed data
                returnObj = {colName: c[i*batchSize:(i+1)*batchSize] for colName, c in {"input": self._imgData, "target": self._labelData}.items()}
                
                # 5. Increment the index and return all the data
                return returnObj
        
        def getData(self, start: int = None, end: int = None, indexes: List[int] = None) -> 'np.ndarray':
            """
                Get the data of with specific indexes.   --- UPDATED (Dexter) 20200104

                Parameters
                ------------------------------

                start       `int`       - Batch starting index (inclusive). If `None`, it notates `0`.

                end         `int`       - Batch ending index (exclusive). If `None`, it notes the epoch size.

                indexes     `list[int]` - A list of indexes of data rows.

                Returns
                ------------------------------

                `np.ndarray` - The requested source data of this generator detail.
            """
            if (self.sourceType in ["mnist", "fashion-mnist"]):
                # Return the requested data.
                outputsetData = {"input": self._imgData, "target": self._labelData}.items()
                if (indexes):
                    processedData = {colName: c[indexes] for colName, c in indexes}
                elif (start or end):
                    processedData = {colName: c[start:end] for colName, c in indexes}
                else:
                    processedData = {colName: c[:] for colName, c in indexes}
            else:
                # There is no data to return.
                processedData = {"input": [], "target": []}
            
            return processedData

        def getRandItems(self, randomSeed: int = None, randomCount: int = None) -> Any:
            """
                Generate the random items of data.   --- UPDATED (Dexter) 20190922

                Parameters
                ------------------------------

                randomSeed      `int`       - Random seed.

                randomCount     `int`       - The count of random items to collect.
            """
            # Initiate the return object. TODO
            returnObj = None

            # Create a temporary source with shuffling but not training use.
            tmpSource = self.copy()
            tmpSource.overrideConfig({"shuffle": True, "batchSize": min(randomCount, tmpSource.batchSize)})

            # Temporarily switch the source dataset to test.
            oriSourceDataset = self.attachObject.train.currentSourceDataset
            self.attachObject.train.assignCurrentSourceDataset(DataGenerator.Dataset.Types.Test)

            # Get the items from the temp source.
            while (randomCount > 0):
                newImgs = tmpSource.getNextBatch()
                if (returnObj is None):
                    returnObj = newImgs
                else:
                    returnObj += newImgs
                randomCount -= self.batchSize

            tmpSource.close()

            # Revert the dataset reference.
            self.attachObject.train.assignCurrentSourceDataset(oriSourceDataset)

            return returnObj

        def close(self):
            '''
                Close and free the resources for the input pipeline.   --- UPDATED (Dexter) 20190921
            '''
            super().close()
            if (self._sess): 
                self._sess.close()     
        
    class GeneratorController(DataGenerator.Controller):
        """
            Class representing a data generator controller for data table sources.   --- UPDATED (Dexter) 20190713
        """
        def __init__(self, attachObject: Union['Source.Image', 'DataPreprocessing.Node.SourceLike']):
            """
                Create a @DataGenerator.Controller object for controlling data generator of a table source.   --- UPDATED (Dexter) 20190921

                Parameters
                ------------------------------

                attachObject `(Source.Config|ModelNode.Config)` - The object this data generator controller is embedded with.
            """
            super().__init__(attachObject = attachObject)
        
        def setSourceType(self, sourceType: str, coreDataDir: str):
            '''
                Set the source type of this image data generator controller.   --- UPDATED (Dexter) 20191010

                Parameters
                ------------------------------

                `str` - The source type string.

                `str` - The folder directory of the image source.
            '''
            # Cache the source type for all detail.
            for detail in self.values():
                detail.sourceType = sourceType

            # Set the epoch size and Dataset API-related file names.
            if (sourceType == "cifar10"):
                self[DataGenerator.Dataset.Types.Train]._fileNames = [coreDataDir +'cifar-10-batches-bin/data_batch_%d.bin' % i for i in range(1, 6)]
                self[DataGenerator.Dataset.Types.Test]._fileNames = [coreDataDir +'cifar-10-batches-bin/test_batch.bin']
                self[DataGenerator.Dataset.Types.Train]._epochSize = 50000 
                self[DataGenerator.Dataset.Types.Test]._epochSize = 10000
            elif (sourceType == "cifar100"):
                self[DataGenerator.Dataset.Types.Train]._fileNames = [coreDataDir +'cifar-100-binary/train.bin'] 
                self[DataGenerator.Dataset.Types.Test]._fileNames = [coreDataDir +'cifar-100-binary/test.bin']
                self[DataGenerator.Dataset.Types.Train]._epochSize = 50000 
                self[DataGenerator.Dataset.Types.Test]._epochSize = 10000
            elif (sourceType == "stl10-labeled"):
                self[DataGenerator.Dataset.Types.Train]._fileNames = {"input": [coreDataDir +'stl10-binary/train_X.bin'], "target": [coreDataDir +'stl10-binary/train_y.bin']}
                self[DataGenerator.Dataset.Types.Test]._fileNames = {"input": [coreDataDir +'stl10-binary/test_X.bin'], "target": [coreDataDir +'stl10-binary/test_y.bin']}
                self[DataGenerator.Dataset.Types.Train]._epochSize = 5000
                self[DataGenerator.Dataset.Types.Test]._epochSize = 8000
            elif (sourceType == "mnist"):
                self[DataGenerator.Dataset.Types.Train]._fileNames = {"input": [coreDataDir +'train-images.idx3-ubyte'], "target": [coreDataDir +'train-labels.idx1-ubyte']}
                self[DataGenerator.Dataset.Types.Test]._fileNames = {"input": [coreDataDir +'t10k-images.idx3-ubyte'], "target": [coreDataDir +'t10k-labels.idx1-ubyte']}
                self[DataGenerator.Dataset.Types.Train]._epochSize = 60000
                self[DataGenerator.Dataset.Types.Test]._epochSize = 10000
            elif (sourceType == "fashion-mnist"):
                self[DataGenerator.Dataset.Types.Train]._fileNames = {"input": [coreDataDir +'train-images-idx3-ubyte'], "target": [coreDataDir +'train-labels-idx1-ubyte']}
                self[DataGenerator.Dataset.Types.Test]._fileNames = {"input": [coreDataDir +'t10k-images-idx3-ubyte'], "target": [coreDataDir +'t10k-labels-idx1-ubyte']}
                self[DataGenerator.Dataset.Types.Train]._epochSize = 60000
                self[DataGenerator.Dataset.Types.Test]._epochSize = 10000
            self[DataGenerator.Dataset.Types.Test]._shuffle = False

        @property
        def detailType(self) -> Callable[..., 'Source.Image.GeneratorDetail']:
            """
                Get the data generator detail type.   --- UPDATED (Dexter) 20190921

                Returns
                ------------------------------

                `Callable<*, Source.Image.GeneratorDetail>`    - The type of the generator detail.
            """
            return Source.Image.GeneratorDetail

    def __init__(self, sourceType = "cifar10", coreDataDir = None, flattenImg = False, name: str = ""):
        '''
			Create an @Source.Image object, a type of @TrainingSource handling the image data sources.   --- UPDATED (Dexter) 20190814
            
            Parameters
            ------------------------------
            
            sourceType      `str`           - The type of image source, only `cifar10`, `cifar100`, `fashion-mnist`, `mnist` is supported currently.

            coreDataDir     `str`           - The directory of the data folder places.

            flattenImg      `bool`          - Whether to flatten into 1D when generating the images from the source.
            
            name        `str`   - Name of this @Source.Table object.
        '''
        super().__init__(name = name)
        self._instanceClass = Source.Types.Image
        # `Source.Table.GeneratorController` - The data generator controller for this source.
        self.generatorController = Source.Image.GeneratorController(self)

        # Initiate some common properties.
        # `str` - The image source type (the renowned image dataset) of this @Source.Image object. 
        self._sourceType = None
        
        # `str` - The folder directory where the renowned data source is saved.
        self.coreDataDir = coreDataDir if (coreDataDir is not None) else ("D:/tmp/" + ("cifar10_data/" if sourceType == "cirfar10" else "cifar100_data/" if sourceType == "cirfar100" else "mnist" if sourceType == "mnist/" else "fashion-mnist" if sourceType == "fashion-mnist/" else ""))

        # `bool` - Whether to flatten into 1D when generating the images from the source
        self.flattenImg = flattenImg

        # Set the source type.        
        if (sourceType):
            self.setSourceType(sourceType)

    def parseJSON(self, obj: Dict[str, Any], train: 'Train'):
        """
            Parse a previously saved object into this @Source.Config object.   --- UPDATED (Dexter) 20200315

            Parameters
            ------------------------------

            obj     `dict<str,*>`   - JSON object from Project file.
            
            train   `Train`         - The train object this @Source.Config object is attached in.
        """
        # Iterate the object keys and take actions on mapping back to the Source.Config Class.
        for k,v in obj.items():
            if (k == "generatorController"):
                # Parse generator controllers.
                self.generatorController.parseFromJSON(v)
                
            elif (k == "outputsetInfo"):
                # Get outputsetInfo as a dictionary.
                setattr(self, k, {ok: ov for (ok, ov) in v})

            elif (k == "_propagateDppNodes"):
                setattr(self, k, set(v))

            elif (k not in ["_testRatio", "_hasHeader", "batchSize", "colConfigs", "shuffle", "epochSize", "dppNodes", "_type","_train","train","_instanceClass"]):
                # For other properties, just directly assign the values.
                setattr(self, k, v)
        
        # Assign the train.
        self.train = train

    @property
    def train(self) -> 'Train':
        """
            The @Train object where this @Source.Config belongs to. If this source is just constructed, it is not belonged to any training instance.   --- UPDATED (Dexter) 20190405

            Returns
            ------------------------------

            `Train` - The @Train object where this @Source.Config belongs to.
        """
        return self._train
    
    @train.setter
    def train(self, train: 'Train') -> 'Train':
        """
            Set the @Train object where this @Source.Image belongs to. This method may be overridden in case some sources may need to auto-connect with data preprocessing nodes once it's attached to the @Train object.   --- UPDATED (Dexter) 20191015

            Parameters
            ------------------------------

            train `Train` - The @Train object where this @Source.Config belongs to.
            
            Returns
            ------------------------------

            `Train` - The @Train object where this @Source.Config belongs to.
        """
        # Ensure all existing source are having splittable as this source.
        if any([s.splittable != self.splittable for s in train.sources]):
            raise ValueError("Sources are incompatible within the Train object. All sources should have consistent splittable nature.")

        # Assign reference to the train object.
        self._train = train

        # Assign this source into the source.
        train.sources.append(self)
        
        # Set the source type.
        self.setSourceType(self.sourceType)

        # Return the train object.
        return self._train

    @property
    def sourceType(self) -> str:
        ''' 
            The source type of this @Source.Image object.   --- UPDATED (Dexter) 20190131

            Returns
            ------------------------------

            `str` - The source type.
        '''
        return self._sourceType
    
    def setSourceType(self, sourceType: str):
        '''
            Set the source type of this @Source.Image object.   --- UPDATED (Dexter) 20190921

            Parameters
            ------------------------------

            `str` - The source type string.
        '''
        # Set the data source shape.
        self._sourceType = sourceType

        # Configure the data generator
        self.generatorController.setSourceType(sourceType, self.coreDataDir)

        # Set the epoch size and Dataset API-related file names.
        if (sourceType == "cifar10"):
            self.outputsetInfo["input"] = Source.OutputsetInfo(["Images"], [None,32,32,3])
            self.outputsetInfo["target"] = Source.OutputsetInfo(["Class Labels"], [None,1])
        elif (sourceType == "cifar100"):
            self.outputsetInfo["input"] = Source.OutputsetInfo(["Images"], [None,32,32,3])
            self.outputsetInfo["target"] = Source.OutputsetInfo(["Class Labels", "Fine Class Lables"], [None,2])
        elif (sourceType == "stl10-labeled"):
            self.outputsetInfo["input"] = Source.OutputsetInfo(["Images"], [None,96,96,3])
            self.outputsetInfo["target"] = Source.OutputsetInfo(["Class Labels"], [None,1])
        elif (sourceType == "mnist"):
            self.outputsetInfo["input"] = Source.OutputsetInfo(["Images"], [None,28,28,1])
            self.outputsetInfo["target"] = Source.OutputsetInfo(["Class Labels"], [None,1])
        elif (sourceType == "fashion-mnist"):
            self.outputsetInfo["input"] = Source.OutputsetInfo(["Images"], [None,28,28,1])
            self.outputsetInfo["target"] = Source.OutputsetInfo(["Class Labels"], [None,1])

        # Assign default data preprocessing nodes.
        if (self.train): 
            self.initializeDefaultDppNodes()
    
    def initializeDefaultDppNodes(self):
        """
            Initialize necessary default data preprocessing nodes from this image source.   --- UPDATED (Dexter) 20191013
        """
        # The shape of the column configs will be manually setup depending on the source type as like the shape of the output Tensors
        dppConfig = DataPreprocessing.Node.Image(dtype = tf.float32)
        dppConfig.appendOn(self, "input", "input")
        self.addPropagateDppNodes("input")

        # Preset with a crop transformation for specific datasets.
        if (self.sourceType in ["cifar10", "cifar100"]):
            dppConfig.setTransform(DataPreprocessing.Transformation.ImageAugmentation.Flip())
            dppConfig.setTransform(DataPreprocessing.Transformation.ImageAugmentation.Brightness())
            dppConfig.setTransform(DataPreprocessing.Transformation.ImageAugmentation.Contrast())
            dppConfig.setTransform(DataPreprocessing.Transformation.Image.Normalize())
        elif (self.sourceType in ["fashion-mnist", "mnist"]):
            dppConfig.setTransform(DataPreprocessing.Transformation.Image.Normalize())
        else:
            dppConfig.clearTransform()

        # Set the data type of the column config.
        if (self.sourceType in ["cifar10", "cifar100", "stl10-labeled", "fashion-mnist", "mnist"]):
            targetDpp = DataPreprocessing.Node.Columns(dtype = "tf.int64")

            # Set the class count of the dpp node.
            if (self.sourceType in ["cifar10", "stl10-labeled", "fashion-mnist", "mnist"]):
                targetDpp.setClassCount(10)
            elif (self.sourceType == "cifar100"):
                targetDpp.setClassCount(100)
            
            # Append the dpp node to this source.
            targetDpp.appendOn(self, "target", "target")

        else:
            self.train.dppNodes.remove("target")
            self.removePropagateDppNodes("target")
        
    def getData(self, sourceDataset = None, start: int = None, end: int = None, batchIndexes: List[int] = None) -> 'np.ndarray':
        """
            Get the source data. The data is    --- UPDATED (Dexter) 20190508

            Parameters
            ------------------------------

            sourceDataset   `DataGenerator.Dataset.Types`  - The type of dataset of a source that's this data is referencing.

            start  `int`       - Batch starting index (inclusive). If `None`, it notates `0`.

            end    `int`       - Batch ending index (exclusive). If `None`, it notes the column count.

            batchIndexes    `list[int]` - A list of indexes of data rows.

            Returns
            ------------------------------

            `np.ndarray` - The requested source data
        """
        raise ValueError("Using deprecated API.")

        # Route another data source if the source dataset is not None.
        if (sourceDataset is not None):
            return self.generatorController[sourceDataset].getData(start = start, end = end, batchIndexes = batchIndexes)

class _SourceCustomImage(_SourceConfig):
    '''
        Class representing a custom image data source.   --- UPDATED (Trista) 20200619
    '''
    class GeneratorDetail(DataGenerator.Detail):
        """
            Class representing a data generator detail for custom image sources.   --- UPDATED (Trista) 20200619
        """
        def __init__(self, controller: 'Source.CustomImage.GeneratorController'):
            """
                Create a @DataGenerator.Detail object for generating data of custom image sources.   --- UPDATED (Trista) 20200619
                
                Parameters
                ------------------------------

                controller `Source.CustomImage.GeneratorController` - The dataset controller maintaining this data generator detail.
            """
            super().__init__(controller)
            # `str` - The image source type (the renowned image dataset) of this @Source.Image object. 
            self.sourceType = None
            # `list<int>` - The shape of the label data.
            self._labelShape = []
            # `bool` - Whether the table has heading. To be defined when data is read.
            self._hasHeader: bool = False
            # `str` - The full/relative file path of the CSV file.
            self._path: str = None
            # `list<str>|dict<str,list<str>>` - A list of actual data files that the training & testing phase would extract on; it may also be a dictionary of list of file names corresponding to a outputset key.
            self._fileNames: Union[List[str], Dict[str, List[str]]] = None
            # `str` - Encoding format of the file.
            self.encoding: str = None
            # `np.ndarray` - A 4-dimension array of dimensions [batch, image height, image width, color depth], for storing all image data in case the data is pre-extracted instead of getting on-demand by handling through TensorFlow Dataset API.
            self._imgData = np.array([], dtype=float)
            # `np.ndarray` - The original Source data
            self._oriCSVData: np.ndarray = None
            # `np.ndarray` - A 2-dimension array of dimensions [batch, class label (must be 1)], for storing all image-label data in case the data is pre-extracted instead of getting on-demand by handling through TensorFlow Dataset API.
            self._labelData = np.array([], dtype=float)
            # `tf.Graph` - The TensorFlow graph for handling data extraction through Dataset API.
            self._imgGraph = None
            # `Iterator` - The TensorFlow Dataset API iterable.
            self._itr = None
            # `tf.Tensor` - A TensorFlow tensor handling the label generation.
            self._nextLabels = None
            # `tf.Tensor` - A TensorFlow tensor handling the image generation.
            self._nextImg = None
            # `list<int>` - The raw image dimensions.
            self._imgRawShape = []
            # `int` - The number of bytes for the images.
            self._imgBytes = 0
            # `int` - The number of bytes for the labels.
            self._targetBytes = 0
            # `dict<str, str>` - The dictionary of image name and label for imageclassify.
            self._labelDict = {}
            # `dict<str, Source.OutputsetInfo>` - Dataset information specifying root information for this dataset.
            self.outputsetInfo = {}
        
        def copy(self) -> 'Source.CustomImage.Detail':
            """
                Copy this generator detail   --- UPDATED (Trista) 20200619

                Returns
                ------------------------------

                `Source.CustomImage.Detail` - The copied generator detail object.
            """
            # Create a new detail object.
            newDetail: Source.CustomImage.GeneratorDetail = Source.CustomImage.GeneratorDetail(self.controller)

            # Copy all information.
            for k in ["_epochSize", "_oriShape", "_batchSize", "_shuffle", "_imgData", "_labelData", "sourceType", "_labelShape", "_fileNames", "_imgRawShape", "_imgBytes", "_targetBytes", "outputsetInfo"]:
                setattr(newDetail, k, getattr(self, k))

            return newDetail
       
        def parseFromJSON(self, obj: Dict[str, Any]):
            """
                Parse a previously saved object into this @Source.CustomImage.GeneratorDetail object.   --- UPDATED (Trista) 20200619

                Parameters
                ------------------------------

                obj     `dict<str,*>`   - JSON object from Project file.
            """
            # Parse for each key of dataset detail.
            for k,v in obj.items():
                if (k == "_path"):
                    v = obj[k]
                    if (v is not None):
                        self.path = v
                    else:
                        # If there is no path provided, just copy remaining values.
                        # for k2 in ["_hasHeader", "encoding", "_path", "_fileName"]:
                        for k2 in ["_hasHeader", "encoding", "_path", "_fileName"]:
                            setattr(self, k2, obj[k2])
                elif (k not in ["_controller"]):
                    setattr(self, k, v)
      
        @property
        def oriData(self) -> 'np.ndarray':
            """
                Get the original data of this table source.   --- UPDATED (Dexter) 20190606

                Returns
                ------------------------------

                `np.ndarray<np.ndarray<int|float|str>>` - The original source data.
            """
            return self._oriData
        
        @property
        def epochSize(self) -> int:
            """
                Epoch size of the data, i.e. total number of records in the training dataset.   --- UPDATED (Dexter) 20190921

                Returns
                ------------------------------

                `int`   - Epoch size of the data, i.e. total number of records in the training dataset.
            """
            return (self._oriShape[0] if len(self._oriShape) else None) if self._epochSize is None else self._epochSize

        @property
        def hasHeader(self) -> bool:
            '''
                Return whether this table source include a heading.   --- UPDATED (Dexter) 20190612

                Returns
                ------------------------------

                `bool` - Whether this data table has heading.
            '''
            return self._hasHeader
        
        def setData(self, inputArray: Union[list,'np.ndarray'] = [], outputArray: Union[list,'np.ndarray'] = [], hasHeader: bool = True):
            """
                Read the image and table and set the raw data of this source.   --- UPDATED (Trista) 20200619

                Parameters
                ------------------------------
                
                inputArray  `list[list[float|int|str]]` - An input data image.

                outputArray `list[list[float|int|str]]` - An output data image/table.

                hasHeader   `bool`  - Whether this table source includes a heading.
            """
            # Set the data table.
            sourceType: str = self.sourceType
            self._hasHeader = hasHeader

            # Convert input array into NumPy array.
            inp = np.array(inputArray)
            out = np.array(outputArray)
            self._epochSize = len(out)

            # Ensure there is at least some data.
            if (len(inp) == 0 or len(out) == 0):
                raise ValueError("Some data should be set instead of `None` or array with zero length")
            elif (len(inp) != len(out)):
                raise ValueError("Data and labels do not match")

            if (sourceType == "imageclassify"):
                # Auto handling data preprocessing column selections if there is output array given.
                outputColCount = len(out[0]) if out.shape[-1] else 0
                targetCol = ("None:" + str(outputColCount)) if outputColCount > 0 else None
                # input
                inputDppNode = DataPreprocessing.Node.Image(dtype = tf.float32)
                # inputDppNode.appendOn(self, "input", "input")
                inputDppNode.appendOn(self.attachObject, "input", "input")
                # self.addPropagateDppNodes("input")
                # output
                # targetDpp = DataPreprocessing.Node.Columns(dtype = "tf.int64")
                outputDppNode = DataPreprocessing.Node.Columns(sourceCol=targetCol)
                # outputDppNode.appendOn(self, "target", "target")
                outputDppNode.appendOn(self.attachObject, "target", "target")
            elif (sourceType == "imageimage"):
                # input
                inputDppNode = DataPreprocessing.Node.Image(dtype = tf.float32)
                inputDppNode.appendOn(self.attachObject, "input", "input")
                # output
                outputDppNode = DataPreprocessing.Node.Image(dtype = tf.float32)
                outputDppNode.appendOn(self.attachObject, "target", "target")

        def readCSVFile(self, path="", encoding="utf8", hasHeader = True):
            """
                Read the csv file to create label dictionary of this generator detail.   --- UPDATED (Trista) 20200619

                Parameters
                ------------------------------

                path        `str`   - The full/relative file path of the CSV file.

                encoding        `str`   - Encoding format of the file.

                hasHeader      `bool`  - Whether the table has heading.
            """
            try:
                # Open the CSV file.
                with open(path, encoding=encoding) as f:
                    data = [*csv.reader(f)]

                # Convert the data into objects.
                targetData = {r[0]: np.array(r[1:]) for r in data if len(r)}
                # targetData = np.vstack([np.array(r, dtype=object) for r in data if len(r)])

                # Save the information.
                self.encoding = encoding
                self._hasHeader = hasHeader

            except:
                raise ValueError("CSV file cannot be correctly read.")

            # Set this generator detail with the required data.
            # self.setData(inputArray = targetData, hasHeader = hasHeader)
            return targetData
     
        def readFile(self, path="", encoding="utf8", hasHeader = True):
            """
                Read the file to create data of this generator detail.   --- UPDATED (Trista) 20200619

                Parameters
                ------------------------------

                path        `str`   - The full/relative file path of the dataset file.

                encoding        `str`   - Encoding format of the file.

                hasHeader      `bool`  - Whether the table has heading.
            """
            sourceType: str = self.sourceType
            
            if not sourceType:
                # Read file in parseFromJSON
                try:
                    # Set the data table.
                    self._hasHeader = hasHeader
                    self._labelDict = self.readCSVFile(self._path + "label.csv")
                    # self._labelCSVData = self.readCSVFile(self._path + "label.csv")

                    (_, _, fileList) = next(os.walk(self._path+"traindata"))
                    self._imgData = np.vstack([np.array([tf.image.convert_image_dtype(tf.image.decode_image(tf.io.read_file(self._path+"traindata/"+f)), tf.float32)]) for f in fileList if len(f)])
                    self._labelData = np.vstack([np.array(self._labelDict[f]) for f in fileList if len(f)])
                except:
                    (_, _, fileList) = next(os.walk(self._path+"traindata"))
                    self._imgData = np.vstack([np.array([tf.image.convert_image_dtype(tf.image.decode_image(tf.io.read_file(self._path+"traindata/"+f)), tf.float32)]) for f in fileList if len(f)])
                    self._labelData = np.vstack([np.array([tf.image.convert_image_dtype(tf.image.decode_image(tf.io.read_file(self._path+"trainlabel/"+f)), tf.float32)]) for f in fileList if len(f)])

            elif (sourceType == "imageclassify"):
                # Set the data table.
                self._hasHeader = hasHeader
                self._labelDict = self.readCSVFile(self._path + "label.csv")
                # self._labelCSVData = self.readCSVFile(self._path + "label.csv")

                (_, _, fileList) = next(os.walk(self._path+"traindata"))
                self._imgData = np.vstack([np.array([tf.image.convert_image_dtype(tf.image.decode_image(tf.io.read_file(self._path+"traindata/"+f)), tf.float32)]) for f in fileList if len(f)])
                self._labelData = np.vstack([np.array(self._labelDict[f]) for f in fileList if len(f)])
                # self._labelData = np.vstack([np.array(self._labelDict[f]) for f in fileList if len(f)])

            elif (sourceType == "imageimage"):
                (_, _, fileList) = next(os.walk(self._path+"traindata"))
                self._imgData = np.vstack([np.array([tf.image.convert_image_dtype(tf.image.decode_image(tf.io.read_file(self._path+"traindata/"+f)), tf.float32)]) for f in fileList if len(f)])
                self._labelData = np.vstack([np.array([tf.image.convert_image_dtype(tf.image.decode_image(tf.io.read_file(self._path+"trainlabel/"+f)), tf.float32)]) for f in fileList if len(f)])

            # Set this generator detail with the required data.
            self.setData(inputArray = self._imgData, outputArray = self._labelData, hasHeader = hasHeader)

            # Set this generator detail with the required data.
            # self.setData(inputArray = inputData, hasHeader = hasHeader)
        
        @property
        def path(self) -> str:
            """
                Get the full/relative file path of the dataset file. reading by this data generator.   --- UPDATED (Dexter) 20190713

                Returns
                ------------------------------

                `str` - The full/relative file path of the dataset file.
            """
            return self._path
        
        @path.setter
        def path(self, val:str):
            """
                Set the full/relative path of the dataset file to read.   --- UPDATED (Trista) 20200619

                Parameters
                ------------------------------

                val     `str`       - The full/relative path of the dataset file to read.
            """
            self._path = val
            self.readFile(path=self._path, encoding=self.encoding, hasHeader = self.hasHeader)

        def prepareIteration(self):
            '''
			    Prepare a new round of data source iteration.   --- UPDATED (Trista) 20200619
            '''
            # 1. Prepare a standalone TF Graph for the input pipeline and define necessary variables.
            batchSize: int = self.batchSize

            # 2. Read file and create data.
            if (len(self._imgData) == 0 or len(self._labelData) == 0):
                self.readFile(path=self._path, encoding=self.encoding, hasHeader = self.hasHeader)

            # 3. Prepare the dataset and data preprocessing.
            dataset: 'tf.data.Dataset' = tf.data.Dataset.from_tensor_slices((self._imgData, self._labelData))
            dataset = dataset.map(lambda x, y: self.mapDatasetImages(x, y))

            # 4. Set dataset.
            if (self.shuffle): 
                dataset = dataset.shuffle(buffer_size=self.epochSize, reshuffle_each_iteration=True)
            dataset = dataset.repeat()
            dataset = dataset.batch(batchSize, drop_remainder = self.dropRemainder)

            # 5. Prepare the data iterator.
            self._itr = dataset.__iter__()

        def mapDatasetImages(self, img, label) -> Dict[str, Any]: 
            """
                Get the dataset image for the raw root source and the propagated dpp nodes.   --- UPDATED (Trista) 20200619

                Parameters
                ------------------------------

                tfDatasetEle    `*`   - A TensorFlow Dataset element, the bytes for a single image (with its label).

                Returns
                ------------------------------

                `Source.PreprocessedData` - A tuple of the image class label, and the image pixel data with dimension (height, width, depth).
            """
            # Get the root data.
            rootData = {"target": label, "input": img}

            # Pretend a RootData to be pre-processed by data preprocessed node.
            sourceRoot = {}
            sourceRoot[self.attachObject.sourceID] = rootData
            pretendRootData = Train.RootData(sourceRoot)

            dppData = {dppKey: dppNode.getProcessedData(pretendRootData) for dppKey, dppNode in self.attachObject.propagateDppNodes.items()}
            return Source.PreprocessedData(rootData, dppData).toDict()

        def getNextBatch(self) -> 'np.ndarray|Source.PreprocessedData':
            '''
                Get the next batch of image.   --- UPDATED (Trista) 20200619

                Returns
                ------------------------------

                `dict<str,*>|Source.PreprocessedData` - Depending on whether labels are needed, the list will contains 2 arrays of labels and images respectively.
            '''
            sourceType: str = self.sourceType

            # Initialize the data if it is not initialized.
            if (not self._initialized):
                self.prepareIteration()
                self.initialize()
            
            # Prepare the next batch of data.
            # tmp = next(self._itr)
            processedData = Source.PreprocessedData.fromDict(next(self._itr))
            processedData.toArray()
            return processedData

        def partition(self, prop: float = 0.2, shuffle: float = False) -> Tuple['np.ndarray','np.ndarray']:
            '''
                Partition current data into 2 parts.   --- UPDATED (Trista) 20200619

                Parameters
                ------------------------------

                prop            `float`     - Proportion of original dataset to be the secondary dataset.

                shuffle         `bool`      - Whether to shuffle before data splitting.

                Returns
                ------------------------------

                `tuple<'np.ndarray','np.ndarray'>` - Primary and secondary datasets.
            '''
            # 0. Check total dataset size
            rowCount = self.epochSize
            portion = math.floor(rowCount*prop)
            # data = self.oriData.copy()[1:] if self.hasHeader else self.oriData.copy() 
            image = self._imgData
            label = self._labelData

            # 1. If needed, shuffle the data.
            if (shuffle):
                randomIdx = np.arange(len(label))
                np.random.shuffle(randomIdx)
                image = image[randomIdx]
                label = label[randomIdx]

            # 2. Split the dataset
            primaryData = (image[portion:], label[portion:])
            secondaryData = (image[:portion], label[:portion])

            # 3. Append the heading.
            # if (self.hasHeader):
            #     primaryData = np.vstack((self.oriData[[0]], primaryData))
            #     secondaryData = np.vstack((self.oriData[[0]], secondaryData))

            # 4. Assign the splitted datasets
            return (primaryData, secondaryData)

    class GeneratorController(DataGenerator.Controller):
        """
            Class representing a data generator controller for custom image data source.   --- UPDATED (Trista) 20200619
        """
        def __init__(self, attachObject: Union['Source.CustomImage', 'DataPreprocessing.Node.SourceLike']):
            """
                Create a @DataGenerator.Controller object for controlling data generator of a custom image data source.   --- UPDATED (Dexter) 20190921

                Parameters
                ------------------------------

                attachObject `(Source.Config|ModelNode.Config)` - The object this data generator controller is embedded with.
            """
            super().__init__(attachObject = attachObject)

        @property
        def detailType(self) -> Callable[..., 'Source.CustomImage.GeneratorDetail']:
            """
                Get the data generator detail type.   --- UPDATED (Dexter) 20190921

                Returns
                ------------------------------

                `Callable<*, Source.CustomImage.GeneratorDetail>`    - The type of the generator detail.
            """
            return Source.CustomImage.GeneratorDetail
        
        def setSourceType(self, sourceType: str, coreDataDir: str):
            '''
                Set the source type of this image data generator controller.   --- UPDATED (Trista) 20200619

                Parameters
                ------------------------------

                `str` - The source type string.

                `str` - The folder directory of the image source.
            '''
            # Cache the source type for all detail.
            for detail in self.values():
                detail.sourceType = sourceType

        def splitValidationDataset(self, validation: float = 0.1, randomFold: bool = False):
            """
                Split current data into training and validation datasets.   --- UPDATED (Trista) 20200619
            
                Parameters
                ------------------------------

                validation      `float`     - Proportion of original dataset to be as the validation dataset.

                randomFold `bool` - Whether to perform random sub-sampling validation.
            """
            # 0. Check total dataset size
            trainset = self[DataGenerator.Dataset.Types.Train]
            rowCount = trainset.epochSize

            # 1. Consider if random sub-sampling or k-fold is required.
            # 1A. If random sub-sampling is needed, subsample with random partition.
            if randomFold:
                (trainData, valData) = trainset.partition(prop = validation, shuffle = True)
            
            # 1B. If k-fold is required, cut the suitable part of the dataset.
            else:
                # 1B-1. Determine no. of validation count required.
                validationCount = math.floor(1/validation)
                i = self.validationTime % validationCount
                # data = trainset.oriData.copy()[1:] if trainset.hasHeader else trainset.oriData.copy() 
                image = self._imgData
                label = self._labelData

                # 1B-2. If it's the first time to split and this dataset needs shuffling, shuffle the dataset.
                if (i == 0 and trainset.shuffle): 
                    # np.random.shuffle(data)
                    randomIdx = np.arange(len(label))
                    np.random.shuffle(randomIdx)
                    image = image[randomIdx]
                    label = label[randomIdx]

                # 1B-3: Split the dataset by the index.
                valPortion = math.ceil(rowCount/validationCount)
                startIdx = i*valPortion
                endIdx = (i+1)*valPortion
                # trainData = np.vstack((data[:startIdx], data[endIdx:]))
                # valData = data[startIdx: endIdx]
                trainData = (np.vstack((image[:startIdx], image[endIdx:])), np.vstack((label[:startIdx], label[endIdx:])))
                valData = (image[startIdx: endIdx], label[startIdx: endIdx])

                # 1B-4. Append the heading.
                # if (trainset.hasHeader):
                #     trainData = np.vstack((trainset.oriData[[0]], trainData))
                #     valData = np.vstack((trainset.oriData[[0]], valData))
            
            # 2. Assign the sliced dataset to the training and validation part
            self[DataGenerator.Dataset.Types.ValidationTrain].setData(inputArray = trainData[0], outputArray = trainData[1], hasHeader = trainset.hasHeader)
            self[DataGenerator.Dataset.Types.Validation].setData(inputArray = valData[0], outputArray = valData[1], hasHeader = trainset.hasHeader)
            
            # 3. Increment the validation time by one
            self.validationTime += 1
        
        def splitTestDataset(self, test: float = 0.2, shuffle: float = False):
            """
                Split original train data into train and test sets.   --- UPDATED (Trista) 20200619

                Parameters
                ------------------------------

                test            `float`     - Proportion of original dataset to be as the test dataset.

                shuffle         `bool`      - Whether to shuffle before data splitting.
            """
            # Get the partiitoned datasets.
            trainset = self[DataGenerator.Dataset.Types.Train]
            (trainData, testData) = trainset.partition(prop = test, shuffle = shuffle)
            
            # Create the test dataset.
            self[DataGenerator.Dataset.Types.Train].setData(inputArray = trainData[0], outputArray = trainData[1], hasHeader = trainset.hasHeader)
            self[DataGenerator.Dataset.Types.Test].setData(inputArray = testData[0], outputArray = testData[1], hasHeader = trainset.hasHeader)
    
    def __init__(self, sourceType: str = "imageclassify", coreDataDir: str = "", name: str = ""):
        '''
			Create an @Source.CustomImage object, a type of @TrainingSource handling the image data sources.   --- UPDATED (Trista) 20200619
            
            Parameters
            ------------------------------
            
            sourceType      `str`           - The type of image source, only `cifar10`, `cifar100`, `fashion-mnist`, `mnist` is supported currently.

            coreDataDir     `str`           - The directory of the data folder places.

            flattenImg      `bool`          - Whether to flatten into 1D when generating the images from the source.
            
            name        `str`   - Name of this @Source.Table object.
        '''
        # super().__init__(sourceType = sourceType, coreDataDir = coreDataDir, flattenImg = flattenImg, name = name)
        super().__init__(name = name)
        self._instanceClass = Source.Types.CustomImage
        # `Source.Table.GeneratorController` - The data generator controller for this source.
        self.generatorController = Source.CustomImage.GeneratorController(self)

        # Initiate some common properties.
        # `str` - The image source type (the renowned image dataset) of this @Source.CustomImage object. 
        self._sourceType = None
        
        # `str` - The folder directory where the renowned data source is saved.
        self.coreDataDir = coreDataDir if (coreDataDir is not "") else ("D:/tmp/" + ("cifar10_data/" if sourceType == "cirfar10" else "cifar100_data/" if sourceType == "cirfar100" else "mnist" if sourceType == "mnist/" else "fashion-mnist" if sourceType == "fashion-mnist/" else ""))

        # Set the source type.        
        # if (sourceType):
        #     self.setSourceType(sourceType)

        # If there is given filename, set it as the trainset.
        if (len(coreDataDir)):
            self.generatorController[DataGenerator.Dataset.Types.Train]._path = coreDataDir
            self.generatorController[DataGenerator.Dataset.Types.Train].readFile(fileName = fileName, encoding = encoding, hasHeader = hasHeader)

    def parseJSON(self, obj: Dict[str, Any], train: 'Train'):
        """
            Parse a previously saved object into this @Source.Config object.   --- UPDATED (Trista) 20200619

            Parameters
            ------------------------------

            obj     `dict<str,*>`   - JSON object from Project file.
            
            train   `Train`         - The train object this @Source.Config object is attached in.
        """
        # Iterate the object keys and take actions on mapping back to the Source.Config Class.
        for k,v in obj.items():
            if (k == "generatorController"):
                # Parse generator controllers.
                self.generatorController.parseFromJSON(v)
                
            elif (k == "outputsetInfo"):
                # Get outputsetInfo as a dictionary.
                setattr(self, k, {ok: ov for (ok, ov) in v})
                self.outputsetInfo["input"] = Source.OutputsetInfo(self.outputsetInfo["input"]["header"], self.outputsetInfo["input"]["shape"])
                self.outputsetInfo["target"] = Source.OutputsetInfo(self.outputsetInfo["target"]["header"], self.outputsetInfo["target"]["shape"])

            elif (k == "_propagateDppNodes"):
                setattr(self, k, set(v))

            elif (k not in ["_testRatio", "_hasHeader", "batchSize", "colConfigs", "shuffle", "epochSize", "dppNodes", "_type","_train","train","_instanceClass"]):
                # For other properties, just directly assign the values.
                setattr(self, k, v)
        
        # Configure the data generator   
        if (self.sourceType):
            self.generatorController.setSourceType(self._sourceType, self.coreDataDir)

        # Assign the train.
        self.train = train

    @property
    def train(self) -> 'Train':
        """
            The @Train object where this @Source.Config belongs to. If this source is just constructed, it is not belonged to any training instance.   --- UPDATED (Dexter) 20190405

            Returns
            ------------------------------

            `Train` - The @Train object where this @Source.Config belongs to.
        """
        return self._train
    
    @train.setter
    def train(self, train: 'Train') -> 'Train':
        """
            Set the @Train object where this @Source.CustomImage belongs to. This method may be overridden in case some sources may need to auto-connect with data preprocessing nodes once it's attached to the @Train object.   --- UPDATED (Trista) 20200619

            Parameters
            ------------------------------

            train `Train` - The @Train object where this @Source.Config belongs to.
            
            Returns
            ------------------------------

            `Train` - The @Train object where this @Source.Config belongs to.
        """
        # Ensure all existing source are having splittable as this source.
        if any([s.splittable != self.splittable for s in train.sources]):
            raise ValueError("Sources are incompatible within the Train object. All sources should have consistent splittable nature.")

        # Assign reference to the train object.
        self._train = train

        # Assign this source into the source.
        train.sources.append(self)
        
        # Set the source type.
        self.generatorController.setSourceType(self._sourceType, self.coreDataDir)


        # Return the train object.
        return self._train

    @property
    def sourceType(self) -> str:
        ''' 
            The source type of this @Source.CustomImage object.   --- UPDATED (Trista) 20200619

            Returns
            ------------------------------

            `str` - The source type.
        '''
        return self._sourceType

class _SourceNoise(_SourceConfig):
    '''
			Class representing a noise data source.   --- UPDATED (Dexter) 20180630
    '''
    class GeneratorDetail(DataGenerator.Detail):
        """
            Class representing a data generator detail for noise sources.   --- UPDATED (Dexter) 20190713
        """
        def __init__(self, controller: 'Source.Noise.GeneratorController'):
            """
                Create a @DataGenerator.Detail object for generating data of csv sources.   --- UPDATED (Dexter) 20190713
                
                Parameters
                ------------------------------

                controller `Source.Noise.GeneratorController` - The dataset controller maintaining this data generator detail.
            """
            super().__init__(controller)
            # `list<int>` - The shape of the data.
            self._noiseShape = []
            # `dict<str, Source.OutputsetInfo>` - Dataset information specifying root information for this dataset.
            self.outputsetInfo = {}

        def copy(self) -> 'Source.Noise.GeneratorDetail':
            """
                Copy this generator detail   --- UPDATED (Dexter) 20191022

                Returns
                ------------------------------

                `Source.Noise.GeneratorDetail` - The copied generator detail object.
            """
            # Create a new detail object.
            newDetail: Source.Noise.GeneratorDetail = Source.Noise.GeneratorDetail(self.controller)

            # Copy all information.
            for k in ["_noiseShape", "outputsetInfo"]:
                setattr(newDetail, k, getattr(self, k))
            
            return newDetail

        def getData(self, batchSize: int = 1) -> 'tf.Tensor':
            """
                Get the data of with specific indexes.   --- UPDATED (Dexter) 20191024

                Parameters
                ------------------------------

                start       `int`       - Batch starting index (inclusive). If `None`, it notates `0`.

                end         `int`       - Batch ending index (exclusive). If `None`, it notes the epoch size.

                indexes     `list[int]` - A list of indexes of data rows.

                Returns
                ------------------------------

                `tf.Tensor` - The requested source data of this generator detail.
            """
            shape = self._noiseShape
            shape[0] = batchSize
            return tf.convert_to_tensor(np.random.random(size=shape), dtype=tf.float32)

        def getNextBatch(self) -> 'tf.Tensor':
            """
                Generate the next batch of data rows of the root source.   --- UPDATED (Dexter) 20190918

                Returns
                ------------------------------

                'tf.Tensor' - The next batch data. In the future, the data will be output as `tf.Tensor` objects.
            """
            # 1. Get the batch size.
            batchSize = self.batchSize

            # 2. Get the noise data.
            returnData: tf.Tensor = self.getData(batchSize = batchSize)

            # 3. Change to dpp format.
            dppData = {"noise": returnData}
            #return Source.PreprocessedData.fromDict(self._sess.run(self._nextTensor))
            processedData = Source.PreprocessedData(dppData, dppData)
            processedData.toArray()
            return processedData

        def getRandItems(self, randomSeed: int = None, randomCount: int = None) -> 'tf.Tensor':
            """Generate the random items of data. No action taken, and a sub-class should be used.   --- UPDATED (Dexter) 20200308

            Parameters
            ------------------------------

            randomSeed      `int`       - Random seed.

            randomCount     `int`       - The count of random items to collect.

            Returns
            ------------------------------

            'tf.Tensor' - Random items of data. In the future, the data will be output as `tf.Tensor` objects.
            """

            # 1. Get the batch size.
            batchSize = self.batchSize

            # 2. Get the noise data.
            returnData: tf.Tensor = self.getData(batchSize = batchSize)

            # 3. Change to dpp format.
            dppData = {"noise": returnData}
            #return Source.PreprocessedData.fromDict(self._sess.run(self._nextTensor))
            processedData = Source.PreprocessedData(dppData, dppData)
            processedData.toArray()
            return processedData

    class GeneratorController(DataGenerator.Controller):
        """
            Class representing a data generator controller for data table sources.   --- UPDATED (Dexter) 20190713
        """
        def __init__(self, attachObject: Union['Source.Noise', 'DataPreprocessing.Node.SourceLike']):
            """
                Create a @Source.Noise.GeneratorController object for controlling data generator of a table source.   --- UPDATED (Dexter) 20190915

                Parameters
                ------------------------------

                attachObject `(Source.Config|ModelNode.Config)` - The object this data generator controller is embedded with.
            """
            super().__init__(attachObject = attachObject)
        
        @property
        def detailType(self) -> Callable[..., 'Source.Noise.GeneratorDetail']:
            """
                Get the data generator detail type.   --- UPDATED (Dexter) 20190713

                Returns
                ------------------------------

                `Callable<*, Source.Noise.GeneratorDetail>`    - The type of the generator detail.
            """
            return Source.Noise.GeneratorDetail

    def __init__(self, sourceType = "noise", noiseShape = [None, 1], name: str = ""):
        '''
			Create an @Source.Image object, a type of @TrainingSource handling the image data sources.   --- UPDATED (Dexter) 20190814
            
            Parameters
            ------------------------------
            
            sourceType      `str`           - The type of image source, only `cifar10`, `cifar100`, `fashion-mnist`, `mnist` is supported currently.

            coreDataDir     `str`           - The directory of the data folder places.

            flattenImg      `bool`          - Whether to flatten into 1D when generating the images from the source.
            
            name        `str`   - Name of this @Source.Table object.
        '''
        # Create the Noise source config.
        super().__init__(name=name)
        # `Source.Types` - The instance class, as defined in @Source.Types .
        self._instanceClass = Source.Types.Noise
        # `Source.Noise.GeneratorController` - The data generator controller for this source.
        self.generatorController = Source.Noise.GeneratorController(self)
        # `list<int>` - The shape of the data.
        self._noiseShape = noiseShape

    def parseJSON(self, obj: Dict[str, Any], train: 'Train'):
        """
            Parse a previously saved object into this @Source.Config object.   --- UPDATED (Dexter) 20190713

            Parameters
            ------------------------------

            obj     `dict<str,*>`   - JSON object from Project file.
            
            train   `Train`         - The train object this @Source.Config object is attached in.
        """
        # Iterate the object keys and take actions on mapping back to the Source.Config Class.
        for k,v in obj.items():
            if (k == "generatorController"):
                # Parse generator controllers.
                self.generatorController.parseFromJSON(v)

            elif (k == "outputsetInfo"):
                # Get outputsetInfo as a dictionary.
                setattr(self, k, {ok: ov for (ok, ov) in v})

            elif (k == "_propagateDppNodes"):
                setattr(self, k, set(v))

            elif (k not in ["_testRatio", "_hasHeader", "batchSize", "colConfigs", "shuffle", "epochSize", "dppNodes", "_type","_train","train","_instanceClass"]):
                # For other properties, just directly assign the values.
                setattr(self, k, v)
                if (k == "_noiseShape"):
                    print(v)
        
        # Assign the train.
        self.train = train
        
        # Set noise shape and epoch size
        self.generatorController[DataGenerator.Dataset.Types.Train]._noiseShape = self._noiseShape
        self.generatorController[DataGenerator.Dataset.Types.Test]._noiseShape = self._noiseShape
        self.generatorController[DataGenerator.Dataset.Types.Train]._epochSize = self._epochSize 
        self.generatorController[DataGenerator.Dataset.Types.Test]._epochSize = self._epochSize

    def getHeader(self, outputsetKey: str) -> List[str]:
        """
            Vitual method to get the header names of this source.   --- UPDATED (Dexter) 20190506

            Parameters
            ------------------------------

            outputsetKey    `str` - The key for a outputset included in this source object .

            Returns
            ------------------------------

            `list<str>`     - A list of header names.
        """
        return self.outputsetInfo[outputsetKey]["header"][0] if outputsetKey in self.outputsetInfo else []

class Source:
    '''
        Module containing classes and functions regarding training sources.   --- UPDATED (Dexter) 20190302
    '''
    class Tensor:
        '''
            Class representing an object containing the placeholder tensors that the training source generates.   --- UPDATED (Dexter) 20180622
        '''
        def __init__(self,**kwargs):
            '''
                Create a @Source.Tensor object.   --- UPDATED (Dexter) 20180622
            '''
            # `dict<str,tf.Tensor>` - The tensors according to the data preprocessing nodes.
            self._tensors = {**kwargs}
        
        def __getitem__(self, key) -> 'tf.Tensor':
            '''
                Get a tensor using a key.   --- UPDATED (Dexter) 20180622

                Parameters
                ------------------------------

                key     `str`   - Key of the source tensor.

                Returns
                ------------------------------

                `tf.Tensor`     - The requested input source tensor.
            '''
            return self._tensors[key]
        
        def __iter__(self) -> Iterable[str]:
            '''
                Create an iterable object on the source tensor keys.   --- UPDATED (Dexter) 20180622
                
                Returns
                ------------------------------

                `iterable[str]`     - Iterable keys of the source tensors.
            '''
            return self._tensors.__iter__()
        
        def items(self) -> Iterable[Tuple[str, 'tf.Tensor']]:
            '''
                Create an iterable object on the source tensor keys and values.   --- UPDATED (Dexter) 20180622
                
                Returns
                ------------------------------

                `iterable[str,tf.Tensor]`     - Iterable keys of the source tensor keys and values.
            '''
            return self._tensors.items()
        
        def keys(self) -> Iterable[str]:
            '''
                Create an iterable object on the source tensor keys.   --- UPDATED (Dexter) 20180622
                
                Returns
                ------------------------------

                `iterable[str]`     - Iterable keys of the source tensors.
            '''
            return self._tensors.keys()

    class PreprocessedData:
        """
            Class representing a combo of root source data and preprocessed data, typically a structured output from Dataset API batched data.   --- UPDATED (Dexter) 20190508
        """
        def __init__(self, rootData: Any = None, dppData: Dict[str, Any] = {}):
            """
                Create a preprocessed data.   --- UPDATED (Dexter) 20190921

                Parameters
                ------------------------------

                rootData    `*`  - The original batched data.

                dppData     `dict<str, *>`  - Any preprocessed data already made during batched generation of the root source.
            """
            # `*` - The original batched data.
            self.rootData = rootData
            # `dict<str, *>`  - Any preprocessed data already made during batched generation of the root source.
            self.dppData = dppData
        
        def toArray(self):
            """
                Convert this object with Tensor data to array data.   --- UPDATED (Dexter) 20191001
            """
            # Check all information to ensure they're of np array objects.
            if (isinstance(self.rootData, dict)):
                self.rootData = {outputKey: np.array(self.rootData[outputKey]) for outputKey in self.rootData}
            else:
                self.rootData = np.array(self.rootData)
            self.dppData = {dppKey: np.array(self.dppData[dppKey]) for dppKey in self.dppData}
        
        def __add__(self, other: 'Source.PreprocessedData') -> 'Source.PreprocessedData':
            """
                Add two different preprocessed data.   --- UPDATED (Dexter) 20190921

                Parameters
                ------------------------------

                other `Source.PreprocessedData`- Another preprocessed data.

                Returns
                ------------------------------

                `Source.PreprocessedData`- The concatenated preprocessed data.
            """
            # Create the new object.
            newObj = Source.PreprocessedData()

            # Copy root data.
            if (isinstance(self.rootData, dict)):
                newObj.rootData = {outputKey: np.concatenate([self.rootData[outputKey], other.rootData[outputKey]]) for outputKey in self.rootData}
            else:
                newObj.rootData = np.concatenate(self.rootData, other.rootData)
            
            # Copy dpp data.
            newObj.dppData = {dppKey: np.concatenate([self.dppData[dppKey], other.dppData[dppKey]]) for dppKey in self.dppData}

            return newObj

        def toDict(self) -> Dict[str, Any]:
            """
                Convert this object into a `dict` object.   --- UPDATED (Dexter) 20190922

                Returns
                ------------------------------

                `dict<str,*>` - A `dict` object representation of this @Source.PreprocessedData object.
            """
            return {"rootData": self.rootData, "dppData": self.dppData}
        
        @staticmethod
        def fromDict(dictObj: Dict[str, Any]) -> 'Source.PreprocessedData':
            """
                Convert a `dict` object into a @Source.PreprocessedData object.   --- UPDATED (Dexter) 20190922

                Parameters
                ------------------------------

                `dict<str,*>` - A @Source.PreprocessedData -like dictionary object.
            """
            return Source.PreprocessedData(dictObj["rootData"], dictObj["dppData"])

    class OutputsetInfo:
        """
            Class representing basic root info for a part of dataset in a training source.   --- UPDATED (Dexter) 20190411
        """
        def __init__(self, header: List[str] = [], shape: List[int] = None):
            """
                Create an outputset information specifying the output details of a part of dataset in a training source.   --- UPDATED (Dexter) 20190411

                Parameters
                ------------------------------

                header      `list<str>`         - The header of the dataset.

                shape       `list<(int|str)>`   - The output shape of the dataset.
            """
            # `list<str>` - The header of the dataset.
            self.header = header
            # `list<(int|str)>` - The output shape of the dataset.
            self.shape = shape

    class Types(Enumeration):
        '''
			Enumeration defining the type of a @Source.Config object.   --- UPDATED (Dexter) 20190302
        '''
        # `int` - Abstract class representing a data source. (Ref: @Source.Config )
        Config = 0
        # `int` - Class representing a table data source. (Ref: @Source.Table )
        Table = 1
        # `int` - Class representing a CSV table data source. (Ref: @Source.CSV )
        CSV = 2
        # `int` - Class representing a centralized class for different image datasets as an input source. (Ref: @Source.Image )
        Image = 3
        # `int` - Class representing a custom image data source. (Ref: @Source.CustomImage )
        CustomImage = 4
        # `int` - Class representing a Noise table data source. (Ref: @Source.Noise )
        Noise = 5
        # `int` - Class representing a MySQL data source. (Ref: @Source.MySQL )
        MySQL = 6
        # `int` - Class representing a MSSQL data source. (Ref: @Source.MySQL )
        MSSQL = 7

    @staticmethod
    def createFromJSON(obj: Dict[str, Any], train: 'Train') -> 'Source.Config':
        """
            Parse a JSON object to a valid @Source.Config object.   --- UPDATED (Dexter) 20190425

            Parameters
            ------------------------------

            obj     `dict<str,*>`   - A JSON-object.

            train   `Train`         - Attach to the train object directly if provided.

            Returns
            ------------------------------

            `Source.Config`         - The parsed @Source.Config object.
        """
        # Determine the object from its instance class.
        # Old version compatability: no "_instanceClass" in obj; MINVER: v1903.02, MAXVER: v1905
        if ("_instanceClass" in obj):
            source = getattr(Source, Source.Types.getName(obj["_instanceClass"]))()
        elif ("_type" in obj):
            if (obj["_type"] == "TableSource" or obj["_type"] == "Table"):
                source = Source.Table()
            elif (obj["_type"] == "CSVSource" or obj["_type"] == "CSV"):
                source = Source.CSV()
            elif (obj["_type"] == "ImageSource" or obj["_type"] == "Image"):
                source = Source.Image()
        else:
            raise ValueError("Not supported JSON format for parsing a Source.Config object.")
        
        # Parse the JSON object.
        source.parseJSON(obj, train)

        return source

    Config = _SourceConfig

    Table = _SourceTable

    CSV = _SourceCSV

    Image = _SourceImage

    CustomImage = _SourceCustomImage

    Noise = _SourceNoise

    MySQL = _SourceMySQL

    MSSQL = _SourceMSSQL

class Event():
    '''
			Class representing an event.    --- UPDATED (Dexter) 20180622
    '''
    def __init__(self, typeArg, eventInit):
        '''
			Create an event.    --- UPDATED (Dexter) 20180622

            Parameters
            ------------------------------

            typeArg     `str`           - The event type of this event.

            eventInit   `dict{str: *}`  - The event initiation object.
        '''
        self._type = typeArg
        self._defaultPrevented = False
        self._target = eventInit["target"] if "target" in eventInit else None

    @property
    def defaultPrevented(self):
        '''
			Property of whether the event is prevented for the default behaviour.    --- UPDATED (Dexter) 20180622

            Returns
            ------------------------------

            `bool`      - Boolean value of whether default behaviour is prevented.
        '''
        return self._defaultPrevented

    @property
    def target(self):
        '''
			Get the target object of this event.    --- UPDATED (Dexter) 20180622

            Returns
            ------------------------------

            `*`         - The Event target.
        '''
        return self._target
    
    @property
    def type(self):
        '''
			Get the type name of this event.    --- UPDATED (Dexter) 20180622

            Returns
            ------------------------------

            `str`        - The Event type.
        '''
        return self._type

    def preventDefault(self):
        '''
			Prevent default behaviour of this event.    --- UPDATED (Dexter) 20180622
        '''
        self._defaultPrevented = True

class BuildEvent(Event):
    '''
			Class representing a training build event, typically on building a new unique model.   --- UPDATED (Dexter) 20181028
    '''
    def __init__(self, typeArg, buildEventInit):
        '''
			Create a new BuildEvent.   --- UPDATED (Dexter) 20181028
            
            Parameters
            ------------------------------

            typeArg     `str`           - The event type of this event.

            eventInit   `dict{str: *}`  - The event initiation object.
        '''
        # Ensure the type is one of the build events.
        if typeArg not in ["buildstart", "buildend"]:
            raise ValueError("BuildEvent can only have `buildstart` or `buildend`.")
        
        # Create the event, and assign specific event attributes.
        super().__init__(typeArg, buildEventInit)
        self.buildNo = buildEventInit["buildNo"] if "buildNo" in buildEventInit else -1
        self.runNo = buildEventInit["runNo"] if "runNo" in buildEventInit else -1
        self.cvNo = buildEventInit["cvNo"] if "cvNo" in buildEventInit else -1

class TrainEvent(Event):
    '''
			Class representing a training train event, typically on training a model.   --- UPDATED (Dexter) 20181028
    '''
    def __init__(self, typeArg, trainEventInit):
        '''
			Create a new TrainEvent.   --- UPDATED (Dexter) 20181028
            
            Parameters
            ------------------------------

            typeArg     `str`           - The event type of this event.

            eventInit   `dict{str: *}`  - The event initiation object.
        '''
        # Ensure the type is one of the train events.
        if typeArg not in ["trainbuild", "trainstart", "trainend"]:
            raise ValueError("BuildEvent can only have `trainbuild`, `trainstart` or `trainend`.")

        # Create the event, and assign specific event attributes.
        super().__init__(typeArg, trainEventInit)
        self.buildNo = trainEventInit["buildNo"] if "buildNo" in trainEventInit else -1
        self.runNo = trainEventInit["runNo"] if "runNo" in trainEventInit else -1
        self.cvNo = trainEventInit["cvNo"] if "cvNo" in trainEventInit else -1
        self.buildConfig = trainEventInit["buildConfig"] if "buildConfig" in trainEventInit else None
            
class StepEvent(Event):
    '''
			Class representing a training step event, typically on training step when training a model.   --- UPDATED (Dexter) 20181028
    '''
    def __init__(self, typeArg, stepEventInit):
        '''
			Create a new StepEvent.   --- UPDATED (Dexter) 20181028
            
            Parameters
            ------------------------------

            typeArg     `str`           - The event type of this event.

            eventInit   `dict{str: *}`  - The event initiation object.
        '''
        # Ensure the type is one of the step events.
        if typeArg not in ["stepprepare", "stepstart", "stepend"]:
            raise ValueError("StepEvent can only have `stepbuild`, `stepstart` or `stepend`.")
        
        # Create the event, and assign specific event attributes.
        super().__init__(typeArg, stepEventInit)
        self.buildNo = stepEventInit["buildNo"] if "buildNo" in stepEventInit else -1
        self.runNo = stepEventInit["runNo"] if "runNo" in stepEventInit else -1
        self.cvNo = stepEventInit["cvNo"] if "cvNo" in stepEventInit else -1
        self.buildConfig = stepEventInit["buildConfig"] if "buildConfig" in stepEventInit else None
        self.i = stepEventInit["i"] if "i" in stepEventInit else -1
        self.localStep = stepEventInit["localStep"] if "localStep" in stepEventInit else -1
        self.globalStep = stepEventInit["globalStep"] if "globalStep" in stepEventInit else -1
        self.feedDict = stepEventInit["feedDict"] if "feedDict" in stepEventInit else None
        self.totalLoss = stepEventInit["totalLoss"] if "totalLoss" in stepEventInit else None
        self.avgLoss = stepEventInit["avgLoss"] if "avgLoss" in stepEventInit else None

class EpochEvent(Event):
    '''
			Class representing a epoch event, typically on finishing an epoch when training a model.   --- UPDATED (Dexter) 20181028
    '''
    def __init__(self, typeArg, epochEventInit):
        '''
			Create a new EpochEvent.   --- UPDATED (Dexter) 20181028
            
            Parameters
            ------------------------------

            typeArg     `str`           - The event type of this event.

            eventInit   `dict{str: *}`  - The event initiation object.
        '''
        # Ensure the type is one of the epoch events.
        if typeArg not in ["epochstart", "epochend"]:
            raise ValueError("EpochEvent can only have `epochstart` or `epochend`.")

        # Create the event, and assign specific event attributes.
        super().__init__(typeArg, epochEventInit)
        self.runNo = epochEventInit["runNo"] if "runNo" in epochEventInit else -1
        self.cvNo = epochEventInit["cvNo"] if "cvNo" in epochEventInit else -1
        self.buildNo = epochEventInit["buildNo"] if "buildNo" in epochEventInit else -1
        self.buildConfig = epochEventInit["buildConfig"] if "buildConfig" in epochEventInit else None
        self.i = epochEventInit["i"] if "i" in epochEventInit else -1
        self.localStep = epochEventInit["localStep"] if "localStep" in epochEventInit else -1
        self.globalStep = epochEventInit["globalStep"] if "globalStep" in epochEventInit else -1
        self.inputSources = epochEventInit["inputSources"] if "inputSources" in epochEventInit else None

class _TrainVariableInitializer:
    """
        Module containing classes on variable initializers.   --- UPDATED (Dexter) 20190220
    """
    class Types(Enumeration):
        """
            Enumeration defining the type of a @Train.Variable.Initializer.Config object.   --- UPDATED (Dexter) 20190220
        """
        # `int` - Abstract class representing a training variable initializer (Ref: @Train.Variable.Initializer.Config object).
        Config = 0
        # `int` - Class representing a configuration of training variable initializer with constant value (Ref: @Train.Variable.Initializer.Constant object).
        Constant = 1
        # `int` - Class representing a configuration of training variable initializer with constant value zero (`0`). (Ref: @Train.Variable.Initializer.Zeros object).
        Zeros = 2
        # `int` - Class representing a configuration of training variable initializer with constant value one (`1`). (Ref: @Train.Variable.Initializer.Ones object).
        Ones = 3
        # `int` - Class representing a configuration of training variable initializer following a normal distribution. (Ref: @Train.Variable.Initializer.RandomNormal object).
        RandomNormal = 4
        # `int` - Class representing a configuration of training variable initializer following a normal distribution with no values exceeding 2 standard deviations from the mean. (Ref: @Train.Variable.Initializer.TruncatedNormal object).
        TruncatedNormal = 5
        # `int` - Class representing a configuration of training variable initializer following a uniform distribution within a range. (Ref: @Train.Variable.Initializer.RandomUniform object).
        RandomUniform = 6
        # `int` - Class representing a configuration of training variable initializer generating an orthogonal matrix. (Ref: @Train.Variable.Initializer.Orthogonal object).
        Orthogonal = 7
        # `int` - Class representing a configuration of training variable initializer generating an identity matrix. (Ref: @Train.Variable.Initializer.Identity object).
        Identity = 8
        # `int` - Class representing a configuration of training variable glorot normal initializer. (Ref: @Train.Variable.Initializer.GlorotNormal object).
        GlorotNormal = 9
        # `int` - Class representing a configuration of training variable glorot uniform initializer. (Ref: @Train.Variable.Initializer.GlorotUniform object).
        GlorotUniform = 10
        # `int` - Class representing a configuration of training variable he normal initializer. (Ref: @Train.Variable.Initializer.HeNormal object).
        HeNormal = 11
        # `int` - Class representing a configuration of training variable he uniform initializer. (Ref: @Train.Variable.Initializer.HeUniform object).
        HeUniform = 12
        # `int` - Class representing a configuration of training variable LeCun normal initializer. (Ref: @Train.Variable.Initializer.LecunNormal object).
        LecunNormal = 13
        # `int` - Class representing a configuration of training variable LeCun uniform initializer. (Ref: @Train.Variable.Initializer.LecunUniform object).
        LecunUniform = 14
    
    class Config():
        '''
            Class representing a variable initializer.   --- UPDATED (Dexter) 20181125
        '''
        def __init__(self, referenceCallable: Callable):
            '''
                Create an initializer.   --- UPDATED (Dexter) 20200120

                Parameters
                ------------------------------

                `Callable`  - A callable function, which should be the corresponding TensorFlow initializer.

            '''
            self._reference = referenceCallable
            self.constructor = ""

        def parseJSON(self, obj: Dict[str, Any]):
            """
                Parse a previously saved object into this class of @Train.Variable.Initializer.Config object.   --- UPDATED (Dexter) 20181128

                Parameters
                ------------------------------

                obj `dict<str,*>` - JSON object from Project file.
            """
            # Iterate the object keys and take actions on mapping back to the @Train.Variable.Initializer.Config Class.
            for (k, v) in obj.items():
                setattr(self, k, v)

        def getConfig(self) -> Dict[str, Any]:
            '''
                Get a configuration dict object describing this variable initializer.   --- UPDATED (Dexter) 20180920

                Returns
                ------------------------------

                `dict[str, *]`  - A configuration dict object describing this variable initializer.
            '''
            return {}

        def getConstructorString(self) -> str:
            """
                Get the constructor string.   --- UPDATED (Dexter) 20180920

                Returns
                ------------------------------

                `str` - An initializer constructor string (without module name / namespace).
            """
            constructorString = self.constructor
            paramString = []
            for (key, val) in self.getConfig().items():
                paramString.append(key + " = " + str(val))
            return constructorString + "(" + ", ".join(paramString) + ")"
        
        def getTrainingInitializer(self, shape: List[int] = None) -> 'tf.initializers':
            '''
                Returns a configuration dict object.   --- UPDATED (Dexter) 20181109

                Parameters
                ------------------------------

                shape   `list[int]`     - The shape of the variable. (Optional for different subclasses.)

                Returns
                ------------------------------

                `tf.initializers`       - An initializer tensor.
            '''
            return self._reference(**self.getConfig())
        
        def fromConfig(self, initializer: type, config: Dict[str, Any]) -> 'VarInitalizer':
            '''
            Create a variable initializer using a configuration dict object.   --- UPDATED (Dexter) 20190118

                Parameters
                ------------------------------

            '''
            return initializer(**config)

    class Constant(Config):
        '''
            Variable initializer with constant value.   --- UPDATED (Dexter) 20180920
        '''
        def __init__(self, value: Any = 0):
            '''
            Create a constant variable initializer.   --- UPDATED (Dexter) 20191007

                Parameters
                ------------------------------

                value   `*` - A constant value of this initializer.
            '''
            super().__init__(tf.initializers.constant)
            self.value = value
            self.constructor = "constant"
        
        def getConfig(self) -> Dict[str, Any]:
            '''
            Get a configuration dict object describing this variable initializer.   --- UPDATED (Dexter) 20180920

                Returns
                ------------------------------

                `dict[str, *]`  - A configuration dict object describing this variable initializer.
            '''
            return {"value": self.value}

    class Zeros(Config):
        '''
            Variable initializer with constant value zero.   --- UPDATED (Dexter) 20190118
        '''
        def __init__(self):
            '''
            Create a zero-constant variable initializer.   --- UPDATED (Dexter) 20181125
            '''
            super().__init__(tf.initializers.zeros)
            self.constructor = "zeros"

    class Ones(Config):
        '''
            Variable initializer with constant value one.   --- UPDATED (Dexter) 20180920
        '''
        def __init__(self):
            '''
            Create a one-constant variable initializer.   --- UPDATED (Dexter) 20190118
            '''
            super().__init__(tf.initializers.ones)
            self.constructor = "ones"

    class RandomNormal(Config):
        '''
            Random value variable initializer following a normal distribution.   --- UPDATED (Dexter) 20180920
        '''
        def __init__(self, mean: float = 0, stddev: float = 1.0, seed: int = None):
            '''
            Create a random variable initializer following a normal distribution.   --- UPDATED (Dexter) 20190118

                Parameters
                ------------------------------

                mean    `float` - The mean of the normal distribution of the random varaible initializer.

                stddev  `float` - The standard deviation of the normal distribution of the random varaible initializer.

                seed    `int`   - Python random seed.
            '''
            super().__init__(tf.initializers.RandomNormal)
            self.mean = mean
            self.stddev = stddev
            self.seed = seed
            self.constructor = "RandomNormal"
        
        def getConfig(self) -> Dict[str, Any]:
            '''
            Get a configuration dict object describing this variable initializer.   --- UPDATED (Dexter) 20180920

                Returns
                ------------------------------

                `dict[str, *]`  - A configuration dict object describing this variable initializer.
            '''
            return {"mean": self.mean, "stddev": self.stddev, "seed": self.seed}

        def getTrainingInitializer(self, shape: List[int] = None) -> 'tf.initializers':
            '''
                Returns a configuration dict object. (override)   --- UPDATED (Dexter) 20190922

                Parameters
                ------------------------------

                shape   `list[int]`     - The shape of the variable. (Optional for different subclasses.)

                Returns
                ------------------------------

                `tf.initializers`       - An initializer tensor.
            '''
            configObj = self.getConfig()
            # Implementation of auto stddev.
            if (configObj["stddev"] == "auto"):
                configObj["stddev"] = 1/shape[0]
            return self._reference(**configObj)

    class TruncatedNormal(Config):
        '''
            Random value variable initializer following a truncated normal distribution with no values exceeding 2 standard deviations from the mean.   --- UPDATED (Dexter) 20180920
        '''
        def __init__(self, mean: float = 0, stddev: Union[float, str] = 1.0, seed: int = None):
            '''
            -Create a random variable initializer following a truncated normal distribution with no values exceeding 2 standard deviations from the mean.   --- UPDATED (Dexter) 20190118

                Parameters
                ------------------------------

                mean    `float` - The mean of the normal distribution of the random varaible initializer.

                stddev  `float` - The standard deviation of the normal distribution of the random varaible initializer. If "auto", it will be the 1/(first dimension of weight matrix) .
                
                seed    `int`   - Python random seed.
            '''
            super().__init__(tf.initializers.TruncatedNormal)
            self.mean = mean
            self.stddev = stddev
            self.seed = seed
            self.constructor = "TruncatedNormal"

        def getConfig(self) -> Dict[str, Any]:
            '''
                Get a configuration dict object describing this variable initializer.   --- UPDATED (Dexter) 20180920

                Returns
                ------------------------------

                `dict[str, *]`  - A configuration dict object describing this variable initializer.
            '''
            return {"mean": self.mean, "stddev": self.stddev, "seed": self.seed}
        
        def getTrainingInitializer(self, shape: List[int] = None) -> 'tf.initializers':
            '''
                Returns a configuration dict object. (override)   --- UPDATED (Dexter) 20181109

                Parameters
                ------------------------------

                shape   `list[int]`     - The shape of the variable. (Optional for different subclasses.)

                Returns
                ------------------------------

                `tf.initializers`       - An initializer tensor.
            '''
            configObj = self.getConfig()
            # Implementation of auto stddev.
            if (configObj["stddev"] == "auto"):
                configObj["stddev"] = 1/shape[0]
            return self._reference(**configObj)

    class RandomUniform(Config):
        '''
            Random value variable initializer following a uniform distribution within a range.   --- UPDATED (Dexter) 20180920
        '''
        def __init__(self, minval: float = -1.0, maxval: float = 1.0, seed: int = None):
            '''
            Create a random variable initializer following a uniform distribution within a range.   --- UPDATED (Dexter) 20190118

                Parameters
                ------------------------------

                minval  `float` - The minimum possible initalized value of random varaible initializer.

                maxval  `float` - The maximum possible initalized value the random varaible initializer.
                
                seed    `int`   - Python random seed.
            '''
            super().__init__(tf.initializers.RandomUniform)
            self.minval = minval
            self.maxval = maxval
            self.seed = seed
            self.constructor = "RandomUniform"

        def getConfig(self) -> Dict[str, Any]:
            '''
            Get a configuration dict object describing this variable initializer.   --- UPDATED (Dexter) 20180920

                Returns
                ------------------------------

                `dict[str, *]`  - A configuration dict object describing this variable initializer.
            '''
            return {"minval": self.minval, "maxval": self.maxval, "seed": self.seed}

    class Orthogonal(Config):
        '''
            Random variable initializer generating an orthogonal matrix.   --- UPDATED (Dexter) 20180920
        '''
        def __init__(self, gain: float = 1.0, seed: int = None):
            '''
            Create a random variable initializer generating an orthogonal matrix.   --- UPDATED (Dexter) 20190118

                Parameters
                ------------------------------

                gain  `float`   - The multiplicative factor applying on the orthogonal matrix.
                
                seed    `int`   - Python random seed.
            '''
            super().__init__(tf.initializers.Orthogonal)
            self.gain = gain
            self.seed = seed
            self.constructor = "Orthogonal"

        def getConfig(self) -> Dict[str, Any]:
            '''
            Get a configuration dict object describing this variable initializer.   --- UPDATED (Dexter) 20180920

                Returns
                ------------------------------

                `dict[str, *]`  - A configuration dict object describing this variable initializer.
            '''
            return {"gain": self.gain, "seed": self.seed}

    class Identity(Config):
        '''
            Variable initializer generating an identity matrix.   --- UPDATED (Dexter) 20180920
        '''
        def __init__(self, gain: float = 1.0):
            '''
            Create a variable initializer generating an identity matrix.   --- UPDATED (Dexter) 20190118

                Parameters
                ------------------------------

                gain  `float`   - The multiplicative factor applying on the identity matrix.
            '''
            super().__init__(tf.initializers.Identity)
            self.gain = gain
            self.constructor = "Identity"

        def getConfig(self) -> Dict[str, Any]:
            '''
            Get a configuration dict object describing this variable initializer.   --- UPDATED (Dexter) 20180920

                Returns
                ------------------------------

                `dict[str, *]`  - A configuration dict object describing this variable initializer.
            '''
            return {"gain": self.gain}

    class GlorotNormal(Config):
        '''
            A glorot normal initializer.   --- UPDATED (Dexter) 20180920
        '''
        def __init__(self, seed: int = None):
            '''
            Create a glorot normal initializer.   --- UPDATED (Dexter) 20190118

                Parameters
                ------------------------------

                seed  `float`   - Python random seed.
            '''
            super().__init__(tf.initializers.GlorotNormal)
            self.seed = seed
            self.constructor = "GlorotNormal"

        def getConfig(self) -> Dict[str, Any]:
            '''
            Get a configuration dict object describing this variable initializer.   --- UPDATED (Dexter) 20180920

                Returns
                ------------------------------

                `dict[str, *]`  - A configuration dict object describing this variable initializer.
            '''
            return {"seed": self.seed}

    class GlorotUniform(Config):
        '''
            A glorot uniform initializer.   --- UPDATED (Dexter) 20180920
        '''
        def __init__(self, seed: int = None):
            '''
            Create a glorot uniform initializer.   --- UPDATED (Dexter) 20190118

                Parameters
                ------------------------------

                seed  `float`   - Python random seed.
            '''
            super().__init__(tf.initializers.GlorotUniform)
            self.seed = seed
            self.constructor = "GlorotUniform"

        def getConfig(self) -> Dict[str, Any]:
            '''
            Get a configuration dict object describing this variable initializer.   --- UPDATED (Dexter) 20180920

                Returns
                ------------------------------

                `dict[str, *]`  - A configuration dict object describing this variable initializer.
            '''
            return {"seed": self.seed}

    class HeNormal(Config):
        '''
            A he normal initializer.   --- UPDATED (Dexter) 20180920
        '''
        def __init__(self, seed: int = None):
            '''
            Create a he normal initializer.   --- UPDATED (Dexter) 20190118

                Parameters
                ------------------------------

                seed  `float`   - Python random seed.
            '''
            super().__init__(tf.initializers.he_normal)
            self.seed = seed
            self.constructor = "he_normal"

        def getConfig(self) -> Dict[str, Any]:
            '''
            Get a configuration dict object describing this variable initializer.   --- UPDATED (Dexter) 20180920

                Returns
                ------------------------------

                `dict[str, *]`  - A configuration dict object describing this variable initializer.
            '''
            return {"seed": self.seed}

    class HeUniform(Config):
        '''
            A he uniform initializer.   --- UPDATED (Dexter) 20180920
        '''
        def __init__(self, seed: int = None):
            '''
            Create a he uniform initializer.   --- UPDATED (Dexter) 20190118

                Parameters
                ------------------------------

                seed  `float`   - Python random seed.
            '''
            super().__init__(tf.initializers.he_uniform)
            self.seed = seed
            self.constructor = "he_uniform"

        def getConfig(self) -> Dict[str, Any]:
            '''
            Get a configuration dict object describing this variable initializer.   --- UPDATED (Dexter) 20180920

                Returns
                ------------------------------

                `dict[str, *]`  - A configuration dict object describing this variable initializer.
            '''
            return {"seed": self.seed}

    class LecunNormal(Config):
        '''
            A LeCun normal initializer.   --- UPDATED (Dexter) 20180920
        '''
        def __init__(self, seed: int = None):
            '''
            Create a LeCun normal initializer.   --- UPDATED (Dexter) 20190118

                Parameters
                ------------------------------

                seed  `float`   - Python random seed.
            '''
            super().__init__(tf.initializers.lecun_normal)
            self.seed = seed
            self.constructor = "lecun_normal"

        def getConfig(self) -> Dict[str, Any]:
            '''
            Get a configuration dict object describing this variable initializer.   --- UPDATED (Dexter) 20180920

                Returns
                ------------------------------

                `dict[str, *]`  - A configuration dict object describing this variable initializer.
            '''
            return {"seed": self.seed}

    class LecunUniform(Config):
        '''
            A LeCun uniform initializer.   --- UPDATED (Dexter) 20180920
        '''
        def __init__(self, seed: int = None):
            '''
            Create a LeCun uniform initializer.   --- UPDATED (Dexter) 20190118

                Parameters
                ------------------------------

                seed  `float`   - Python random seed.
            '''
            super().__init__(tf.initializers.lecun_uniform)
            self.seed = seed
            self.constructor = "lecun_uniform"

        def getConfig(self) -> Dict[str, Any]:
            '''
            Get a configuration dict object describing this variable initializer.   --- UPDATED (Dexter) 20180920

                Returns
                ------------------------------

                `dict[str, *]`  - A configuration dict object describing this variable initializer.
            '''
            return {"seed": self.seed}

class _TrainVariableConfig():
    '''
        Class representing a variable configuration.   --- UPDATED (Dexter) 20181125
    '''
    def __init__(self, initializer: 'Train.Variable.Initializer.Config' = _TrainVariableInitializer.TruncatedNormal(), l1Loss: bool = False, l2Loss: bool = False, l1Decay: float = None, l2Decay: float = None, device: str = None):
        '''
			Create a variable initializer.   --- UPDATED (Dexter) 20181125

            Parameters
            ------------------------------

            initializer         `Train.Variable.Initializer.Config`     - The initializer configuration of this variable; default as the truncated normal initializer.

            l1Loss              `bool`          - Whether to take L1-loss on this variable.

            l2Loss              `bool`          - Whether to take L2-loss on this variable.

            l1Decay             `float`         - The constant for weighting L1-loss of this variable.

            l2Decay             `float`         - The constant for weighting L2-loss of this variable.

            device              `str`           - The device like CPU or GPU this training will rely on.
        '''
        self.initializer = initializer
        self.l1Loss = l1Loss
        self.l2Loss = l2Loss
        self.l1Decay = l1Decay
        self.l2Decay = l2Decay
        self.device = device
    
    def parseJSON(self, obj: Dict[str, Any]):
        """
            Parse a previously saved object into this class of @Train.Variable.Config.   --- UPDATED (Dexter) 20181124

            Parameters
            ------------------------------

            obj `dict<str,*>` - JSON object from Project file
        """
        # Iterate the object keys and take actions on mapping back to the @Train.Variable.Config Class.
        for (k, v) in obj.items():
            if (k == "initializer"):
                setattr(self, k, getattr(Train.Variable.Initializer, v["_type"])())
                getattr(self, k).parseJSON(v)
            else:
                setattr(self, k, v)
    
    def create(self, name: str, shape: List[int], dtype: 'tf.DType' = tf.float32, defaultDevice: str = "/cpu:0") -> 'tf.Variable':
        '''
			Create the variable in TensorFlow. Object-oriented creation over the deprecated static Train.createVar() methods.   --- UPDATED (Dexter) 20181003

            Parameters
            ------------------------------

            name                `str`           - The variable name.
            
            shape               `list(int+)`    - The shape of the variable.
            
            dtype               `tf.DType`      - The data type of this variable.
            
            Returns
            ------------------------------

            'tf.Variable'   - A TensorFlow variable.
        '''
        # Create the variable on the requested device.
        with tf.device(self.device or defaultDevice):
            # Get the variable of the requested name, shape, initializer and dtype.
            newVar = tf.compat.v1.get_variable(name, shape, initializer=self.initializer if isinstance(self.initializer, tf.keras.initializers.Initializer) else self.initializer.getTrainingInitializer(shape = shape), dtype=dtype)
        
        # Add to L1-loss if needed.
        if (self.l1Loss):
            l1Loss = tf.reduce_sum(newVar)
            tf.compat.v1.add_to_collection("losses", l1Loss if (self.l1Decay is None or self.l1Decay == 1) else tf.multiply(l1Loss, self.l1Decay, name="l1Loss"))
        
        # Add to L2-loss if needed.
        if (self.l2Loss):
            l2Loss = tf.nn.l2_loss(newVar)
            tf.compat.v1.add_to_collection("losses", l2Loss if (self.l2Decay is None or self.l2Decay == 1) else tf.multiply(l2Loss, self.l2Decay, name="l2Loss"))

        # Return the newly created variable.
        return newVar

class _TrainVariableLinearTransform():
    '''
        Class representing a linear transformation configuration on a given tensor, controlling the behavior of the Wx+b operation.   --- UPDATED (Dexter) 20180920
    '''
    class Tensors():
        '''
            Class representing a linear transformation results and it's corresponding weights.   --- UPDATED (Dexter) 20181003
        '''
        def __init__(self, results: 'tf.Tensor', weights: List['tf.Tensor']):
            '''
                Ccreate a linear transformation tensors object, wrapping the results and it's corresponding weights as an object.   --- UPDATED (Dexter) 20181003

                Parameters
                ------------------------------

                results `tf.Tensor` - The result tensor.

                weights `list<tf.Tensor>` - The weight tensors.
            '''
            # `tf.Tensor` - The result tensor.
            self.results = results
            # `list<tf.Tensor>` - The weight tensors.
            self.weights = weights

    @staticmethod
    def createFromJSON(obj: Dict[str, Any]) -> 'Train.Variable.LinearTransform':
        """
            Parse a previously saved object into a new @Train.Variable.LinearTransform object. This will auto-determine the sub-class of the object, and pass the JSON object to the inner method to continue to parse.   --- UPDATED (Dexter) 20190731

            Parameters
            ------------------------------

            obj `dict<str,*>` - JSON representation of the linear transform object.

            Returns
            ------------------------------

            `Train.Variable.LinearTransform` - A @Train.Variable.LinearTransform object.
        """
        # Parse this @Train.Variable.LinearTransform object.
        linearTransform = Train.Variable.LinearTransform()
        linearTransform.parseJSON(obj)
        return linearTransform

    def __init__(self, weightConfig: 'Train.Variable.Config' = None, biasConfig: 'Train.Variable.Config' = None):
        '''
			Create a linear transform option using customized variable configurations. By default, an identity transformation is given.   --- UDPATED (Dexter) 20181003

            Parameters
            ------------------------------

            weightConfig        `Train.Variable.Config`     - The weight configuration. If None, no weight will be given, i.e. identity transformation.

            biasConfig          `Train.Variable.Config`     - The bias configuration. If None, no bias will be given.
        '''
        # `Train.Variable.Config` - The weight configuration. If `None`, no weight will be given, i.e. identity transformation.
        self.weightConfig = weightConfig
        # `Train.Variable.Config` - The bias configuration. If `None`, no bias will be given.
        self.biasConfig = biasConfig

    def parseJSON(self, obj: Dict[str, Any]):
        """
            Parse a previously saved object into this @Train.Variable.LinearTransform object.   --- UPDATED (Dexter) 20181128

            Parameters
            ------------------------------

            obj `Dict<str,*>` - JSON object from Project file.
        """
        for (k, v) in obj.items():
            if (v is None or v == "null"):
                setattr(self, k, None)
            else:
                if not hasattr(self, k) or getattr(self, k) is None:
                    setattr(self, k, Train.Variable.Config())
                if v is not None:
                    getattr(self, k).parseJSON(v)

    def buildOn(self, incomingTensor: 'tf.Tensor', toUnit: int = 1, axis: int = -1, weightSharing: 'tf.Tensor' = None, defaultDevice: str = "/cpu:0") -> 'Train.Variable.LinearTransform.Tensors':
        '''
			Build the linear transformation on a given tensor. Noted this function should be wrapped within a scope.   --- UDPATED (Dexter) 20190922

            Parameters
            ------------------------------

            incomingTensor      `tf.Tensor`     - An incoming tensor to undergo this linear transformation.

            toUnit              `int`           - The number of outcoming unit (channel) of the last dimension of the output tensor.

            axis                `int`           - The axis of matrix multiplication.
            
            weightSharing       `tf.Tensor`     - Any sharing weight to override this settings.

            defaultDevice       `str`           - The default device that the variable will be created.

            Returns

            ------------------------------

            `Train.Variable.LinearTransform.Tensors`    - The result tensor after this linear transformation, and the list of the weights.

        '''
        w = weightSharing
        weights = []
        incomingShape = [*incomingTensor.shape]

        # Update the axis if it's negative.
        if (axis < 0):
            axis = len(incomingShape) + axis

        # Raise Error if incoming tensor has dimension lower than the required axis.
        if (axis >= len(incomingShape)) or (axis < 0):
            raise ValueError("Linear Transformation axis should within the dimension of the incoming tensor.")
        elif (axis == 0):
            raise ValueError("Linear Transformation cannot be taken on the batch dimension.")
        
        # Flatten the incoming tensor to the required axis.
        if (axis < len(incomingShape)):
            flattenSize = functools.reduce(lambda x,y: x*y, incomingShape[axis:], 1)
            incomingTensor = tf.reshape(incomingTensor, [*Train.Variable.setAsReshape(incomingShape[:axis]), flattenSize])
            incomingShape = [*incomingTensor.shape]
        
        # Temp reshape the tensor if the linear transformation axis is not the 2nd dimension.
        tempTranspose = False
        if (axis > 1):
            incomingTensor = tf.transpose(incomingTensor, [*[s for s in range(1, len(incomingShape) - 1)], 0, len(incomingShape) - 1])
            tempTranspose = True

        # Create weight if it's not using a sharing variable.
        if (w is None):
            
            # Create weight if there is a configuration.
            if (self.weightConfig is not None):
                w = self.weightConfig.create("weight", [*incomingShape[1:-1], incomingTensor.shape[-1], toUnit], dtype = incomingTensor.dtype, defaultDevice = defaultDevice)
                weights.append(w)
                wx = tf.matmul(incomingTensor, w)

            # Otherwise, just identically give the incoming tensor.
            else:
                wx = incomingTensor

        # Otherwise, use the sharing weight.
        else:
            weights.append(w)
            wx = tf.matmul(incomingTensor, w)
        
        # Create bias if there is a configuration.
        if (self.biasConfig is not None):
            b = self.biasConfig.create("bias", [toUnit], dtype = incomingTensor.dtype, defaultDevice = defaultDevice)
            results = tf.nn.bias_add(wx, b)
            weights.append(b)

        # Otherwise, there is no bias included.
        else:
            results = wx

        # Recover pre-axis dimensions.
        if (tempTranspose):
            results = tf.transpose(results, [len(incomingShape) - 2, *[s for s in range(0, len(incomingShape) - 2)], len(incomingShape) - 1])

        # Return the results and weights.
        return Train.Variable.LinearTransform.Tensors(results, weights)
    
    @staticmethod
    def createBasicConfig(weightAvg: float = 0.0, weightStdDev: Union[float,str] = 0.04, weightL1Loss: bool = False, weightL2Loss: bool = True,
                            weightL1Decay: float = 1, weightL2Decay: float = 0.004, biasInitial = 0.001) -> 'Train.Variable.LinearTransform':
        '''
			Create a basic linear transform config, with truncated normal distributed initialized weight and a constant initialized bias.   --- UPDATED (Dexter) 20181109

            Parameters
            ------------------------------

            weightAvg           `float`         - The mean of the normal distributed weight. If None, no weight will be given, i.e. identity transformation.

            weightStdDev        `float|str`     - The standard deviation of the normal distributed weight. If "auto", it will be 1/(last dimension of incoming tensor).

            weightL1Loss        `bool`          - Whether to take L1-loss on this variable.

            weightL2Loss        `bool`          - Whether to take L2-loss on this variable.

            weightL1Decay       `float`         - The constant for weighting L1-loss of this variable.

            weightL2Decay       `float`         - The constant for weighting L2-loss of this variable.

            biasInitial          `float`        - Constant initial value of biases. If None, no bias will be given.

            Returns
            ------------------------------

            `Train.Variable.LinearTransform` - A @Train.Variable.LinearTransform object from the given basic configuration settings.
        '''
        return Train.Variable.LinearTransform(weightConfig = None if weightAvg is None else Train.Variable.Config(initializer = Train.Variable.Initializer.TruncatedNormal(mean = weightAvg, stddev = weightStdDev),
                                                                                            l1Loss = weightL1Loss, l2Loss = weightL2Loss, l1Decay = weightL1Decay, l2Decay = weightL2Decay),
                                        biasConfig = None if biasInitial is None else Train.Variable.Config(initializer = Train.Variable.Initializer.Constant(value = biasInitial)))
    
class _TrainVariable:
    """
        Module containing classes regarding training variables.   --- UPDATED (Dexter) 20190220 
    """
    Initializer = _TrainVariableInitializer
    Config = _TrainVariableConfig
    LinearTransform = _TrainVariableLinearTransform

    @staticmethod
    def getFromGraph(name: str, device: str = '/cpu:0') -> 'tf.Variable':
        '''
			Create a previously created training variable in TensorFlow graph.   --- UPDATED (Dexter) 20190118

            Parameters
            ------------------------------

            name                `str`           - The name of the model graph variable.

            device              `str`           - The device like CPU or GPU this training will rely on.

            Returns

            ------------------------------

            `tf.Variable`   - The TensorFlow variable of the requested name.
        '''
        # Get the variable on the requested device.
        with tf.device(device):
            var = tf.compat.v1.get_variable(name)
            
        # Return the newly created variable.
        return var
    
    @staticmethod
    def setAsReshape(shape: List[Union[int, 'tf.Dimension']]) -> List[int]:
        '''
			Convert a shape to be a definition of reshape.   --- UPDATED (Dexter) 20191001

            Parameters
            ------------------------------

            shape               `list(int+)|list(tf.Dimension+)`    - The shape of the variable.


            Returns
            ------------------------------

            shape               `list(int+)`    - The shape of the variable, with None shape as -1.
        '''
        if len([s for s in shape if s in [None, -1]]) > 1:
            raise ValueError("Shapes with more than 2 dynamic dimension is not reshapeable.")

        return [(lambda s: -1 if s is None else s)(s) for s in shape]

class _TrainBuildConfig():
    '''
		Class representing a configuration of training in a build.   --- UPDATED (Dexter) 20190319
    '''
    # def __init__(self, noOfEpoch: int = 200, batchSize: int = 64, shuffle: bool = True, 
    #                 initialLearningRate: float = 0.001, 
    #                 learningRateDecayFactor: float = None, numEpochsPerDecay: int = None, exponentialLossDecay = 0.9, exponentialVarDecay=0.9999, 
    #                 crossValidationCount: int = 0, crossValidationType: str =  None, crossValidationProportion: float = 0.2, 
    #                 optimizer: str = "adam", optimizerParams: Dict[str,Any] = {}):    
    def __init__(self, noOfEpoch: int = 200, batchSize: int = 64, shuffle: bool = True, 
                    exponentialLossDecay = 0.9, exponentialVarDecay=0.9999, 
                    crossValidationCount: int = 0, crossValidationType: str =  None, crossValidationProportion: float = 0.2, 
                    optimizer: str = "adam", optimizerParams: Dict[str,Any] = {}):
        '''
			Create a configuration of training in a build.   --- UPDATED (Dexter) 20190915

            Parameters
            ------------------------------

            noOfEpoch                   `int`       - The number of epoches the training will go over for.

            batchSize                   `int`       - Batch size when using batched training.

            shuffle                     `bool`      - Whether the data source will be shuffled on training.

            initialLearningRate         `float`     - The initial learning rate.

            learningRateDecayFactor     `float`     - A decay factor for gradually reducing the learning rate.

            numEpochsPerDecay           `int`       - The number of epoches to go over before each learning rate decay takes place.

            exponentialLossDecay        `float`     - The exponential decay factor for reducing the moving average of the loss.

            exponentialVarDecay         `float`     - The exponential decay factor for reducing the moving average of the training variables.

            crossValidationCount        `int`       - The no. of rounds to go over for a cross validation epoch.

            crossValidationType         `str`       - The type of cross validation.

            crossValidationProportion   `float`     - The proportion of training data partition to take as a validation dataset.

            optimizer                   `str`       - An optimizer for training the models.

            optimizerParams             `dict<str,*>`    - TensorFlow optimizer paramerters in an object.
        '''
        # `int`     - The number of epoches the training will go over for.
        self.noOfEpoch = noOfEpoch
        # `int`    - Batch size when using batched training.
        self.batchSize = batchSize
        # `bool`    - Whether the data source will be shuffled on training.
        self.shuffle = shuffle
        # `bool` - Whether to drop the remaining batch of data in case the full dataset cannot be fully divided by the requested batch size.
        self.dropRemainder = False
        
        # # `float`   - The initial learning rate.
        # self.initialLearningRate = float(initialLearningRate)
        # # `float`   - A decay factor for gradually reducing the learning rate.
        # self.learningRateDecayFactor = learningRateDecayFactor
        # # `int`     - The number of epoches to go over before each learning rate decay takes place.
        # self.numEpochsPerDecay = numEpochsPerDecay
        
        self.exponentialLossDecay = exponentialLossDecay
        self.exponentialVarDecay = exponentialVarDecay

        # `str`     - An optimizer for training the models.
        self.optimizer = optimizer
        # `dict<str,*>`    - Tensorflow optimizer paramerters in an object.
        self.optimizerParams = optimizerParams

        # `str`     - The type of cross validation; `"None"` for no cross validation is to be used.
        self.crossValidationType = crossValidationType
        # `float`   - The proportion of training data partition to take as a validation dataset. Normally for K-fold cross validation, the proportion is 1/K. 
        self.crossValidationProp = crossValidationProportion
        # `float`   - The number of cross validation rounds. For K-fold cross validation, one round will include K training rounds. For random cross validation, one round will inclue one training round.
        self.crossValidationCount = crossValidationCount

        # Count how many runs requires for a full cross validation, and initiate the matching runID.
        self._cvCount = 0
    
    @property
    def validationRuns(self) -> int:
        """
            Total count on how many runs requireed for a full cross validation, and initiate the matching runID.    --- UPDATED (Dexter) 20190319

            Returns
            ------------------------------

            `int`   - The count for a full cross validation on the entire dataset.
        """
        return (int((1/self.crossValidationProp)*self.crossValidationCount) if self.crossValidationType == "fold" else self.crossValidationCount) if self.crossValidationType is not None else 1

    def parseJSON(self, obj: Dict[str, Any]):
        """
            Parse a previously saved object into this class of @Train.BuildConfig.   --- UPDATED (Dexter) 20190408

            Parameters
            ------------------------------

            obj     `Dict<str,*>`    - A JSON-stringifiable object.
        """
        # Iterate the object keys and take actions on mapping back to the Train.BuildConfig Class.
        for k,v in obj.items():
            setattr(self, k, v)

        # Convert old cross validation type.
        if (self.crossValidationType == "None"):
            self.crossValidationType = None

        # Only 2 types of cross validation or None are supported.
        if ((self.crossValidationType is not None) and (self.crossValidationType not in ["fold", "rand"])):
            raise ValueError("Validation Type (" + str(self.crossValidationType) +") is not supported.")

        # Ensure the cross validation count and proportions are of positive numbers.
        if (self.crossValidationCount < 0 or self.crossValidationProp < 0):
            raise ValueError("Validation Count and Proportion should be positive numbers.")
        
        # Noted if the validation proportion is 1/0, or no cross validation rounds, it acts just like no cross validation.
        if self.crossValidationProp == 1 or self.crossValidationProp == 0 or self.crossValidationCount == 0:
            self.crossValidationType = None
            self.crossValidationProp = 0
            self.crossValidationCount = 0
        else:
            if self.crossValidationProp > 1:
                # If cross validation propotion is over 1, it assumes it represents the no. of partition on the training data.
                self.crossValidationProp = 1/self.crossValidationProp
                self.crossValidationCount = int(self.crossValidationCount)
            else:
                self.crossValidationProp = self.crossValidationProp
                self.crossValidationCount = int(self.crossValidationCount)
        
    @staticmethod
    def createFromJSON(obj: Dict[str, Any]) -> 'Train.BuildConfig':
        """
            Parse a previously saved object into a @Train.BuildConfig object.   --- UPDATED (Dexter) 20190319

            Parameters
            ------------------------------

            obj     `Dict<str,*>`    - A JSON-stringifiable object.

            Returns
            ------------------------------

            `Train.BuildConfig`     - A build config object for a training.
        """
        trainBuildConfig = Train.BuildConfig()
        trainBuildConfig.parseJSON(obj)
        return trainBuildConfig

class Train:
    '''
        Class representing an object for catering TensorFlow training and implementation.   --- UPDATED (Dexter) 20181028
    '''
    BuildConfig = _TrainBuildConfig
    Variable = _TrainVariable
    
    class Activation(Enumeration):
        '''
            Enumeration representing the type of a @Train.Activation object.   --- UPDATED (Trista) 20190730
        '''
        # An relu activation function. (Ref: @Train.Activation.Relu )
        Relu = 1
        # An sigmoid activation function. (Ref: @Train.Activation.Sigmoid )
        Sigmoid = 2
        # An tanh activation function. (Ref: @Train.Activation.tanh )
        Tanh = 3
        # An hardsigmoid activation function. (Ref: @Train.Activation.HardSigmoid )
        HardSigmoid = 4
        # An linear activation function. (Ref: @Train.Activation.Linear )
        Linear = 5
        # An relu6 activation function. (Ref: @Train.Activation.Relu6 )
        Relu6 = 6
        # An selu activation function. (Ref: @Train.Activation.Selu )
        Selu = 7
        # An elu activation function. (Ref: @Train.Activation.Elu )
        Elu = 8
        # An softplus activation function. (Ref: @Train.Activation.Softplus )
        Softplus = 9
        # An softsign activation function. (Ref: @Train.Activation.Softsign )
        Softsign = 10
        # An crelu activation function. (Ref: @Train.Activation.Crelu )
        Crelu = 11

    # `int` - 6 digit number indicate the versioning (MMDDvv).
    Version = 2109
    
    class RootData:
        """
            Class representing a collection of data from root sources.   --- UPDATED (Dexter) 20190508
        """
        def __init__(self, sourceData: List[Union['tf.Tensor', 'np.ndarray']] = [], dppNodeData: Dict[str, Union['tf.Tensor', 'np.ndarray']] = {}):
            """
                Create a collection of data from root sources.   --- UPDATED (Dexter) 20190508

                Parameters
                ------------------------------

                sourceData `list<(tf.Tensor|np.ndarray)>` - The list of sources included as root sources.
                
                dppNodeData `dict<str, (tf.Tensor|np.ndarray)>` - The dict of data preprocessing nodes included as root sources.
            """
            # `list<(tf.Tensor|np.ndarray)>` - The list of sources included as root sources.
            self.sources = sourceData
            # `dict<str, (tf.Tensor|np.ndarray)>` - The dict of data preprocessing nodes included as root sources.
            self.dppNodes = dppNodeData

    class RootSources:
        """
            Class representing a collection from root sources, only necessery sources should be included.   --- UPDATED (Dexter) 20190508
        """
        def __init__(self, train: 'Train', rootSources: List[Union['Source.Config', 'DataPreprocessing.Node.SourceLike']], sourceDataset: 'DataGenerator.Dataset.Types' = None):
            """
                Create a collection of root sources, only necessery sources should be included.   --- UPDATED (Dexter) 20200501

                Parameters
                ------------------------------

                train `Train` - The train object.

                rootSources `list<(Source.Config|DataPreprocessing.Node.SourceLike)>` - The root source objects.

                sourceDataset   `DataGenerator.Dataset.Types`  - The type of dataset of a source that's this data is referencing.
            """
            # `list<Source.Config>` - The list of sources included as root sources.
            self.sources = [(s if s in rootSources else None) for s in train.sources]
            # `dict<str, DataPreprocessing.Node.Config>` - The dict of data preprocessing nodes included as root sources.
            self.dppNodes = {dppNode.key: dppNode for dppNode in rootSources if isinstance(dppNode, DataPreprocessing.Node.Config)}
            # `DataGenerator.Dataset.Types` - The type of dataset of a source that's this data is referencing.
            self.sourceDataset = sourceDataset
        
        def __len__(self) -> int:
            """
                Get the count of root sources.   --- UPDATED (Dexter) 20190508

                Returns
                ------------------------------

                `int` - The count of root sources.
            """
            return len(self.sources) + len(self.dppNodes)
        
        def __next__(self) -> 'Train.RootData':
            """
                Get the next batch of root data.   --- UPDATED (Dexter) 20190921
                
                Returns
                ------------------------------

                `Train.RootData` - Get the next batch of root data
            """
            # Some sources may preprocess some datapreprocessing nodes.
            dppData = {dppKey: dppNode.getNextBatch(sourceDataset = self.sourceDataset) for dppKey,dppNode in self.dppNodes.items()}
            
            # Get data from all remaining sources.
            sourceData = []
            for s in self.sources:
                batchedData = s.getNextBatch(sourceDataset = self.sourceDataset)
                # If batched data from root source already covers some data preprocessing node results, add them to dppData.
                if isinstance(batchedData, Source.PreprocessedData):
                    sourceData.append(batchedData.rootData)
                    for dppNodeKey, dppNodeData in batchedData.dppData.items():
                        dppData[dppNodeKey] = dppNodeData
                else:
                    sourceData.append(batchedData)

            # Return the structured data.
            return Train.RootData(sourceData, dppData)
        
        @property
        def batchCountPerEpoch(self) -> int:
            """
                Get the number of batches included in one epoch of this set of root sources.   --- UPDATED (Dexter) 20200310

                Returns
                ------------------------------

                `int` - The number of batches included in one epoch.
            """
            return [*self.sources, *(self.dppNodes.values())][0].batchCountPerEpoch

        @property
        def epochSize(self) -> int:
            """
                The epoch size of this set of root sources.   --- UPDATED (Dexter) 20200310

                Returns
                ------------------------------

                `int` - The epoch size of this set of root sources.
            """
            return [*self.sources, *(self.dppNodes.values())][0].epochSize

        def splitTestDataset(self, ratio: float = 0.2, shuffle: bool = True):
            """
                Split the trainset of all root sources and assign them as test dataset.   --- UPDATED (Dexter) 20191015

                Parameters
                ------------------------------

                ratio `float` - Proportion of original dataset to be as the test dataset.
                
                shuffle `bool` - Whether to shuffle before data splitting.
            """
            # Raise error if this root data is not referencing the train object.
            if (self.sourceDataset not in [None, DataGenerator.Dataset.Types.Train]):
                raise ValueError("Non training dataset cannot split test dataset.")
            
            # Loop for all root sources.
            for s in [*self.sources, *(self.dppNodes.values())]:
                # Raise error if it's splittable.
                if not s.splittable:
                    raise ValueError("Some of the root sources are not splittable.")
                
                # Split the dataset.
                s.generatorController.splitTestDataset(ratio, shuffle)

        def nextCrossValidation(self, buildConfig: 'Train.BuildConfig'):
            '''
                Prepare the next cross validation dataset.   --- UPDATED (Dexter) 20180630

                Parameters
                ------------------------------

                buildConfig `Train.BuildConfig` - The build configuration.
            '''
            # Loop for all root sources.
            for s in [*self.sources, *(self.dppNodes.values())]:
                if (s.splittable):
                    # Dataset is split according to the training profile settings.
                    s.generatorController.splitValidationDataset(validation=buildConfig.crossValidationProp, randomFold = buildConfig.crossValidationType=="rand")

        def getData(self, start: int = None, end: int = None) -> 'Train.RootData':
            """
                Get all of the root data. NOTE: Large dataset using Dataset API is not allowed due to memory restrictions.   --- UPDATED (Dexter) 20190921
                
                Parameters
                ------------------------------

                start  `int`       - Batch starting index (inclusive). If `None`, it notates `0`.

                end    `int`       - Batch ending index (exclusive). If `None`, it notes the column count.
                
                Returns
                ------------------------------

                `Train.RootData` - Get all of the root data.
            """
            # Some sources may preprocess some datapreprocessing nodes.
            dppData = {dppKey: dppNode.getData(sourceDataset = self.sourceDataset, start = start, end = end) for dppKey,dppNode in self.dppNodes.items()}

            # /Get data for remaining sources.
            sourceData = []
            for s in self.sources:
                batchedData = s.getData(sourceDataset = self.sourceDataset, start = start, end = end)
                # If batched data from root source already covers some data preprocessing node results, add them to dppData.
                if isinstance(batchedData, Source.PreprocessedData):
                    sourceData.append(batchedData.rootData)
                    for dppNodeKey, dppNodeData in batchedData.dppData.items():
                        dppData[dppNodeKey] = dppNodeData
                else:
                    sourceData.append(batchedData)

            # Return the structured data.
            return Train.RootData(sourceData, dppData)
        
        def getRandItems(self, randomCount: int) -> 'Train.RootData':
            """
                Get random items form the root data.   --- UPDATED (Dexter) 20190509

                Parameters
                ------------------------------

                randomCount `int` - The count of random items to collect.
                
                Returns
                ------------------------------

                `Train.RootData` - Random items of the root data.
            """
            # Set the random seed for shuffling.
            randomSeed = int(random.random()*100000)

            # Some sources may preprocess some datapreprocessing nodes.
            dppData = {dppKey: dppNode.getRandItems(sourceDataset = self.sourceDataset, randomSeed = randomSeed, randomCount = randomCount) for dppKey,dppNode in self.dppNodes.items()}
            sourceData = []

            for s in self.sources:
                batchedData = s.getRandItems(sourceDataset = self.sourceDataset, randomSeed = randomSeed, randomCount = randomCount)
                # If batched data from root source already covers some data preprocessing node results, add them to dppData.
                if isinstance(batchedData, Source.PreprocessedData):
                    sourceData.append(batchedData.rootData)
                    for dppNodeKey, dppNodeData in batchedData.dppData.items():
                        dppData[dppNodeKey] = dppNodeData
                else:
                    sourceData.append(batchedData)

            # Return the structured data.
            return Train.RootData(sourceData, dppData)

        def closeCurrentDatasets(self):
            """
                Close all current datasets.   --- UPDATED (Dexter) 20190921
            """
            # Close the source first.
            for s in reversed(self.sources):
                s.generatorController.close(self.sourceDataset)

            # Then close the dpp nodes.
            for dppKey in reversed([*self.dppNodes.keys()]):
                self.dppNodes[dppKey].generatorController.close(self.sourceDataset)

    class UsingData:
        '''
                Class representing a structured data prepared for the data using in a training model.   --- UPDATED (Dexter) 20190515
        '''
        def __init__(self, train: 'Train', rootData: 'Train.RootData'):
            '''
                Create a validated structured source data object that mirrors the training object structure.   --- UPDATED (Dexter) 20190922

                Parameters
                ------------------------------

                train       `Train`                 - The training object that this data is following with.

                rootData    `Train.RootData`        - The root data to produce for the using data.
            '''
            # Ensure the source lengths match.
            if len(rootData.sources) < len(train.rootSources.sources) or len(rootData.dppNodes) < len(train.rootSources.dppNodes):
                raise ValueError("The structure does not match with the root data of the Train object.")
            
            # `dict<str,(np.ndarray|tf.Tensor)>` - The structured data to be used as training / target data of the data model.
            self._data = {dppKey: dppNode.getProcessedData(rootData) for dppKey, dppNode in train.dppNodes.items()}
        
        def __len__(self) -> int:
            '''
                Get the length of the using data.   --- UPDATED (Dexter) 20190515

                Returns
                ------------------------------

                `int`   - The length of this @Train.UsingData object. It should be the same as the length as the @Train.dppNodes .
            '''
            return len(self._data)
        
        def getCount(self) -> int:
            '''
                Get the data size of the using data.   --- UPDATED (Dexter) 20190922

                Returns
                ------------------------------

                `int`   - The number of records in this using data.
            '''
            return len([*self._data.values()][0])
        
        def __iter__(self) -> Iterable['np.ndarray']:
            '''
                Create an iterable object on the source data.   --- UPDATED (Dexter) 20190515
                
                Returns
                ------------------------------

                `iterable<np.ndarray>`     - Iterable on the data of each source.
            '''
            return self._data.__iter__()
        
        def __getitem__(self, idx: str) -> 'np.ndarray':
            '''
                Get the data of one of the using data.   --- UPDATED (Dexter) 20190515

                Returns
                ------------------------------

                `np.ndarray`   - The data in on of the using data.
            '''
            return self._data[idx]
        
        def values(self) -> Iterable[Union['np.ndarray', 'tf.Tensor']]:
            '''
                Get the dict iterable of the using data.   --- UPDATED (Dexter) 20190516

                Returns
                ------------------------------

                `iterable<(np.ndarray|tf.Tensor)>`   - The dict iterable.
            '''
            return self._data.values()
        
        def items(self) -> Iterable[Tuple[str, Union['np.ndarray', 'tf.Tensor']]]:
            '''
                Get the dict iterable of the using data.   --- UPDATED (Dexter) 20190516

                Returns
                ------------------------------

                `iterable<tuple<str, (np.ndarray|tf.Tensor)>>`   - The dict iterable.
            '''
            return self._data.items()
        
        def keys(self) -> Iterable[str]:
            '''
                Get the dict iterable of the using data.   --- UPDATED (Dexter) 20190516

                Returns
                ------------------------------

                `iterable<str>`   - The dict iterable.
            '''
            return self._data.keys()

    def __init__(self, trainName: str = "train", folder: str = "D:/tmp/", restorePath: Optional[str] = None, device: str = '/cpu:0', 
                logFreq: int = 0, saveFreq: int = 0, testFreq: int = 0, traceFreq: int = 0, weightLogFreq: int = 0, filterFreq: int = 0, 
                source: Optional[Union['Source.Config', List['Source.Config']]] = None, testSource: Optional[Union['Source.Config', List['Source.Config']]] = None, 
                runCount: Optional[int] = None, buildConfig: Optional[Union['Train.BuildConfig', List['Train.BuildConfig']]] = None, traceRecord: int = 0):
        '''
			Create a Train object.   --- UPDATED (Dexter) 20200112

            Parameters
            ------------------------------

            trainName           `str`       - The name of this training.

            folder              `str`       - The folder that this training results folder will save to.

            restorePath         `str`       - A directory that referencing TensorFlow checkpoint files that this training will recover from.

            device              `str`       - The device like CPU or GPU this training will rely on.

            logFreq             `int`       - The train logging frequency (per training step).

            saveFreq            `int`       - The model saving frequency (per training step).

            testFreq            `int`       - The test (with either validation or test dataset) frequency (per training step).

            traceFreq           `int`       - The trace logging frequency (per training step).

            weightLogFreq       `int`       - The weight logging frequency (per training step).

            filterFreq          `int`       - The image filter logging frequency (per training step).   --- RESERVED

            source              `Source.Config|list[Source.Config]`         - A training source or a list of training source that this training is using.   --- DEPRECATED

            runCount             `int`       - The count of multi-run setup.

            buildConfig     `Train.BuildConfig|list[Train.BuildConfig]` - The training profile configuration this training is using.

            traceRecord         `int`       - The count of records to trace with along the training.
        '''
        ### Define folder-related names, and create folder if needed.
        # `str` - The name of this training.
        self.trainName = trainName
        # `str` - The training timestamp for creating the results folders.
        self.trainTime = TimeHelper.getDateTimeStr(timeExpr="hhmmss")
        # `str` - The folder that this training results folder will save to.
        self.folder = folder if folder is not None else ""

        # `DataGenerator.Dataset.TestSource` - The type of getting test dataset, as defined by @DataGenerator.Dataset.TestSource .
        self.testDatasetType = DataGenerator.Dataset.TestSource.Split
        # `float` - The test data proportion.
        self.testRatio = 0.2
        # `bool` - Whether to shuffle before test splitting.
        self.testShuffle = True
        # `str` - The model restore path if it's restoring from a trained model.   --- RESERVED
        self.restorePath = restorePath

        ### Define log frequencies.
        # `int` - The train logging frequency (per training step).
        self.logFreq = math.inf if logFreq == -1 else logFreq
        # `int` - The model saving frequency (per training step).
        self.saveFreq = saveFreq
        # `int` - The test (with either validation or test dataset) frequency (per training step).
        self.testFreq = math.inf if testFreq == -1 else testFreq
        # `CSVLogger` - The test record logger.
        self.testLogger = None
        # `int` - The trace logging frequency (per training step).
        self.traceFreq = math.inf if traceFreq == -1 else traceFreq
        # `int` - The weight logging frequency (per training step).
        self.weightLogFreq = math.inf if weightLogFreq == -1 else weightLogFreq
        # `int` - The image filter logging frequency (per training step).   --- RESERVED 
        self.filterFreq = math.inf if filterFreq == -1 else filterFreq
        # `list<tf.Tensor>` - A list of tensor that would be printed on console.
        self._printTensors = []
                
        ### Define the model graph information.
        # `str` - The device like CPU or GPU this training will rely on.
        self.device = device
        # `dict<str, DataPreprocessing.Node.Config>` - Data pre-processing nodes in this @Train object.
        self.dppNodes = {}
        # `dict<str,ModelNode.Layer.Config>` - A key-value map storing all model node objects in this @Train object. 
        self.modelNodes = {}
        ### Raise Errors for `buildConfig` not matching Train.BuildConfig type.
        if (buildConfig is not None and ((classof(buildConfig) != "list" and (not issubclass(buildConfig.__class__, Train.BuildConfig))) or (classof(buildConfig) == "list" and any([(not issubclass(tp.__class__, Train.BuildConfig)) for tp in buildConfig])))):
            raise ValueError("`buildConfig` parameters should be a `Train.BuildConfig` object.")
        self.buildConfigs = [buildConfig or Train.BuildConfig()] if classof(buildConfig) != "list" else buildConfig
        ### Raise Errors for `source` not matching Source.Config type.
        if (source is not None and ((classof(source) != "list" and (not isinstance(source, Source.Config))) or (classof(source) == "list" and any([(not isinstance(s, Source.Config)) for s in source])))):
            raise ValueError("`source` parameters should be a `Source.Config` object.")
        # `list<Source.Config>` - A list of all @Source.Config training source objects attached to this @Train object.
        self.sources = []
        if isinstance(source, list):
            self.setDataSources(*source)
        elif source is not None:
            self.addDataSource(source)
        # `Train.RootSources` - The root sources to loop through. To be determined just before each build.
        self._rootSources = None
        # `dict<str,tf.Tensor>` - A list of placeholder tensors.
        self._sourceTensors = {}
        # `int` - The current editing build no.
        self._editingBuild = 0
        # `list<Source.Config|Train.UsingData>` - A list of @Source.Config evaluation sources or @Train.UsingData structured evaluation data.
        self._evalSources = None
        # `int` - 6 digit number indicating the versioning (MMDDvv).
        self._version = Train.Version
        # `int` - 6 digit number indicating the old version if this is an opened NOM (MMDDvv).
        self._oldVersion = 0

        ### Define record tracing-related items.
        # `int` - The count of records to trace with along the training.
        self.traceRecord = traceRecord
        # `Source.Data` - An list of all the items to be traced.
        self.traceItems = None

        ### Define graph and training-related items.
        if (runCount is not None and runCount <= 0):
            raise ValueError("The given value of runCount is inappropriate.")
        # `int` - The count of multi-run setup.
        self.runCount = runCount if runCount is not None else 1
        # `tf.Graph` - The TensorFlow model graph.
        self._graph = None
        # `tf.compat.v1.Session` - The TensorFlow session.
        self._sess = None
        # `bool` - Whether this train is built or not.
        self._built = False
        # `int` - The current training build no.; initiated as -1.
        self._buildNo = -1
        # `int` - The current training run no.; initiated as -1.
        self._runNo = -1
        # `int` - The current cross validation no.; initiated as -1.
        self._cvNo = -1
        # `DataGenerator.Dataset.Types` - The current source dataset is using.
        self._currentSourceDataset = DataGenerator.Dataset.Types.Train
        # `int` - The step count within a build.
        self.localStep = 0
        # `int` - The step count on the overall training process, i.e. same as `self.localStep` when `self._buildNo == 0`.
        self.globalStep = 0
        # `tf.Tensor` - A variable tensor to store the local step info.
        self.localStepTensor = 0; 
        # `tf.Tensor` - A variable tensor to store the global step info.
        self.globalStepTensor = 0
        # `tf.Tensor` - A train or not tensor as the paramter for the batch normalization parameter.
        self._bnTensor = None
        # `int` - An auto-increment ID for naming of @ModelNode.Config objects injecting to this @Train object.
        self._layerIDInc = 0
        # `int` - 6 digit number indicate the versioning (MMDDvv).
        self._version = 190400
        # `dict<str, list<Callable>>` - Functions to store when triggered by a specific event.
        self._eventFtns = {eType: [] for eType in ["buildstart", "buildend", "stepprepare", "stepstart", "stepend", 
                                                    "epochstart", "epochend", "trainbuild", "trainstart", "trainend"]}

    def assignCurrentSourceDataset(self, v: 'DataGenerator.Dataset.Types'):
        """
            The current source dataset is using.   --- UPDATED (Dexter) 20190918

            Parameters
            ------------------------------

            `DataGenerator.Dataset.Types` - The current source dataset is using.
        """
        self._currentSourceDataset = v
        self.rootSources.sourceDataset = v

    def parseJSON(self, obj: Dict[str, Any]):
        """
            Parse a previously saved object into this class of Train.   --- UPDATED (Dexter) 20200311

            Parameters
            ------------------------------

            obj `dict<str,*>` - JSON object from Project file.
        """
        # Iterate the object keys and take actions on mapping back to the Train Class.
        for k in Train.getPrioritizedKeys(obj, ["_version", "testDatasetType", "buildConfigs", "trainingProfiles", "sources", "dppNodes", "modelNodes"]):
            if k == "sources":
                # If it is a source, it will need to create their class and parse themselves.
                self.sources = [Source.createFromJSON(sObj, self) for sObj in obj[k]]

                # Backward compatability: If there is test ratio and batch size in Source.Config, this is the old format; and these values will be assigned to the build config or train object.
                if any(["_testRatio" in sObj for sObj in obj[k]]):
                    for bc in self.buildConfigs:
                        bc.batchSize = obj[k][0].batchSize
                        bc.shuffle = obj[k][0].shuffle
                    self.testRatio = obj[k][0]._testRatio / 100
                    self.testDatasetType = DataGenerator.Dataset.TestSource.Split if all([sObj.splittable for sObj in obj[k]]) else DataGenerator.Dataset.TestSource.Assign
                
            elif (k == "trainingProfiles" or k == "buildConfigs"):
                # If it is a training profile, it will need to create its class and parse itself.
                # BACKWARD_COMPATIBILITY: v1903.00 or before may use "trainingProfiles"
                self.buildConfigs = [Train.BuildConfig.createFromJSON(v) for v in obj[k]]
            elif (k == "layerProfiles" or k == "modelNodes"):
                # Create each layer from JSON.
                for (mk,mv) in obj[k]:
                    
                    layerKey = ModelNode.updateNodeName(mk)
                    ModelNode.createFromJSON(layerKey, mv, self)
                    try:
                        if self.modelNodes[layerKey]._type == 'Group':
                            if self.modelNodes[layerKey].refLayerName is None:
                                sub_layer_list = []
                                sub_layer_dict = {}
                                for (smk,smv) in self.modelNodes[layerKey].subModelNodes:
                                    sublayerKey = ModelNode.updateNodeName(smk)
                                    sublayer = ModelNode.returnLayer(sublayerKey, smv, self)
                                    sub_layer_dict[sublayerKey] = sublayer
                                    sub_layer_list.append(sublayer)
                            
                                self.modelNodes[layerKey].subModelNodes = sub_layer_list
                                self.modelNodes[layerKey].sub_layer_dict = sub_layer_dict
                            else:
                                rflayerKey = ModelNode.updateNodeName(self.modelNodes[layerKey].refLayerName)
                                self.modelNodes[layerKey].subModelNodes = self.modelNodes[rflayerKey].subModelNodes
                                self.modelNodes[layerKey].sub_layer_dict = self.modelNodes[rflayerKey].sub_layer_dict

                            
                    except:
                        pass
                    
                # Noted the layer profile will have cross referencing, so it is done now after all layer profiles are created.
                for modelNode in self.modelNodes.values():
                    # Complicated parsing, due to backward compatibility of 1 dimension connection without updated multi-build support.   --- MAXVER 1905
                    modelNode.toNode = ([[self.modelNodes[ModelNode.updateNodeName(ln)] for ln in build] for build in modelNode.toNode] if isinstance(modelNode.toNode[0], list) else [[self.modelNodes[ModelNode.updateNodeName(ln)] for ln in modelNode.toNode]]) if len(modelNode.toNode) else [[]]
                    modelNode.fromNode = ([[self.modelNodes[ModelNode.updateNodeName(ln)] for ln in build] for build in modelNode.fromNode] if isinstance(modelNode.fromNode[0], list) else [[self.modelNodes[ModelNode.updateNodeName(ln)] for ln in modelNode.fromNode]]) if len(modelNode.fromNode) else [[]]
                    if (len(modelNode.fromSource)):
                        if any([(isinstance(build, list) and any([not isinstance(ele, list) for ele in build]) and any([(not isinstance(ele, str)) for ele in build])) for build in modelNode.fromSource]):
                            # Backward compability for early version: [[number, string]+]  (no build dimension)
                            modelNode.fromSource = [[dppKey for (sourceID, dppKey) in modelNode.fromSource]]
                        elif any([(isinstance(build, list) and any([isinstance(ele, list) for ele in build])) for build in modelNode.fromSource]):
                            # Backward compability for v1903: [[[number, string]+]+] (build dimension included)
                            modelNode.fromSource = [[dppKey for (sourceID, dppKey) in build] for build in modelNode.fromSource]
                    else:
                        modelNode.fromSource = [[]]
                    
                    # If it's a Ladder Layer, it will need to parse core node issues.
                    if isinstance(modelNode, ModelNode.Layer.Config):
                        # Recover the incoming config node.
                        if (modelNode.incomingConfig.coreNode):
                            modelNode.incomingConfig.coreNode = self.modelNodes[ModelNode.updateNodeName(modelNode.incomingConfig.coreNode)]
                        
                        # Recover the incoming config node transformGate.
                        if (isinstance(modelNode.incomingConfig, ModelNode.Layer.Incoming.ElementWiseConfig) and modelNode.incomingConfig.transformGate):
                            modelNode.incomingConfig.transformGate = self.modelNodes[modelNode.incomingConfig.transformGate]
            
            elif (k == "dppNodes"):
                # Clear any default setup by runtime constructors, i.e. default dpp nodes by Image Source.
                self.dppNodes.clear()

                # Sort the dpp nodes first.
                dppNodes = obj["dppNodes"]
                dppNodes.sort(key = lambda ele: ele[1]["_order"])

                for (dppKey, dppNode) in dppNodes:
                    # Recover the data preprocessing nodes.
                    self.dppNodes[dppKey] = DataPreprocessing.Node.createFromJSON(dppNode, self)
                
            elif (k == "_version"):
                # Declare the opening version.
                self._oldVersion = obj[k]

            elif (k in ["logFreq", "testFreq", "traceFreq", "weightLogFreq", "filterFreq"]):
                setattr(self, k, math.inf if obj[k] == -1 else obj[k] )

            elif (k == "testDatasetType"):
                setattr(self, k, DataGenerator.Dataset.TestSource.parse(obj[k]))

            elif (k not in ["_project", "testSources", "_editingProfile", "_editingSource", "_editingDppNode", "traceItems", "_editingModelNode", "_layerIDInc", "_buildNo", "_runNo", "_cvNo", "_localStep", "_globalStep", "_rootSources", "_currentSourceDataset"]):
                if (not hasattr(self, k)):
                    print(k)
                setattr(self, k, obj[k])

    @property
    def currentSourceDataset(self):
        """
            The current source dataset is using.   --- UPDATED (Dexter) 20190507

            Returns
            ------------------------------

            `DataGenerator.Dataset.Types` - The current source dataset is using.
        """
        return self._currentSourceDataset
    
    @property
    def currentBuildConfig(self) -> "Train.BuildConfig":
        """
            Get the current build configuration.   --- UPDATED (Dexter) 20190915

            Returns
            ------------------------------

            `Train.BuildConfig` - The current build configuration.
        """
        return self.buildConfigs[self._buildNo]

    @staticmethod
    def createFromJSON(obj: Dict[str, Any]) -> 'Train':
        """
            Parse a previously saved object into a new NOM @Train object. This will auto-determine the sub-class of the object, and pass the JSON object to the inner method to continue to parse.   --- UPDATED (Dexter) 20190822

            Parameters
            ------------------------------

            obj     `Dict[str,Any]` - JSON representation of the model node object.

            Returns
            ------------------------------

            `ModelNode.Config` - A @ModelNode.Config object.
        """
        # Parse the model node object.
        train = Train()
        train.parseJSON(obj)
        return train

    @staticmethod
    def createFromJSONString(jsonString: str) -> 'Train':
        """
            Parse a previously saved object into a new NOM @Train object. This will auto-determine the sub-class of the object, and pass the JSON object to the inner method to continue to parse.   --- UPDATED (Dexter) 20190822

            Parameters
            ------------------------------

            jsonString     `str` - JSON string representation of the model node object.

            Returns
            ------------------------------

            `ModelNode.Config` - A @ModelNode.Config object.
        """
        # Parse the model node object.
        return Train.createFromJSON(json.loads(jsonString))

    @property
    def version(self) -> int:
        """
            Get the 6 digit number indicating the versioning (MMDDvv).   --- UPDATED (Dexter) 20190717

            Returns
            ------------------------------

            `int` - digit number indicating the versioning (MMDDvv).
        """
        return self._version

    @property
    def oldVersion(self) -> int:
        """
            Get the 6 digit number indicating the old version if this is an opened NOM (MMDDvv).   --- UPDATED (Dexter) 20190717

            Returns
            ------------------------------

            `int` - 6 digit number indicating the old version if this is an opened NOM (MMDDvv).
        """
        return self._oldVersion

    @property
    def editingBuild(self) -> int:
        """
            The current editing build number.   --- UPDATED (Dexter) 20190719

            Returns
            ------------------------------

            `int` - The current editing build number.
        """
        return self._editingBuild

    @property
    def rootSources(self) -> 'Train.RootSources':
        """
            The root sources to loop through. To be determined just before each build.    --- UPDATED (Dexter) 20190508

            Returns
            ------------------------------

            `Train.RootSources` - The root sources to loop through. To be determined just before each build.
        """
        return self._rootSources

    @property
    def layerProfiles(self) -> Dict[str, 'ModelNode.Config']:
        """
            All the model nodes in this @Train object, using compatibility on old attribute name `layerProfiles`.   --- UPDATED (Dexter) 20190804
            
            Returns
            ------------------------------

            `dict<str,ModelNode.Config>` - All the model nodes in this @Train object, using compatibility on old attribute name `layerProfiles`.
        """
        print("Using Deprecated API: Train.layerProfiles .")
        return self.modelNodes

    def __repr__(self):
        '''
			Get a string representation of this log table.   --- UPDATED (Dexter) 20181028

            Returns
            ------------------------------

            `str`   - The representation including the row and column count.
        '''
        return "<Train: name: \"" + self.trainName + "\", run times: " + self.runCount + ", no. of layers: " + str(self.size) + ">"

    def __len__(self):
        '''
			The length of the training is the number of build times number of runs.   --- UPDATED (Dexter) 20181028
        '''
        return len(self.buildConfigs) * self.runCount

    @staticmethod
    def activationFunctions(activationName: 'Train.Activation') -> Callable:
        '''
			Get an activation function from a key.   --- UPDATED (Dexter, Trista) 20190815

            Parameters
            ------------------------------

            activationName `Train.Activation` - The activation enumeration.
            
            Returns
            ------------------------------

            `Function`   - A calling function for the activation function.
        '''
        return {Train.Activation.Relu: tf.nn.relu, Train.Activation.Relu6: tf.nn.relu6, Train.Activation.Crelu: tf.nn.crelu, Train.Activation.Elu: tf.nn.elu,
                Train.Activation.Selu: tf.nn.selu, Train.Activation.Softplus: tf.nn.softplus, Train.Activation.Softsign: tf.nn.softsign,
                Train.Activation.Sigmoid: tf.nn.sigmoid, Train.Activation.Tanh: tf.nn.tanh, Train.Activation.HardSigmoid: tf.keras.backend.hard_sigmoid,
                Train.Activation.Linear: lambda x: x }[activationName]

    @staticmethod
    def optimizers(string):
        '''
			Get an optimizer function from a key.   --- UPDATED (Dexter) 20191002

            Returns
            ------------------------------

            `Function`   - A calling function for the optimizer function.
        '''
        return {"rmsprop": tf.compat.v1.train.RMSPropOptimizer, "adam": tf.compat.v1.train.AdamOptimizer, 
                "grad": tf.compat.v1.train.GradientDescentOptimizer, "momentum": tf.compat.v1.train.MomentumOptimizer,
                "adagrad": tf.compat.v1.train.AdagradOptimizer, "ftrl": tf.compat.v1.train.FtrlOptimizer, 
                "nadam": tf.compat.v1.keras.optimizers.Nadam, "adamax": tf.compat.v1.keras.optimizers.Adamax,
                "adadelta": tf.compat.v1.train.AdadeltaOptimizer, "sgd": tf.compat.v1.keras.optimizers.SGD}[string.lower()]

    @property
    def buildNo(self):
        '''
			Get the build no, the building state of this training.   --- UPDATED (Dexter) 20180623

            Returns
            ------------------------------

            `int`   - The current build number.
        '''
        return self._buildNo
    
    @property
    def size(self):
        '''
			Count the number of layers in this training.   --- UPDATED (Dexter) 20180623

            Returns
            ------------------------------

            `int`   - The total count of all layers in this training.
        '''
        return len(self.modelNodes)

    @property
    def length(self):
        '''
			Alias for returning the length of this training.   --- UPDATED (Dexter) 20181028
        '''
        return len(self)
    
    def getNewLayerID(self):
        '''
			Get a unique positive id, typically for creating a new layer.   --- UPDATED (Dexter) 20180623

            Returns
            ------------------------------

            `int`   - A new integer as the layer id.
        '''
        # Get the current number and increment it for next-time use.
        toID = self._layerIDInc
        self._layerIDInc += 1

        # Return the id.
        return toID

    def dispatchEvent(self, event):
        '''
			Dispatch an event, typically firing actions based on some event-criteria.   --- UPDATED (Dexter) 20180623

            Parameters
            ------------------------------

            `Event`     - An event to be dispatched.
        '''
        for ftn in self._eventFtns[event.type]:
            ftn(event)

    @staticmethod
    def createVar(name: str, shape: List[int], initializer: 'Train.Variable.Initializer.Config|tf.initializers', device: str = None, dtype: 'tf.DType' = tf.float32, l1loss: bool = False, l2loss: bool = False, weightDecayRate: float = None):
        '''
			Create a new training variable.   --- DEPRECATED --- UPDATED (Dexter) 20190210

            Parameters
            ------------------------------

            name                `str`           - The variable name.

            shape               `list(int+)`    - The shape of the variable.

            initializer         `Train.Variable.Initializer.Config|tf.initializers`   - An initializer for the variable.

            device              `str`           - The device like CPU or GPU this training will rely on.

            dtype               `tf.DType`      - The data type of this variable.

            l1loss              `bool`          - Whether to take L1-loss on this variable. 

            l2loss              `bool`          - Whether to take L2-loss on this variable.

            weightDecayRate     `float`         - The constant for weighting L2-loss of this variable.
        '''
        varConfig = Train.Variable.Config(l1Loss = l1loss, l2Loss=l2loss, l2Decay=weightDecayRate, initializer=initializer, device=device)
        return varConfig.create(name, shape, dtype)
    
    @staticmethod
    def creatVarFromConfig(name: str, shape: List[int], dtype: 'tf.DType' = tf.float32, config: 'Train.Variable.Config' = None):
        '''
			Create a new training variable based on a configuration.   --- DEPRECATED --- UPDATED (Dexter) 20181003

            Parameters
            ------------------------------

            name                `str`           - The variable name.

            shape               `list(int+)`    - The shape of the variable.

            dtype               `tf.DType`      - The data type of this variable.

            config              `bool`          - Whether to take L1-loss on this variable. [deprecated]
        '''
        return config.create(name, shape, dtype)

    @staticmethod
    def createNormDistVar(name, shape,mean=0, stdDev=5e-2, l1loss = False, l2loss = False, weightDecayRate=None, dtype=tf.float32):
        '''
			Create a new training variable with normally distributed initalizing values.   --- DEPRECATED --- UPDATED (Dexter) 20180623

            Parameters
            ------------------------------

            name                `str`           - The variable name.

            shape               `list(int+)`    - The shape of the variable.

            mean                `float`         - The initializing mean value.

            stdDev              `float`         - The initializing values standard deviation.

            device              `str`           - The device like CPU or GPU this training will rely on.

            l1loss              `bool`          - Whether to take L1-loss on this variable.

            l2loss              `bool`          - Whether to take L2-loss on this variable.

            weightDecayRate     `float`         - The constant for weighting L2-loss of this variable.

            dtype               `tf.DType`      - The data type of this variable.
        '''
        # Handler for `Train.createVar()` method but with a `tf.truncated_normal_initalizer`
        return Train.createVar(name, shape, Train.Variable.Initializer.TruncatedNormal(mean=mean,stddev=stdDev), l1loss = l1loss, l2loss = l2loss, weightDecayRate = weightDecayRate, dtype=dtype)
    
    def setSequentialTraining(self, *buildConfigs: 'Train.BuildConfig'):
        '''
			Set sequential training by applying several training profiles.   --- UPDATED (Dexter) 20180623

            Parameters
            ------------------------------

            *buildConfigs   `Train.BuildConfig+`  - Multi-build training configurations in a sequential order.
        '''
        # Ensure the training has not been built yet.
        if self._built:
            raise ValueError("Training Profile cannot be modified during training processes. Please consider to close this training before update the training profile.")
        
        # Raise Errors for not matching Train.BuildConfig type.
        if any([(buildConfig is not None and ((classof(buildConfig) != "list" and (not issubclass(buildConfig.__class__, Train.BuildConfig))) or (classof(buildConfig) == "list" and any([(not issubclass(tp.__class__, Train.BuildConfig)) for tp in buildConfig])))) for buildConfig in buildConfigs]):
            raise ValueError("`buildConfig` parameters should be a `Train.BuildConfig` object.")
        
        # Assign the training profiles.
        self.buildConfigs = buildConfigs

    def __next__(self):
        """
            Generate the next batch of data rows.   --- UPDATED (Dexter) 20190508

            Returns
            ------------------------------

            'dict<str,np.ndarray>' - The dictionary with preprocessing node key and corresponding data. In the future, the data will be output as `tf.Tensor` objects.
        """
        # 1. Get the root data from current root source.
        rootData = next(self.rootSources)

        # 2. Process the data.
        return {dppKey: dppNode.getProcessedData(rootData) for dppKey, dppNode in self.dppNodes.items()}

    def _build(self, buildNo: int = 0):
        '''
			Build the TensorFlow Graph of the model.   --- UPDATED (Dexter) 20190915

            Parameters
            ------------------------------

            buildNo    `int`   - The build number of this training build.
        '''
        # 1. Create TensorFlow Graph and Session for building training model
        tf.compat.v1.reset_default_graph()
        self._graph = g = tf.Graph()
        self._buildNo = buildNo
        
        # 2. Dispatch buildstart event
        bs = BuildEvent("buildstart", {"target": self, "buildno": self._buildNo})
        self.dispatchEvent(bs)

        # 3. If buildstartevent is not prevented, it will go over the default model building
        if not bs.defaultPrevented:
            with g.as_default():
                # 30. Prepare a step variable.
                self.globalStepTensor = tfGlobalStep = tf.compat.v1.train.get_or_create_global_step()
                assignStep = tfGlobalStep.assign(self.globalStep)
                self.localStepTensor = tf.Variable(self.localStep, trainable=False, dtype=tf.int64)

                # 3A. For all Train Sources, build the placeholder Tensors
                self._sourceTensors = {dppKey: dppNode.getTensor() for dppKey,dppNode in self.dppNodes.items()}
                if len(self._sourceTensors) == 0:
                    raise ValueError("There is no training data sources.")

                # 3B.   Attach a final layer node if there is no final layer defined.   --- DEPRECATED --- MAXVER 1906
                if (len(self.getFinalNodes(buildNo=buildNo)) == 0):
                    if ("target" in self.dppNodes):
                        #   If all target data is a positive integer, a classification is assumed to be done
                        targetTensor = self._sourceTensors["target"]
                        if targetTensor.dtype in [tf.int32, tf.int64]:
                            self.appendNode(ModelNode.Layer.Task.Classifier(lossDppKey="target"), buildNo = buildNo)
                        elif targetTensor.dtype in [tf.float32, tf.float64]:
                            self.appendNode(ModelNode.Layer.Task.Regressor(lossDppKey="target"), buildNo = buildNo)
                        else:
                            raise ValueError("The dtype of the target tensor (" + str(targetTensor.dtype) + ") is not supported for auto trainning task assignment.")
                    #   If no target tensor is found, it is assumed a reconstruction is to be done to compare with the source data.
                    elif ("input" in self.dppNodes):
                        inputTensor = self._sourceTensors["input"]
                        if (inputTensor.dtype in [tf.float32, tf.float64]):
                            self.appendNode(ModelNode.Layer.Task.Regressor(lossDppKey="input"), buildNo = buildNo)
                        else:
                            raise ValueError("The dtype of the input tensor (" + str(inputTensor.dtype) + ") is not supported for auto trainning task assignment.")
                    else:
                        raise ValueError("There is default input/target data preprocessing nodes for auto task assignment.")

                # 3C. Batch Normalization Training
                self._bnTensor = tf.compat.v1.placeholder(tf.bool)

                # 3D. For all meta data (Layer Profiles), build the TensorFlow graph
                for x in [l for l in self.modelNodes.values() if len(l.fromSource[buildNo]) > 0 and len(l.fromNode[buildNo]) == 0]:
                    x._build(buildNo)
        
        # 4. Build end event is fired
        self._built = True
        
        # 5. Build end event is fired
        be = BuildEvent("buildend", {"target": self, "buildno": self._buildNo})
        self.dispatchEvent(be)
    
    def _initTraceItems(self, traceLogger, buildNo: int = 0):
        '''
			Initialize trace items.   --- UPDATED (Dexter) 20200115
            
            Parameters
            ------------------------------

            traceLogger     `CSVLogger` - A csv logger for the trace items.

            buildNo     `int`   - The build number to be built.
        '''
        # Get trace items only if there is a request, and there is no existing trace items.
        if (self.traceRecord > 0 and self.traceItems is None):
            traceRootData = self.rootSources.getRandItems(self.traceRecord)
            self.traceItems = self.createUsingData(traceRootData)
        
        # Print the trace item input data. TODO
        initialStep = 0
        nowTime = datetime.now()
        for dppNode in self.getDataPreprocessingNodesUsedByLayers(buildNo = buildNo, inputOnly = True):
            traceLogger.log([nowTime,self._runNo, self._cvNo, initialStep, initialStep, "","Input", dppNode.key, -1, "Table", json.dumps(dppNode.getHeader(step = DataPreprocessing.Node.Columns.StepEnum.Input))])
            traceLogger.log(*[[nowTime,self._runNo, self._cvNo, initialStep, initialStep, "","Input", dppNode.key, sii, *pi] for sii,pi in enumerate(dppNode.getPrintableItems(self.traceItems[dppNode.key], recovered=False))])
        traceLogger.flush()

        # Ask the training source to print the trace items.
        for l in self.getFinalNodes(buildNo=buildNo):
            targetItems = l.getTraceTarget()
            traceLogger.log([nowTime,self._runNo, self._cvNo, initialStep, initialStep, "","Target", l.lossDppKey, -1, "Table", json.dumps(self.dppNodes[l.lossDppKey].getHeader())])
            traceLogger.log(*[[nowTime,self._runNo, self._cvNo,  initialStep, initialStep, "","Target", *ti] for ti in targetItems])
        traceLogger.flush()

    def _train(self, buildNo = 0):
        '''
			Train the model.   --- UPDATED (Dexter) 20200501
        
            Parameters
            ------------------------------

            buildNo    `int`   - The build number of this training build.
        '''
        ### Create the results folder.
        os.makedirs(self.folder + "outputLogs/" + self.trainTime + "/", exist_ok=True)
        os.makedirs(self.folder + "builds/" + self.trainTime + "/", exist_ok=True)
        os.makedirs(self.folder + "tmpLogs/" + self.trainTime + "/", exist_ok=True)
        
        #   0-c.  Collect current training profile.
        nowTrainBuildConfig: Train.BuildConfig = self.currentBuildConfig
        
        #   1. Cache the root source linkages
        #   1-a. Get the actively-using data preprocessing nodes.
        activeDppNodes: List[DataPreprocessing.Node.Config] = self.getDataPreprocessingNodesUsedByLayers()

        #   1-b. Get the root sources with unique and used sources.
        uniqueSources: set(Source.Config) = set()
        for dppNode in activeDppNodes:
            for s in dppNode.getRootSources(rawGraph = False):
                uniqueSources.add(s)
        self._rootSources = Train.RootSources(self, [*uniqueSources])

        #   1-c. Split Test Datasets
        if (self.testDatasetType == DataGenerator.Dataset.TestSource.Split and self.testRatio > 0):
            self.rootSources.splitTestDataset(self.testRatio, self.testShuffle)
            self.testRatio = 0
        
        #   2. Validation Setup
        validationType: str = nowTrainBuildConfig.crossValidationType
        isCrossVal: bool = validationType is not None and self._cvNo >= 0
        validationRuns: int = nowTrainBuildConfig.validationRuns
        if (isCrossVal):
            if (nowTrainBuildConfig._cvCount < validationRuns):
                self.rootSources.nextCrossValidation(nowTrainBuildConfig)
                nowTrainBuildConfig._cvCount += 1
                self.assignCurrentSourceDataset(DataGenerator.Dataset.Types.ValidationTrain)
            elif (nowTrainBuildConfig._cvCount == validationRuns):
                nowTrainBuildConfig._cvCount = 0
                self.assignCurrentSourceDataset(DataGenerator.Dataset.Types.Train)
        else:
            self.assignCurrentSourceDataset(DataGenerator.Dataset.Types.Train)
        
        #   3.  Basic Step Counting
        primaryTrainingSource: Source.Config = self.sources[0]
        batchCountPerEpoch: int = primaryTrainingSource.batchCountPerEpoch
        decaySteps: int = int(batchCountPerEpoch * nowTrainBuildConfig.numEpochsPerDecay) if (nowTrainBuildConfig.numEpochsPerDecay is not None) else None
        
        #   4.  Start Train Looping
        finalStep: int = nowTrainBuildConfig.noOfEpoch * batchCountPerEpoch

        #   5A.  No actions to be taken if the current local step is greater than the final step (usually from a recovered state)
        if (self.localStep >= finalStep):
            self.localStep -= finalStep

        #   5B. Continue for training if the local step is within final step.
        else:
            #   6.  Build the model if it has not built yet.
            if (not self._built):
                self._build(buildNo)

            #   7.  Fire trainbuild event
            tb: TrainEvent = TrainEvent("trainbuild", {"target": self, "buildNo": self._buildNo, "buildConfig": nowTrainBuildConfig})
            self.dispatchEvent(tb)

            with self._graph.as_default():
                #   8.  Define the learning rate
                (learningRate): tf.Tensor
                if nowTrainBuildConfig.learningRateDecayFactor is not None and (nowTrainBuildConfig.learningRateDecayFactor != 1 and nowTrainBuildConfig.learningRateDecayFactor > 0):
                    learningRate = tf.compat.v1.train.exponential_decay(nowTrainBuildConfig.initialLearningRate, self.localStepTensor, decaySteps, nowTrainBuildConfig.learningRateDecayFactor, staircase=True)
                else:
                    learningRate = tf.constant(nowTrainBuildConfig.initialLearningRate)
                
                #   9A.  Collect all the losses.
                totalLoss: tf.Tensor = tf.add_n(tf.compat.v1.get_collection('losses'), name='totalLoss')
                allLosses: tf.Tensor = tf.compat.v1.get_collection('losses')

                #   9B. Apply moving average loss decay.
                (controlStep1): tf.Tensor
                (lossAvgs): tf.Tensor
                if nowTrainBuildConfig.exponentialLossDecay is not None:
                    lossAvgs = tf.train.ExponentialMovingAverage(nowTrainBuildConfig.exponentialLossDecay, name='lossAvgs')
                    controlStep1 = lossAvgs.apply(allLosses + [totalLoss])
                else:
                    controlStep1 = totalLoss

                #   10.  Training Operation
                bnOps: tf.Tensor = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.UPDATE_OPS)
                controlStep2: List[tf.Tensor] = []
                with tf.control_dependencies([controlStep1, *bnOps]):
                    opt: tf.Tensor = Train.optimizers(nowTrainBuildConfig.optimizer)(learningRate, **nowTrainBuildConfig.optimizerParams)
                    controlStep2.append(opt.minimize(totalLoss, global_step=self.localStepTensor))
                
                #   11. Update Global Step
                controlStep2.append(self.globalStepTensor.assign_add(1))

                #   12. Take Moving AVerage Decay on Trainable Variables
                if nowTrainBuildConfig.exponentialVarDecay is not None:
                    variableAvgs: tf.Tensor = tf.train.ExponentialMovingAverage(nowTrainBuildConfig.exponentialVarDecay, self.localStep)
                    controlStep2.append(variableAvgs.apply(tf.compat.v1.trainable_variables()))
                
                #   13. Grab the final training operation
                with tf.control_dependencies(controlStep2):
                    totalLossOP: tf.Tensor = tf.add_n(allLosses, name='totalLoss')
                    avgLossOP: tf.Tensor = totalLossOP/len(allLosses)

                #   14. Create session
                sess: tf.compat.v1.Session = tf.compat.v1.Session(graph=self._graph)
                self._sess = sess
                
                #   15. Restore previous training states or previous build
                sess.run(tf.compat.v1.global_variables_initializer())

            print("Training Initialized.\n")
            if (self.restorePath):
                self._restoreHistory(self.restorePath)
            elif (self._buildNo > 0):
                self._restoreBuild()
            
            #   16. Fire trainstart event
            recorder: TimeHelper.Recorder = TimeHelper.Recorder()
            recorderStep: int = self.localStep-1
            tb: TrainEvent = TrainEvent("trainstart", {"target": self, "buildNo": self._buildNo, "runNo": self._runNo, "cvNo": self._cvNo, "buildConfig": nowTrainBuildConfig})
            self.dispatchEvent(tb)
            
            #   17. Prepare Train Log and Test Log
            toLog: bool = True if self.logFreq > 0 and self.logFreq <= math.inf else False
            if (toLog):
                trainLogger: CSVLogger = CSVLogger(self.folder+"outputLogs/"+self.trainTime+"/", "trainLog_"+str(self._buildNo), ["Timestamp", "Run", "Cross Validation Step", "Global Step", "Local Step", "Total Loss", "Average Loss", "Learning Rate", "Examples per Second", "Seconds per Step"])
            toTest: bool = True if self.testFreq > 0 and self.testFreq <= math.inf else False
            if (toTest):
                self.testLogger: CSVLogger = CSVLogger(self.folder+"outputLogs/"+self.trainTime+"/", "testLog_"+str(self._buildNo), ["Timestamp", "Run", "Cross Validation Step", "Global Step", "Local Step", "Test Type", *[(l.name + ": " + l.measurement) for l in self.getFinalNodes(buildNo=buildNo)]])
            toLogWeight: bool = True if self.weightLogFreq > 0 and self.weightLogFreq <= math.inf else False
            if (toLogWeight):
                weightLogger: CSVLogger = CSVLogger(self.folder+"outputLogs/"+self.trainTime+"/", "weightLog_"+str(self._buildNo), ["Timestamp", "Run", "Cross Validation Step", "Global Step", "Local Step", *[w.name for w in sum([l._weights for l in self.modelNodes.values() if l.weightLogging], [])]])
            toLogTrace: bool = True if self.traceFreq > 0 and self.traceRecord > 0 and self.traceFreq <= math.inf else False
            if (toLogTrace):
                traceLogger: CSVLogger = CSVLogger(self.folder+"outputLogs/"+self.trainTime+"/","traceLog_"+str(self._buildNo), ["Timestamp", "Run", "Cross Validation Step", "Global Step", "Local Step", "Task Name", "Task Type", "Data Preprocessing Node Key", "Item ID", "Data Type", "Data"])
                self._initTraceItems(traceLogger, buildNo = buildNo)
            
            #   misc. cache
            toFireEpochStart: int = len(self._eventFtns["epochstart"])
            toFireEpochEnd: int = len(self._eventFtns["epochend"])
            toFireStepPrepare: int = len(self._eventFtns["stepprepare"])
            toFireStepStart: int = len(self._eventFtns["stepstart"])
            toFireStepEnd: int = len(self._eventFtns["stepend"])

            for i in range(self.localStep, finalStep):
                self.localStep += 1
                self.globalStep += 1
                ii: int = self.localStep

                #   18A.  Fire epochstart event
                if (toFireEpochStart and ((i-1) % batchCountPerEpoch == 0)):
                    es: EpochEvent = EpochEvent("epochstart", {"target": self, "buildNo": self._buildNo, "runNo": self._runNo, "cvNo": self._cvNo, "buildConfig": nowTrainBuildConfig, "i": i, "localStep": ii, "globalStep": self.globalStep, "inputSources": self.sources})
                    self.dispatchEvent(es)

                #   18B.  Fire stepprepare event
                if (toFireStepPrepare):
                    sp: StepEvent = StepEvent("stepprepare", {"target": self, "buildNo": self._buildNo, "runNo": self._runNo, "cvNo": self._cvNo, "buildConfig": nowTrainBuildConfig, "i": i, "localStep": ii, "globalStep": self.globalStep})
                    self.dispatchEvent(sp)
                
                #   18C.  The feeddict object with all input and dropout tensours
                feedDictInput: Dict[tf.Tensor, Union[tf.Tensor, np.ndarray]] = {}
                for dppKey,data in next(self).items():
                    feedDictInput[self._sourceTensors[dppKey]] = data
                feedDictDropout: Dict[tf.Tensor, float] = {l._dropoutTensor: l.dropout for l in self.modelNodes.values() if l.dropout < 1}
                feedDictObj: Dict[tf.Tensor, Union[tf.Tensor, np.ndarray, float]] = {**feedDictInput, **feedDictDropout, self._bnTensor: True}

                #   18D.  Fire stepstart event
                if (toFireStepStart):
                    ss: StepEvent = StepEvent("stepstart", {"target": self, "buildNo": self._buildNo, "runNo": self._runNo, "cvNo": self._cvNo, "buildConfig": nowTrainBuildConfig, "i": i, "localStep": ii, "globalStep": self.globalStep, "feedDict": feedDictObj})
                    self.dispatchEvent(ss)

                with self._graph.as_default():
                    #   18E.  Print debugging tensors
                    if len(self._printTensors):
                        print(sess.run(self._printTensors, feed_dict=feedDictObj))

                    #   18F.  Run the loss function
                    totalLoss: float
                    avgLoss: float
                    totalLoss, avgLoss =  sess.run([totalLossOP, avgLossOP], feed_dict=feedDictObj)

                    #   18G.  Log if needed
                    if (toLog and (ii%self.logFreq == 0 or i == finalStep - 1)):
                        duration: float = recorder.logAndRestart()
                        nowTime: datetime.datetime = datetime.now()
                        examplesPerSec: float = self.logFreq * self.sources[0].batchSize / duration
                        secPerStep: float = float(duration / (self.localStep - recorderStep))
                        recorderStep: int = self.localStep
                        logLR: float
                        localStepNow: int
                        logLR, localStepNow = sess.run([learningRate, self.localStepTensor])
                        trainLogger.log([nowTime, self._runNo, self._cvNo, self.globalStep, ii, escapeNaN(totalLoss), escapeNaN(avgLoss), logLR, examplesPerSec, secPerStep])
                        if (self._cvNo == -1):
                            print('%s: Run #%d, Step %d (%d) --- loss: %f; learning rate: %f; %.1f examples/s; %.3f s/step' % (nowTime, self._runNo, self.globalStep, ii, avgLoss, logLR, examplesPerSec, secPerStep))
                        else:
                            print('%s: Run #%d - Cross Validation %d, Step %d (%d) --- loss: %f; learning rate: %f; %.1f examples/s; %.3f s/step' % (nowTime, self._runNo, self._cvNo, self.globalStep, ii, avgLoss, logLR, examplesPerSec, secPerStep))

                    #   18H.  Print Weights if needed
                    if (toLogWeight and (ii%self.weightLogFreq == 0 or ii == finalStep)):
                        nowTime: datetime.datetime = datetime.now()
                        weightLogger.logAndSave([nowTime, self._runNo, self._cvNo, self.globalStep, ii, *[escapeNaNNPAry(ary).tolist() for ary in sess.run(functools.reduce(lambda a,b: a+b, [l._weights for l in self.modelNodes.values() if l.weightLogging], []))]])

                    #   18I.  Save if needed
                    if (self.saveFreq > 0 and (ii%self.saveFreq == 0 or ii == finalStep)):
                        #   18I-1.  Save the trained graph.
                        self._saveTempTrain()
                        
                        #   18-2.  Flush Training Log
                        if (toLog):
                            trainLogger.flush()
                    
                    #   18J.  Log trace items if needed
                    if (toLogTrace and (ii % self.traceFreq == 0 or ii == finalStep)):
                        #   10J-1.  Predict acoording to the saved model.
                        predictedValues: np.ndarray = self.predict(x=self.traceItems, buildNo = buildNo)
                        nowTime = datetime.now()
                        for l in self.getFinalNodes(buildNo = buildNo):
                            predictedItems: np.ndarray = l.getTraceItems(predictedValues, buildNo = buildNo)
                            if predictedItems is not None:
                                traceLogger.log(*[[nowTime, self._runNo, self._cvNo, self.globalStep, self.localStep, l.name, l.__class__.__name__, *escapeNaNList(pi)] for pi in predictedItems])
                        traceLogger.flush()
                    
                #   18K.  Test if needed, using cross validation data as testing if needed
                if (toTest and (ii%self.testFreq == 0 or ii == finalStep)):
                    #   18K-1.  Evaluate acoording to the saved model.
                    self.evaluate(validation = isCrossVal, event="Test Log", buildNo = buildNo)

                #   18L.  Fire stepend event
                if (toFireStepEnd):
                    se: StepEvent = StepEvent("stepend", {"target": self, "runNo": self._runNo, "cvNo": self._cvNo, "buildNo": self._buildNo, "buildConfig": nowTrainBuildConfig, "i": i, "localStep": ii, "globalStep": self.globalStep, "feedDict": feedDictObj, "totalLoss": totalLoss, "avgLoss": avgLoss})
                    self.dispatchEvent(se)

                #   18M.  Fire epochend event
                if (toFireEpochEnd and ((i) % batchCountPerEpoch == 0)):
                    ee: EpochEvent = EpochEvent("epochend", {"target": self, "runNo": self._runNo, "cvNo": self._cvNo, "buildNo": self._buildNo, "buildConfig": nowTrainBuildConfig, "i": i, "localStep": ii, "globalStep": self.globalStep, "inputSources": self.sources})
                    self.dispatchEvent(ee)
            
            # 19B. Flush Training Log
            if (toLog):
                trainLogger.flush()

            # 20. Save current training graph   --- BETA
            with self._graph.as_default():
                tmpSaver: tf.compat.v1.train.Saver = tf.compat.v1.train.Saver()
                tmpSaver.save(sess, self.folder + 'tmpLogs/' + self.trainTime + '/tmpLog.ckpt')
                tmpSaver.save(sess, self.folder + 'builds/' + self.trainTime + '/build_' + str(self._buildNo) + '/buildLog.ckpt')

            # 21. Dispatch trainend event.
            te: TrainEvent = TrainEvent("trainend", {"target": self, "runNo": self._runNo, "cvNo": self._cvNo, "buildNo": self._buildNo, "buildConfig": nowTrainBuildConfig})
            self.dispatchEvent(te)

            # 21. Close and reset TensorFlow training graph
            self.close()

    def _saveTempTrain(self):
        '''
			Save a temp log for current train status.   --- BETA --- UPDATED (Dexter) 20190118
            '''
        with self._graph.as_default():
            tmpSaver = tf.compat.v1.train.Saver()
            tmpSaver.save(self._sess, self.folder + 'tmpLogs/' + self.trainTime + '/tmpLog.ckpt')
        
        with open(self.folder + 'tmpLogs/' + self.trainTime + '/tmpStep.txt', "w", encoding = "utf-8", newline="") as f:
            f.write(str(self.globalStep-1))

    def _restoreHistory(self, restorePath = None):
        '''
			Restore model training history from a specific path.   --- BETA --- UPDATED (Dexter) 20190118

            Parameters
            ------------------------------

            restorePath     `str`   - The restore folder of the checkpoint history file.
        '''
        # Get the checkpoint info.
        restoreDirectory = ((self.folder  + 'tmpLogs/' + self.trainTime + "/") if (restorePath is None) else self.restorePath)
        tmpSaver = tf.compat.v1.train.Saver([x for x in tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES)])

        # Restore the training session.
        tmpSaver.restore(self._sess, restoreDirectory + "tmpLog.ckpt")
        print("Training restored.\n")

    def _restoreBuild(self):
        '''
			Restore model build training history from a specific path.   --- BETA --- UPDATED (Dexter) 20190118
        '''
        # Get the build checkpoint info.
        restoreDirectory = self.folder  + 'builds/' + self.trainTime + '/build_' + str(self._buildNo) + "/"
        allPreviousBuildNodeNames = [n for n,l in self.modelNodes if l.buildNo < self._buildNo]
        tmpSaver = tf.compat.v1.train.Saver([x for x in tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES) if any([x.name.startswith(n + "/") for n in allPreviousBuildNodeNames])])

        # Restore the training session.
        tmpSaver.restore(self._sess, restoreDirectory + "buildLog.ckpt")
        print("Build restored.\n")
    
    def _nextCrossValidation(self, buildConfig):
        '''
			Prepare the next cross validation dataset.   --- UPDATED (Dexter) 20180630
        '''
        # Update validation dataset if needed.
        for s in self.sources:
            if (s.splittable):
                # Dataset is split according to the training profile settings.
                s.splitValidationDataset(validation=buildConfig.crossValidationProp, shuffle = buildConfig.crossValidationType=="rand")
  
    def _evaluate(self, validation: bool = False, isPredict: bool = False, event: str = "", saveOriginal: bool = False, saveResults: bool = False, buildNo: int = 0) -> Any:
        '''
			Evaluate the training model.   --- UPDATED (Dexter) 20190922

            Parameters
            ------------------------------

            validation  `bool`  - Whether this evaluation is taken on validation dataset.

            isPredict   `bool`  - Whether this evaluation requires a prediction.

            event       `str`   - An event name for this evaluation. If it is "Test Log", it is an evaluation during training.

            saveOriginal    `bool`  - Whether to save original data. (Reserved)

            saveResults     `bool`  - Whether to save result data. (Reserved)

            buildNo     `int`   - The build number to be built.

            Returns
            ------------------------------

            `*`     - Prediction results if needed.
        '''
        # Define the training stage.
        oriSourceDataset = self.currentSourceDataset
        if (validation):
            self.assignCurrentSourceDataset(DataGenerator.Dataset.Types.Validation)    
        else:
            self.assignCurrentSourceDataset(DataGenerator.Dataset.Types.Test)

        #   1.  Confirm the test data are with same batch-size as source data
        predictSources = self._evalSources
        useExternalSource = predictSources is not None
        isProcessedData = isinstance(predictSources, Train.UsingData)
        if isProcessedData:
            if len(predictSources) == 0:
                raise ValueError("There is no test or validation data sources.")
            elif len(self.dppNodes) != len(predictSources):
                raise ValueError("Sources and test sources are not matched.")

        #   2.  Define the epoch size and relevant learning rate, not all data would be looped through depending on previously defined batch size 
        primaryTrainingSource = self.sources[0]
        totalStepCount = math.ceil(predictSources.getCount()/primaryTrainingSource.batchSize) if useExternalSource else primaryTrainingSource.batchCountPerEpoch

        #   3.  Loop the test source in several batches for testing
        finalTensors = self.getFinalNodes(buildNo=buildNo)
        savedResults = {lt.name: None for lt in finalTensors}
        if (self._sess is None):
            sess = self._sess = tf.compat.v1.Session(graph=self._graph)
            sess.run(tf.compat.v1.global_variables_initializer())
            self._restoreHistory()
        
        #   3-0. Initialize dataSize, and all evaluation measurements.
        dataSize = 0
        for lt in finalTensors:
            lt._clearEvalInfo()

        for i in range(0, totalStepCount):
            #   3-1.    Get all test data and count the test data size.
            allTestData = predictSources if isProcessedData else {dppKey: dppData for dppKey,dppData in next(self).items()}
            feedDictInput = {}
            dataSize += max([len(colData) for colData in allTestData.values()])

            #   3-2.    Feed the test data into feed dict object.
            for dppKey,dppTensor in self._sourceTensors.items():
                feedDictInput[dppTensor] = allTestData[dppKey]

            #   3-3.    Feed the dropout tensor as 1 as well.
            feedDictDropout = {l._dropoutTensor: 1 for l in self.modelNodes.values() if l.dropout < 1}
            feedDictObj = {**feedDictInput, **feedDictDropout, self._bnTensor: False}

            #   3-4.    Run the model.
            with self._graph.as_default():
                allPredictedResults = self._sess.run({lt.name: lt._predictionTensors for lt in finalTensors}, feed_dict=feedDictObj)

            #   3-5.    Recover the predicted results from reversed data transformations or circular range bounding.
            for lt in finalTensors:
                allPredictedResults[lt.name] = lt.recoverPredictedResults(allPredictedResults[lt.name])
            
            #   3-6.    If it's an evaluation, partially evaluate this batch of test data for each final layers.
            if not isPredict:
                #   3-6-1.  Partial evaluate each final tensor.
                for lt in finalTensors:
                    lt.partialEvaluate(allTestData, allPredictedResults[lt.name])
            
            #   3-7.  Save all predicted results for all final layers.
            if isPredict:
                for lt in finalTensors:
                    if (savedResults[lt.name] is None):
                        # If it's the first batch testing, initialte with the results.
                        savedResults[lt.name] = allPredictedResults[lt.name]
                    else:
                        # Concatenate the results on the first axis (batch axis [looping on batched testing]).
                        savedResults[lt.name] = np.concatenate((savedResults[lt.name], allPredictedResults[lt.name]), axis=0)
        
        if (isPredict):
            # Revert the dataset reference.
            self.assignCurrentSourceDataset(oriSourceDataset)

            #   4X. Return the results if it's a prediction.
            return savedResults
        else:
            #   4A.  Get the final score by aggregating all previously partially evaluated information.
            toAns = [[lt.measurement, lt.getTestScore()] for lt in finalTensors]

            #   5.  Log the test results.
            nowTime = datetime.now()
            testLog = [nowTime, self._runNo, self._cvNo, self.globalStep, self.localStep, event, *escapeNaNList([ans[1] for ans in toAns])]
            if (self._cvNo == -1):
                print('\nTEST (Validation: %s - %s) Test Data Size: %d\n%s: Run #%d, Step %d (%d) --- \n' % (validation, event, dataSize, nowTime, self._runNo, self.globalStep, self.localStep), *[ans[0] + ": " + ('%f' % ans[1]) for ans in toAns], "\n")
            else:
                print('\nTEST (Validation: %s - %s) Test Data Size: %d\n%s: Run #%d - Cross Validation %d, Step %d (%d) --- \n' % (validation, event, dataSize, nowTime, self._runNo, self._cvNo, self.globalStep, self.localStep), *[ans[0] + ": " + ('%f' % ans[1]) for ans in toAns], "\n")
            if (self.testLogger is not None):
                self.testLogger.logAndSave(testLog)
            
            #   6.  Close Test Sources
            if not isProcessedData:
                if predictSources is not None:
                    # 6A. Case of external sources that are not UsingData.
                    for s in reversed(predictSources):
                        if isinstance(s, Source.Config):
                            s.close()
                else:
                    # 6B. Case of using internal dataset.
                    self.rootSources.closeCurrentDatasets()
            
            # Revert the dataset reference.
            self.assignCurrentSourceDataset(oriSourceDataset)

    def close(self):
        '''
			Close the training model.   --- UPDATED (Dexter) 20200305
        '''
        # In a reversed order, close all the training sources.
        for x in reversed(self.sources):
            x.close()
        
        # Revert the build status, the local step counter, clear print tensors.
        self._built = False
        for lp in self.modelNodes.values():
            lp._built = False
        self.localStep = 0
        self._printTensors = []
    
    def fit(self):
        '''
			Fit (Train) a model.   --- UPDATED (Dexter) 20191002
        '''
        # Train the model.
        self.fullModelTrain()
    
    def evaluate(self, x: Optional[Union['Source.Config', 'np.ndarray', List[List[Any]], List['Source.Config'], 'Source.Data']] = None, 
                batchSize: int = -1, shuffle: bool = False, validation: bool = False, isPredict: bool = False, event: str = "", saveOriginal: bool = False, saveResults: bool = False, buildNo: int = 0):
        '''
			Evaluation this model (A user-oriented API).   --- UPDATED (Dexter) 20190917

            Parameters
            ------------------------------

            x               `Source.Config|np.ndarray[np.ndarray]|list[list]|list[Source.Config]|Source.Data`     - Input data of a model.

            batchSize       `int`   - Batch size of the test dataset. -1: Full epoch evaluation will be processed.

            shuffle         `bool`  - Whether shuffling is needed.

            validation      `bool`  - Whether this evaluation is taken on validation dataset.

            isPredict       `bool`  - Whether this evaluation requires a prediction.
            
            event           `str`   - An event name for this evaluation.

            saveOriginal    `bool`  - Whether to save original data. (Reserved for future use)

            saveResults     `bool`  - Whether to save result data. (Reserved for future use)

            buildNo     `int`       - The build number to be built.
        '''
        #   1A.  If there is evaluation data, set it as test data source.
        if (x is not None):
            if (validation):
                raise ValueError("NSC Internal Error - Validation triggered with customized source data.")

            #   1A-1.    Dependiing on the training source and set up the test sources.
            ## TODO: Remove redundant codes.
            if (isinstance(x, Source.Config)):
                self._evalSources = [x]
            elif (isinstance(x, Train.UsingData)):
                self._evalSources = x
            elif (isinstance(x, list) and all([isinstance(x, Source.Config) for t in x])):
                self._evalSources = x
                '''elif (len(self.sources) == 0 and isinstance(self.sources[0], Source.Table)):
                    self._evalSources = [Source.Table(x,training=False)]'''
            else:
                raise ValueError("Evaluation source is not supported.")
            
            #   1A-2.    Check the compatibility of the original source configurations.
            if (not isinstance(x, Train.UsingData)):
                if (len(self._evalSources) != len(self.sources)):
                    raise ValueError("Evaluation sources length does not match originally desinged model.")
                elif (any([not isinstance(s, self._evalSources[idx].__class__) for idx,s in enumerate(self.sources)])):
                    raise ValueError("Evaluation sources class does not match originally desinged model.")

        #   2. If there are available evaluation source, evaluate the data.
        if (isPredict):
            #   Calculate the prediction results.
            predictionResults = self._evaluate(validation, isPredict = isPredict, event=event, saveOriginal=saveOriginal, saveResults=saveResults, buildNo = buildNo)
            
            #   Clear evaluation sources.
            self._evalSources = None

            return predictionResults
        else:
            #   Evaluate the data.
            self._evaluate(validation, event=event, saveOriginal=saveOriginal, saveResults=saveResults, buildNo = buildNo)
    
            #   Clear evaluation sources.
            self._evalSources = None
    
    def predict(self, x: Optional[Union['Source.Config', 'np.ndarray', List[List[Any]], List['Source.Config'], 'Source.Data']] = None, 
                batchSize: int = -1, shuffle: bool = False, validation: bool = False, event: str = "", saveOriginal: bool = False, saveResults: bool = False, buildNo: int = 0):
        '''
			Predict some data using this model (A user-oriented API).   --- UPDATED (Dexter) 20180713

            Parameters
            ------------------------------

            x               `Source.Config|np.ndarray[np.ndarray]|list[list]|list[Source.Config]|Source.Data`     - Input data of a model.

            batchSize       `int`   - Batch size of the test dataset. -1: Full epoch evaluation will be processed.

            shuffle         `bool`  - Whether shuffling is needed.

            validation      `bool`  - Whether this evaluation is taken on validation dataset.

            event           `str`   - An event name for this evaluation.

            saveOriginal    `bool`  - Whether to save original data. (Reserved for future use)

            saveResults     `bool`  - Whether to save result data. (Reserved for future use)

            buildNo     `int`   - The build number to be built.
        '''
        return self.evaluate(x=x, batchSize=batchSize, shuffle=shuffle, validation=validation, isPredict=True, event=event, saveOriginal=saveOriginal, saveResults=saveResults, buildNo = buildNo)
    
    def perBuildTrain(self, buildNo = 0):
        '''
			Perform a single model train for building once only.   --- UPDATED (Dexter) 20190319
        
            Parameters
            ------------------------------

            buildNo    `int`   - The build number of this training build.
        '''
        # Create temporary global step.
        startGlobalStep = self.globalStep

        # Run for several tiems on this build.
        for runNo in range(0, self.runCount):
            self._runNo += 1
            
            buildConfig = self.buildConfigs[buildNo]
            validationRuns = buildConfig.validationRuns
            if (buildConfig.crossValidationType is None or validationRuns == 1):
                self.globalStep = startGlobalStep
                self._train(buildNo = buildNo)
            else:
                # Loop by each cross validation and overall performance.
                for cv in range(0, validationRuns+1):
                    self.globalStep = startGlobalStep
                    self._cvNo += 1
                    if (self._cvNo == validationRuns):
                        self._cvNo = -1
                    self._train(buildNo = buildNo)
        
        # Reset Run No.
        self._runNo = -1

    def fullModelTrain(self):
        '''
			Perform a full model train with multi-build settings.   --- UPDATED (Dexter) 20190319
        '''
        # If there is a restore path, find the from-globaleStep.   --- BETA
        if self.restorePath:
            if any([tp.crossValidationType is not None for tp in self.buildConfigs]):
                raise ValueError("Train Resuming is not allowed for cross-validating trainings.")
            
            step = 0
            try:
                with open(self.restorePath + '/tmpStep.txt', "r", encoding = "utf-8", newline="") as f:
                    step = f.read().trim()
            except:
                raise ValueError("No model is restored from: " + str(self.restorePath))
                
            self.localStep = self.globalStep = int(step)

        # Train the model for every training profiles, loop by each run.
        for runNo in range(0, self.runCount):
            self._runNo += 1
            self.globalStep = 0

            # Loop by each build.
            for toBuildNo,x in enumerate(self.buildConfigs):
                validationRuns = x.validationRuns
                if (x.crossValidationType is None or validationRuns == 1):
                    self._train(buildNo = toBuildNo)
                else:
                    # Create temporary global step.
                    startGlobalStep = self.globalStep

                    # Loop by each cross validation and overall performance.
                    for cv in range(0, validationRuns+1):
                        self.globalStep = startGlobalStep
                        self._cvNo += 1
                        if (self._cvNo == validationRuns):
                            self._cvNo = -1
                        self._train(buildNo = toBuildNo)
        
        # Reset Run No.
        self._runNo = -1
    
    def addDataSource(self, source):
        '''
			Add a data source to existing collections of this training.   --- UPDATED (Dexter) 20190506

            Parameters
            ------------------------------

            source          `Source.Config`  - A data source to be assigned.
        '''
        source.train = self
    
    def setDataSources(self, *sources):
        '''
			Set (Replace existing) several data sources into this training.   --- UPDATED (Dexter) 20190506

            Parameters
            ------------------------------

            *sources        `Source.Config+`  - A data source to be assigned.
        '''
        for s in sources:
            s.train = self
    
    def getDataSource(self, sourceID = 0, colConfigKey = "input"):
        '''
			Get a particular data source.   --- UPDATED (Dexter) 20180625

            Parameters
            ------------------------------

            sourceID        `int`   - The id of the data source.

            colConfigKey    `str`   - The particular column configuration.

            Returns
            ------------------------------

            `DataPreprocessing.Node.Config`     - A specific column configuration.
        '''
        return self.sources[sourceID][colConfigKey]
    
    def getDataPreprocessingNodesUsedByLayers(self, buildNo: int = 0, inputOnly: bool = False) -> List['DataPreprocessing.Node.Conifg']:
        '''
			Get the necessary data preprocessing nodes used by layers.   --- UPDATED (Dexter) 20200105

            Parameters
            ------------------------------

            buildNo         `int`   - The build no.

            inputOnly       `bool` - Whether only include input only dpp nodes.

            Returns
            ------------------------------

            `list<DataPreprocessing.Node.Conifg>`    - A list of necessary input column information.
        '''
        # Set a set of dpp keys.
        allDppKeys: List[str] = set()

        # Loop for all model nodes.
        for l in self.modelNodes.values():
            # Normally add all layers depending on the dpp nodes.
            if (len(l.fromSource[buildNo]) > 0):
                allDppKeys |= set(l.fromSource[buildNo])
            
            # For task layers, add the loss dpp key as well.
            if (not inputOnly and isinstance(l, ModelNode.Layer.Task.Config)):
                l: ModelNode.Layer.Task.Config
                allDppKeys.add(l.lossDppKey)

        # Return all dpp node objects.
        return [self.dppNodes[dppKey] for dppKey in allDppKeys]

    def getEpochSize(self, refresh: bool = False) -> int:
        """
            Get the epoch size of this training using the current source dataset.   --- UPDATED (Dexter) 20190515

            Parameters
            ------------------------------

            refresh `bool` - Whether to refresh cached epoch size.
            
            Returns
            ------------------------------

            `int` - The epoch size of this training using the current source dataset.
        """
        # Get all the epoch sizes be sources and data preprocessing nodes.
        epochSizes = set()

        # Get the epoch size of each final dpp nodes.
        for dppNode in self.getDataPreprocessingNodesUsedByLayers():
            epochSizes.add(dppNode.getEpochSize(refresh))

        # Raise error if there is inconsistency of epoch sizes.
        if (len(epochSizes) > 1):
            raise ValueError("Inconsistent epoch sizes of data preprocessing nodes.")

        # Return the structured data.
        return [*epochSizes][0]
    
    def getLayersUsingSource(self, sourceID: int = 0, colConfigKey: str = "input", buildNo: int = 0):
        '''
			Get a list of layers using a particular data source.   --- UPDATED (Dexter) 20181115

            Parameters
            ------------------------------

            sourceID        `int`   - The id of the data source.

            colConfigKey    `str`   - The particular column configuration.

            buildNo         `int`   - The build no.

            Returns
            ------------------------------

            `ModelNode.Layer.Config`          - A list of layer profile objects.
        '''
        return [l for l in self.modelNodes.values() if any([(s[0] == sourceID and s[1] == colConfigKey) for s in l.fromSource[buildNo]])]

    def createUsingData(self, rootData: 'Train.RootData') -> 'Train.UsingData':
        '''
			Create a source data object based on structured data as if this Train object.   --- UPDATED (Dexter) 20190515

            Parameters
            ------------------------------

            rootData      `Train.RootData`    - The root data that constitutes the base source for producing the train incoming data.

            Returns
            ------------------------------

            `Train.UsingData`   - A source data object.
        '''
        return Train.UsingData(self, rootData)

    def getOutputTensor(self, name: str) -> 'tf.Tensor':
        '''
			Get an output tensor of a particular layer, normally called during graph building.   --- UPDATED (Dexter) 20190118

            Parameters
            ------------------------------

            name    `str`      - The layer profile name.
            
            Returns
            ------------------------------

            `tf.Tensor`     - The output output of the requested layer.
        '''
        if (name not in self.modelNodes):
            raise ValueError("Requested layer is not within this train object.")

        return self.modelNodes[name]._outputTensor
    
    def getInputSources(self, buildNo: int = 0):
        '''
			Get the necessary input sources.   --- UPDATED (Dexter) 20181115

            Parameters
            ------------------------------

            buildNo         `int`   - The build no.

            Returns
            ------------------------------

            `list[tuple(int,str)+]`    - A list of necessary input column information.
        '''
        return functools.reduce(lambda x,y: x+y, [l.fromSource[buildNo] for l in self.modelNodes.values() if len(l.fromSource[buildNo]) > 0])

    def updateLayerOrder(self, layerProfile: 'ModelNode.Layer.Config', buildNo: int = 0):
        '''
			Initiate an update on layer profile order.   --- UPDATED (Dexter) 20181115

            Parameters
            ------------------------------

            layerProfile    `ModelNode.Layer.Config`  - A layer profile to update.

            buildNo         `int`           - The build no.
        '''
        layerProfile._updateOrder(buildNo=buildNo)
        if (not layerProfile._final):
            for lp in [lp for lp in self.modelNodes.values() if lp._final]:
                lp.updateOrder(buildNo=buildNo)

    def getEndingNodes(self, buildNo: int = 0):
        '''
			Get the current model ending layer profiles (leaf nodes).   --- UPDATED (Dexter) 20190730

            Parameters
            ------------------------------

            buildNo         `int`           - The build no.

            Returns
            ------------------------------

            `list[ModelNode.Layer.Config]`    - The list of ending layer profiles.
        '''
        return [l for l in self.modelNodes.values() if len(l.toNode[buildNo]) == 0]
    
    def getFinalNodes(self, buildNo: int = 0):
        '''
			Get the final layer profiles (layers).   --- UPDATED (Dexter) 20180701

            Parameters
            ------------------------------

            buildNo         `int`           - The build no.

            Returns
            ------------------------------

            `list[ModelNode.Layer.Config]`    - The list of final layer profiles.
        '''
        return [l for l in self.modelNodes.values() if l.isFinal() and len([*l.fromSource[buildNo], *l.fromNode[buildNo]]) > 0]
    
    def getNewLayerName(self, layerProfile: 'ModelNode.Layer.Config'):
        '''
			Get a automated new profile name for the layer profile.   --- UPDATED (Dexter) 20181029

            Parameters
            ------------------------------

            layerProfile    `ModelNode.Layer.Config`      - The layer profile to be appended.
        '''
        layerProfile.name = "Layer"+str(self.getNewLayerID())
    
    def switchOffWeightLogging(self, *layerProfiles: Union[str, 'ModelNode.Layer.Config']):
        '''
			Optionally switch off weightLogging.   --- UPDATED (Dexter) 20190730

            Parameters
            ------------------------------

            layerProfile    `str|ModelNode.Layer.Config`      - The layer profile to disable weight logging.
        '''
        for lp in layerProfiles:
            if isinstance(lp, ModelNode.Layer.Config):
                lp.switchOffWeightLogging()
            elif isinstance(lp, str) or str(lp):
                self.modelNodes[str(lp)].switchOffWeightLogging()
            else:
                raise ValueError("Layer profiles in Train.switchOffWeightLogging() should be a string or LayerProject object.")

    def appendNode(self, layerProfile: 'ModelNode.Layer.Config', appendAt: Union[int, str, 'ModelNode.Layer.Config', 'DataPreprocessing.Node.Config'] = None, buildNo: int = 0):
        '''
			Append a layer to existing ending node, or any specified node.   --- UPDATED (Dexter) 20190730

            Parameters
            ------------------------------

            appendAt        `None|int|str|ModelNode.Layer.Config|DataPreprocessing.Node.Config`      - If `None`, Auto select the ending node; if `str`, a specific layer name; if `int`, a specific source id.

            layerProfile    `ModelNode.Layer.Config`      - The layer profile to be appended.

            buildNo         `int`               - The build no.

            Returns
            ------------------------------

            `Train`   - The current training object.
        '''
        # Ensure the name is not in existing layer profiles.
        if layerProfile.name in self.modelNodes:
            raise ValueError("This layer exists in the model already.")
        # Ensure this layer has not been attached to any training model.
        elif layerProfile.train is not None:
            raise ValueError("This layer is in another model.")
        
        #   1.  If there has not been name given, assign a unique layer name to it.
        if layerProfile.name is None:
            self.getNewLayerName(layerProfile)

        #   2A. If there is no `appendAt`, it automatically find the ending node to append on.
        if appendAt is None:
            endingLayerProfile = self.getEndingNodes(buildNo=buildNo)
            #   2A-A.   If there is one and only one ending node, it can append on it.
            if (len(endingLayerProfile) == 1):
                endingLayerProfile[0].appendNode(layerProfile, buildNo = buildNo)
            #   2A-B.   If there is no ending node, while there is only one data preprocessing node, the layer takes the data preprocessing node as input
            elif (len(endingLayerProfile) == 0 and len(self.dppNodes) == 1):
                layerProfile.addSources((self.dppNodes["input"] if "input" in self.dppNodes else [*self.dppNodes.values()][0]), clear = True, buildNo = buildNo)
            #   2A-C.   Otherwise, the layer cannot be appended
            else:
                raise ValueError("Layer cannot be appended to multiple previous outputs.")
        
        #   2B. If the `appendAt` is a string, get that layer and append on it.
        elif classof(appendAt) in ["str", "String"]:
            # Ensure the requested layer is in this training model.
            if (str(appendAt) not in self.modelNodes):
                raise ValueError("The previous layer (" + str(appendAt) + ") is not in this training model.")

            # Append the new layer to the previous layer.
            self.modelNodes[str(appendAt)].appendNode(layerProfile, buildNo = buildNo)

        #   2C. If the `appendAt` is a layer profile, append on it.
        elif isinstance(appendAt, ModelNode.Layer.Config):
            # Ensure the requested layer is in this training model.
            if (appendAt.name is None):
                raise ValueError("The previous layer is not in any training models.")
            elif (appendAt.name not in self.modelNodes):
                raise ValueError("The previous layer " + (appendAt.name) + " is not in this training models.")

            # Append the new layer to the previous layer.
            appendAt.appendNode(layerProfile, buildNo = buildNo)

        #   2D. If the `appendAt` is an integer, set the source on this new layer.
        elif classof(appendAt) in ["int"] or isinstance(appendAt, DataPreprocessing.Node.Config):
            #   If it is a column config, ensure it is in this training model.
            if isinstance(appendAt, DataPreprocessing.Node.Config) and (appendAt.train != self):
                raise ValueError("The previous column config is not in this training models.")

            layerProfile.addSources(appendAt, clear = True, buildNo = buildNo)

        #   2E. Otherwise, a wrong `appendAt` was specified.
        else:
            raise ValueError("`appendAt` parameter should be either a string (previous layer name), an integer (a data source index), a ModelNode.Layer.Config object (previous layer profile), or a DataPreprocessing.Node.Config object (a data source input data).")

        #   3.  Assign the ModelNode.Layer.Config to this Train object, and add to the collection of the layer profiles.
        layerProfile.train = self
        self.modelNodes[layerProfile.name] = layerProfile

        return self
    
    def appendRepeatedLayers(self, layerProfile, appendAt = None, repeatedCount = 0, buildNo: int = 0):
        '''
			Append a layer in a repeated sequential way to existing ending node, or any specified node.   --- UPDATED (Dexter) 20181115

            Parameters
            ------------------------------

            appendAt        `None|int|str|ModelNode.Layer.Config|DataPreprocessing.Node.Config`      - If `None`, Auto select the ending node; if `str`, a specific layer name; if `int`, a specific source id.

            layerProfile    `ModelNode.Layer.Config`      - The layer profile to be appended.

            repeatedCount   `int`               - Repeated layer count. 0 is the same as the `appendNode()` method.

            buildNo         `int`               - The build no.

            Returns
            ------------------------------

            `Train`   - The current training object.
        '''
        # Ensure the layer is not a final layer.
        if (layerProfile._final):
            raise ValueError("Task Layers cannot be repeated appended.")
        # Ensure the name is not in existing layer profiles.
        elif layerProfile.name in self.modelNodes or any([re.search(layerProfile.name + "R[0-9]+", l.name) for l in self.modelNodes]):
            raise ValueError("This layer exists in the model already.")
        # Ensure this layer has not been attached to any training model.
        elif layerProfile.train is not None:
            raise ValueError("This layer is in another model.")
        
        #   1.  If there has not been name given, assign a unique layer name to it.
        if layerProfile.name is None:
            self.getNewLayerName(layerProfile)
        
        #   2.  Set a counter for repeated layer name.
        rTotal = repeatedCount

        #   3.  Recurssively copy and append the layer.
        while (repeatedCount >= 0):
            #   3-1.    Copy the layer
            ln = layerProfile.name + "R" + str(rTotal - repeatedCount + 1)
            lp = layerProfile.copy(ln)
            #   3-2.    Append the layer
            self.appendNode(lp, appendAt = appendAt, buildNo=buildNo)
            appendAt = lp

    def attachNode(self, layerProfile: Union[str, 'ModelNode.Layer.Config'], *layerOrSources: Union[str, 'ModelNode.Layer.Config', int, Tuple[int, str], 'DataPreprocessing.Node.Config'], buildNo: int = 0):
        '''
			Attach a layer to one or more specific sources or existing layers.   --- UPDATED (Dexter) 20190814
            
            Parameters
            ------------------------------

            layerProfile    `ModelNode.Layer.Config`          - The layer profile to be attached.

            layerOrSources   `list<(ModelNode.Layer.Config|DataPreprocessing.Node.Config)>`  - A list of source index, with its corresponding generated tensor index.

            buildNo         `int`               - The build no.
        '''
        # Ensure there are some input for this layer.
        if (len(layerOrSources) == 0):
            raise ValueError("This layer must attach to at least one precedent source or layer.")
        # Ensure the name is not in existing layer profiles.
        elif layerProfile.name in self.modelNodes:
            raise ValueError("This layer exists in the model already.")
        # Ensure this layer has not been attached to any training model.
        elif layerProfile.train is not None:
            raise ValueError("This layer is in another model.")
        
        #   1.  If there has not been name given, assign a unique layer name to it.
        if layerProfile.name == None:
            self.getNewLayerName(layerProfile)

        #   2. Assign the ModelNode.Layer.Config to this Train.
        layerProfile.train = self

        #   3.  Loop all attaching layer or sources.
        for ls in layerOrSources:
            #   3A. If this is a layer.
            if (isinstance(ls, ModelNode.Layer.Config)):
                #   Ensure the previous layer is in this training model.
                if (ls not in self.modelNodes):
                    raise ValueError("The previous layer (" + ls + ") is not in this training model.")
                elif (isinstance(ls, ModelNode.Layer.Config)):
                    if (ls.name is None):
                        raise ValueError("The previous layer is not in any training models.")
                    elif (ls.name not in self.modelNodes):
                        raise ValueError("The previous layer " + (ls.name) + " is not in this training models.")
                    
                #   Get the previous layer and append on it.
                previousLayer = self.modelNodes[ls] if classof(ls) == "str" else ls
                previousLayer.appendNode(layerProfile, buildNo = buildNo)

            #   3B. If this is a data source.   
            else:
                #   If it is a col config, ensure it is in this training model.
                if isinstance(ls, DataPreprocessing.Node.Config) and (ls.train != self):
                    raise ValueError("The previous column config is not in this training models.")
                
                #   Add the source to the layer profile.
                layerProfile.addSources(ls, buildNo = buildNo)

        #   4. Set the order.
        self.updateLayerOrder(layerProfile, buildNo=buildNo)
        self.modelNodes[layerProfile.name] = layerProfile 
      
    def appendNodes(self, appendAt: Optional[Union[int, str, 'ModelNode.Layer.Config', 'DataPreprocessing.Node.Config']] = None, *layerProfiles: List['ModelNode.Layer.Config'], buildNo: int = 0):
        '''
			Append multiple layers to the network.   --- UPDATED (Dexter) 20181115

            Parameters
            ------------------------------

            appendAt        `None|int|str|ModelNode.Layer.Config|DataPreprocessing.Node.Config`      - If `None`, Auto select the ending node; if `str`, a specific layer name; if `int`, a specific source id.

            *layerProfile    `ModelNode.Layer.Config+`    - One or multiple layer profile(s) to be appended.

            buildNo         `int`               - The build no.

            Returns
            ------------------------------

            `Train`   - The current training object.
        '''
        # Reverse the order all layers to be allowed for setting up the appendAt of each step of the layers.
        toAppendAt = [*reversed([appendAt, *[l.name for l in layerProfiles]][:-1])]

        # Append the layer in order.
        for l in enumerate(layerProfiles):
            self.appendNode(l, appendAt=toAppendAt.pop(), buildNo=buildNo)

        return self

    def appendBasicLayers(self, hiddenUnitCounts: Tuple[int] = (100), activation: 'Train.Activation' = Activation.Relu, appendAt: Optional['ModelNode.Layer.Config'] = None, buildNo: int = 0):
        '''
			Append fully connected layers to the network.   --- UPDATED (Dexter) 20200120

            Parameters
            ------------------------------

            hiddenUnitCounts    `tuple(int+)`   - The number of hidden units of layers in a sequential order.

            activation          `Train.Activation`           - The kind of activation function of the hidden layers.

            appendAt            `ModelNode.Layer.Config`  - The layer to be appended on.

            buildNo             `int`               - The build no.
        '''
        nowAppendAt = appendAt
        for h in hiddenUnitCounts:
            newLayerName = "Basic"+str(self.getNewLayerID())
            self.appendNode(ModelNode.Layer.FullyConnected(layerUnits=h,name=newLayerName,activation=activation), appendAt=nowAppendAt, buildNo=buildNo)
            nowAppendAt = newLayerName
        return self
    
    def appendSymmetricLayers(self, buildNo: int = 0):
        '''
			Append symmetric layer structure to the network, typically useful for Auto Encoder structure.   --- BETA --- UPDATED (Dexter) 20190730

            Parameters
            ------------------------------

            buildNo             `int`               - The build no.
            
            Returns
            ------------------------------

            `Train`   - This Train object
        '''
        # Get all the ending layers in the model.
        endingLayerProfile = self.getEndingNodes(buildNo=buildNo)

        # Ensure it is not a Task Layer.
        if (len(endingLayerProfile) != 1):
            raise ValueError("Symmetric Layer Creation only allows for a single final layer scenario.")
        elif (isinstance(endingLayerProfile[0], ModelNode.Layer.Task.Config)):
            raise ValueError("Symmetric Layer Creation can't be stacked on ModelNode.Layer.Task.Config objects.")
        
        # Maintain a list of whether the symmetric nodes are built
        currentLayers = endingLayerProfile
        currentLayerNames = [l.name for l in currentLayers]
        builtLayerList = {ln: (True if ln in currentLayerNames else False) for ln in self.modelNodes.keys()}

        # Loop the layer paths.
        for l in currentLayers:
            l.setSymmetricLayer(builtLayerList, startFromThis = True, buildNo = buildNo)

        return self

    def addEventListener(self, eventName, ftn):
        '''
			Add an event listener.   --- UPDATED (Dexter) 20180630

            Parameters
            ------------------------------

            eventName   `str`       - The event name.

            ftn         `Function`  - Function to be triggered on the specified event.
        '''
        self._eventFtns[eventName].append(ftn)
    
    def removeEventListener(self, eventName, ftn):
        '''
			Remove and event listener.   --- UPDATED (Dexter) 20180630

            Parameters
            ------------------------------

            eventName   `str`       - The event name.

            ftn         `Function`  - Function to be removed from the specified event.
        '''
        # Only remove if it has.
        if ftn in self._eventFtns[eventName]:
            self._eventFtns[eventName].pop(self._eventFtns[eventName].index(ftn))

    def get_batch_data(self):
        '''
			get batch data from sources   --- UPDATED (Kai Hsiang) 20190717
        '''
        batch_data = []
        for si,s in enumerate(self.sources):
            tmp = {}
            for key,data in next(s.trainset).items():
                tmp[key] = data
            batch_data.append(tmp)
        return batch_data

    def create_model_file(self, buildNo = 0):
        '''
			write all string to keras_model.py   --- UPDATED (Kai Hsiang, Dexter) 20200120

            Parameters
            ------------------------------

            optimizer   `str`       - The optimizer name.

            buildNo     `int`       - The build no.
        '''
        self.model_file = open('keras_model.py', 'w')
        # import
        self.model_file.write('import tensorflow as tf\n')
        self.model_file.write('import numpy as np\n')
        if self.import_hub:
            self.model_file.write('import tensorflow_hub as hub\n')
        self.model_file.write('from tensorflow.keras.layers import Dense, Conv2D, Reshape, Input, Flatten, BatchNormalization\n')
        self.model_file.write('from tensorflow import initializers\n')
        self.model_file.write('from tensorflow.keras import regularizers\n')
        self.model_file.write('from datetime import datetime\n')
        # group
        try:
            for key,value in self.sub_model.items():
                self.model_file.write('\n\n')
                self.model_file.write('class %s(tf.keras.Model):\n'%(key))
                self.model_file.write('\tdef __init__(self):\n')
                self.model_file.write('\t\tsuper(%s, self).__init__()\n'%(key))
                for obj in value[0]:
                    self.model_file.write('\t\t%s\n'%(obj))
                self.model_file.write('\n')
                self.model_file.write('\tdef call(self, x, training=False):\n')
                for obj in value[1]:
                    self.model_file.write('\t\t%s\n'%(obj))
        except:
            pass
        # model
        self.model_file.write('\n\n')
        self.model_file.write('class eagerModel(tf.keras.Model):\n')
        # __init__
        self.model_file.write('\tdef __init__(self):\n')
        self.model_file.write('\t\tsuper(eagerModel, self).__init__()\n')
        # self.model_file.write('\t\tself.lr = %g\n' % (self.buildConfigs[buildNo].initialLearningRate))
        self.write_init()
        # learning_dict = {4: self.Task4_learningConfig, 5: self.Task5_learningConfig}
        learning_dict = [(l[0]+": "+l[1]) for l in self.learning_list]
        self.model_file.write('\t\tself.learning_list = {%s}\n\n' % (', '.join(learning_dict)))
        # self.model_file.write('\t\tself.optimizer_list = [%s]\n\n' % (', '.join(self.optimizer_list)))
        # __call__
        self.model_file.write('\tdef call(self, batch_source, training=False):\n')
        self.write_call()
        self.model_file.write('\n')
        # loss value
        self.model_file.write('\tdef loss_value(self):\n')
        self.write_loss()
        self.model_file.write('\n')
        self.model_file.write('\tdef update(self, tape, task_order):\n')
        self.write_optimizer()
        self.model_file.write('\n')
        self.model_file.write('\tdef general_s2d(self, tensor, input_shape, final_shape, mul_list, trans_parm, block_size):\n')
        self.model_file.write('\t\ttensor = tf.convert_to_tensor(tensor)\n')
        self.model_file.write('\t\taxis = 1\n')
        self.model_file.write('\t\tfor i in range(len(block_size)):\n')
        self.model_file.write('\t\t\ttensor = tf.split(tensor, mul_list[i], axis = axis)\n')
        self.model_file.write('\t\t\tif i != len(block_size)-1:\n')
        self.model_file.write('\t\t\t\taxis += 1\n')
        self.model_file.write('\t\t\t\ttensor = tf.concat(tensor, axis = axis)\n')
        self.model_file.write('\t\t\telse:\n')
        self.model_file.write('\t\t\t\ttensor = tf.transpose(tensor, perm=trans_parm)\n')
        self.model_file.write('\t\t\t\ttensor = tf.reshape(tensor, final_shape)\n')
        self.model_file.write('\t\treturn tensor\n')
        # self.model_file.write('\n')
        # self.model_file.write('\tdef get_lr(self):\n')
        # self.model_file.write('\t\treturn self.lr\n')
        # check RNN
        isRecurrentCreated = [False,False,False,False,False,False]
        for layerProfile in self.modelNodes.keys():
            if self.modelNodes[layerProfile].nodeType == ModelNode.Types.Layer.value:
                if self.modelNodes[layerProfile]._layerType == ModelNode.Layer.Types.RNN.value:
                    if self.modelNodes[layerProfile].isKerasLayer == False:
                        isRecurrentCreated[0] = True
                elif self.modelNodes[layerProfile]._layerType == ModelNode.Layer.Types.GRU.value:
                    if self.modelNodes[layerProfile].isKerasLayer == False:
                        isRecurrentCreated[1] = True
                elif self.modelNodes[layerProfile]._layerType == ModelNode.Layer.Types.LSTM.value:
                    if self.modelNodes[layerProfile].isKerasLayer == False:
                        isRecurrentCreated[2] = True
                elif self.modelNodes[layerProfile]._layerType == ModelNode.Layer.Types.BiRNN.value:
                    if self.modelNodes[layerProfile].isKerasLayer == False:
                        isRecurrentCreated[3] = True
                elif self.modelNodes[layerProfile]._layerType == ModelNode.Layer.Types.BiGRU.value:
                    if self.modelNodes[layerProfile].isKerasLayer == False:
                        isRecurrentCreated[4] = True
                elif self.modelNodes[layerProfile]._layerType == ModelNode.Layer.Types.BiLSTM.value:
                    if self.modelNodes[layerProfile].isKerasLayer == False:
                        isRecurrentCreated[5] = True

        if isRecurrentCreated[0]:
            self.write_RNN()
        if isRecurrentCreated[1]:
            self.write_GRU()
        if isRecurrentCreated[2]:
            self.write_LSTM()
        if isRecurrentCreated[3]:
            self.write_BiRNN()
        if isRecurrentCreated[4]:
            self.write_BiGRU()
        if isRecurrentCreated[5]:
            self.write_BiLSTM()
        self.model_file.close()

    def connect_keras_layer(self, buildNo = 0):
        '''
			create and connect keras layer object for keras_model.py   --- UPDATED (Kai Hsiang, Dexter) 20200119

            Parameters
            ------------------------------

            buildNo         `int`   - The build no.
        '''
        self.sub_model = {} #group
        self.call_obj = [] #call function
        self.return_obj = [] 
        self.layer_obj = [] # __init__
        self.loss_obj = []  # loss
        self.loss_ret = []
        self.update_obj = [] # update
        self.return_loss = []
        self.learning_list = []
        self.import_hub = False
        # self.optimizer_list = []
        RM = RegisterManger()
        call_order = {}
        order_list = []
        use_time = {} # final layer=> -1
        for key, value in self.modelNodes.items():
            toNode = value.toNode[buildNo]
            # if any(isinstance(node, _ModelNodeTFHub) for node in toNode):
            if isinstance(value, _ModelNodeTFHub.Config):
                self.import_hub = True
            if len(toNode) == 0:
                use_time[key] = -1
            else:
                use_time[key] = len(toNode)
            try:
                call_order[value._order[buildNo]].append(key)
            except:
                call_order[value._order[buildNo]] = [key]
        
        for key, value in call_order.items():
            order_list.append(key)
        order_list.sort()
        # print(order_list)
        # start connect layer
        for order in order_list:
            for layer_name in call_order[order]:
                # print(layer_name)
                # if layer_name in ['Task1','Task2']:
                #     # print(self.modelNodes[layer_name].trainingConfig)
                #     value = self.modelNodes[layer_name]
                #     layer, loss, update = value.keras_call()
                #     for string in layer:
                #         self.layer_obj.append(string)
                #     for string in loss:
                #         self.loss_obj.append(string)
                #     for string in update:
                #         self.update_obj.append(string)
                #     break
                value = self.modelNodes[layer_name]
                try:
                    if value._type == 'Task':
                        self.learning_list.append((value.name.replace('Task', ''), 'self.' + value.name + '_learningConfig'))
                        # self.optimizer_list.append('self.' + value.name + '_optimizer')
                        layer, loss, update = value.keras_call(value.name.replace('Task', ''))
                        for string in layer:
                            self.layer_obj.append(string)
                        # for string in loss[0]:
                        #     self.loss_obj.append(string)
                        self.loss_obj.append(loss[0])
                        # print(self.loss_obj)
                        self.loss_ret.extend(loss[1])
                        for string in update:
                            self.update_obj.append(string)

                        # self.return_obj.append('self.%s_output'%(layer_name))
                        # self.return_loss.append('%s_loss'%(layer_name))
                        # self.loss_obj.append(value.keras_get_loss())
                        break
                except:
                    # print('Build error: Task')
                    pass

                try:
                    if value._type == 'Group':
                        g_layer, g_call = value.keras_group(buildNo, self)
                        if len(g_layer)>0:
                            self.sub_model[layer_name] = [g_layer,g_call]
                except:
                    pass
                input_sources_shape, input_tensors_shape = [],[]
                input_sources_name, input_tensors_name = [],[]
                
                # final layer
                # try:
                #     if value._type == 'Task':
                #         self.return_obj.append('self.%s_output'%(layer_name))
                #         self.return_loss.append('%s_loss'%(layer_name))
                #         self.loss_obj.append(value.keras_get_loss())
                # except:
                #     pass
                
                # get input name and shape
                '''for s in value.fromSource[buildNo]:
                    source_id,source_key = s
                    input_shape = [x if x != 'None' else None for x in self.sources[source_id].getColShape(source_key)]
                    input_sources_shape.append(input_shape)
                    s_name = str(source_id) + str(source_key)
                    try:
                        reg = RM.name_to_reg[s_name]
                    except:
                        RM.get_register(s_name)
                        reg = RM.name_to_reg[s_name]
                        self.call_obj.append('%s = tf.cast(batch_source[%d][\'%s\'], tf.float32)'%(reg, source_id,source_key))
                    
                    input_sources_name.append(reg)'''

                for dpp_key in value.fromSource[buildNo]:
                    input_shape = [x if x != 'None' else None for x in self.dppNodes[dpp_key].getShape(False)]
                    input_sources_shape.append(input_shape)
                    s_name = dpp_key
                    if dpp_key in RM.name_to_reg:
                        reg = RM.name_to_reg[dpp_key]
                    else:
                        RM.get_register(dpp_key)
                        reg = RM.name_to_reg[dpp_key]
                        if isinstance(self.dppNodes[dpp_key], DataPreprocessing.Node.BERT):
                            self.call_obj.append('%s = batch_source[\'%s\']'%(reg, dpp_key))
                            self.call_obj.append('self.input_output = batch_source[\'input\']')
                            # self.call_obj.append('self.input_output = tf.cast(batch_source[\'input\'], tf.float32)')
                            self.call_obj.append('self.target_output = tf.cast(batch_source[\'target\'], tf.float32)')
                        else:
                            self.call_obj.append('%s = tf.cast(batch_source[\'%s\'], tf.float32)'%(reg, dpp_key))
                            self.call_obj.append('self.input_output = tf.cast(batch_source[\'input\'], tf.float32)')
                            self.call_obj.append('self.target_output = tf.cast(batch_source[\'target\'], tf.float32)')
                    
                    input_sources_name.append(reg)

                for t in value.fromNode[buildNo]:
                    input_shape = [x if x != 'None' else None for x in t._shape[buildNo]]
                    input_tensors_shape.append(input_shape)
                    reg = RM.name_to_reg[t.name]
                    use_time[t.name] -= 1
                    if use_time[t.name] == 0:
                        RM.free_register(t.name)
                    input_tensors_name.append(reg)
                
                # get new register
                RM.get_register(layer_name)
                output_reg = RM.get_register(layer_name)
                
                # combine incoming tensors
                try:
                    input_shape, combine_layer, combine_call, combine_output_name = value.keras_call_combineIncomingTensors \
                    (input_sources_shape, input_tensors_shape, input_sources_name, input_tensors_name, output_reg)
                except: 
                    pass
                for string in combine_layer:
                    self.layer_obj.append(string)
                for string in combine_call:
                    self.call_obj.append(string)
                # auto reshape
                try:
                    auto_layer, reshape_call, reshape_output_name, new_shape = value.keras_call_auto_reshape_build(input_shape, combine_output_name, output_reg)
                except:
                    pass
                for string in auto_layer:
                    self.layer_obj.append(string)
                for string in reshape_call:
                    self.call_obj.append(string)
                # layer object
                try:
                    layer, call_layer, layer_output_name, new_shape = value.keras_call_layer_build(reshape_output_name, output_reg, new_shape)
                except:
                    pass
                for string in call_layer:
                    if string != '':
                        self.call_obj.append(string)
                for string in layer:
                    self.layer_obj.append(string)
                # output config
                layer, call_layer = value.keras_call_processOutputTensor(new_shape, layer_output_name, output_reg)
                for string in call_layer:
                    if string != '':
                        self.call_obj.append(string)
                for string in layer:
                    self.layer_obj.append(string)
                try:
                    if value.forLoss:
                        self.call_obj.append('self.%s_output = %s'%(value.name, output_reg))
                        self.return_obj.append('self.%s_output'%(layer_name))
                except:
                    pass
                               
    def write_RNN(self):
        '''
			write RNN class to keras_model.py.   --- UPDATED (CYK) 20190925

        '''
        self.model_file.write('\n\n')
        self.model_file.write('class RNN(tf.keras.layers.Layer):\n')
        #init
        self.model_file.write('\tdef __init__(self,units,activation = \'tanh\',kernelInitializer = \'glorot_uniform\',recurrentInitializer = \'orthogonal\',biasInitializer = \'zero\',\n')
        self.model_file.write('\t\t\tkernelRegularizer = None,recurrentRegularizer = None,biasRegularizer = None,kernelConstraint = None,recurrentConstraint = None,biasConstraint = None,\n')
        self.model_file.write('\t\t\tuseBias = True,dropout = 0.,recurrentDropout = 0.,stateClip = None,statefull = False,goBackwards = False,returnSequenceLen = 1,returnState = False,**kwargs):\n')
        self.model_file.write('\t\tsuper(RNN, self).__init__(**kwargs)\n')
        self.model_file.write('\t\tself.units = units\n')
        self.model_file.write('\t\tself.activation =  tf.keras.activations.get(activation)\n')
        self.model_file.write('\t\tself.kernelInitializer = tf.keras.initializers.get(kernelInitializer)\n')
        self.model_file.write('\t\tself.recurrentInitializer = tf.keras.initializers.get(recurrentInitializer)\n')
        self.model_file.write('\t\tself.biasInitializer = tf.keras.initializers.get(biasInitializer)\n')
        self.model_file.write('\t\tself.kernelRegularizer = tf.keras.regularizers.get(kernelRegularizer)\n')
        self.model_file.write('\t\tself.recurrentRegularizer =tf.keras.regularizers.get(recurrentRegularizer)\n')
        self.model_file.write('\t\tself.biasRegularizer = tf.keras.regularizers.get(biasRegularizer)\n')
        self.model_file.write('\t\tself.kernelConstraint = tf.keras.constraints.get(kernelConstraint)\n')
        self.model_file.write('\t\tself.recurrentConstraint = tf.keras.constraints.get(recurrentConstraint)\n')
        self.model_file.write('\t\tself.biasConstraint = tf.keras.constraints.get(biasConstraint)\n')
        self.model_file.write('\t\tself.useBias = useBias\n')
        self.model_file.write('\t\tself.dropout = dropout\n')
        self.model_file.write('\t\tself.recurrentDropout = recurrentDropout\n')
        self.model_file.write('\t\tself.stateClip = stateClip\n')
        self.model_file.write('\t\tself.statefull = statefull\n')
        self.model_file.write('\t\tself.goBackwards = goBackwards\n')
        self.model_file.write('\t\tself.returnSequenceLen = returnSequenceLen\n')
        self.model_file.write('\t\tself.returnState = returnState\n')
        self.model_file.write('\t\tself._initialState = None\n\n')
        #build
        self.model_file.write('\tdef build(self, input_shape):\n')
        self.model_file.write('\t\tinput_dim = int(input_shape[-1])\n')
        self.model_file.write('\t\tself.kernel = self.add_weight(shape=(input_dim,self.units),name=\'kernel\',initializer=self.kernelInitializer,regularizer=self.kernelRegularizer,constraint=self.kernelConstraint)\n')
        self.model_file.write('\t\tself.recurrentKernel = self.add_weight(shape=(self.units,self.units),name=\'recurrentKernel\',initializer=self.recurrentInitializer,regularizer=self.recurrentRegularizer,constraint=self.recurrentConstraint)\n')
        self.model_file.write('\t\tif self.useBias:\n')
        self.model_file.write('\t\t\tself.bias = self.add_weight(shape=(self.units,),name=\'bias\',initializer=self.biasInitializer,regularizer=self.biasRegularizer,constraint=self.biasConstraint)\n')
        self.model_file.write('\t\tself.built = True\n\n')
        #_stepFunction
        self.model_file.write('\tdef _stepFunction(self,inputs,state,training = None):\n')
        self.model_file.write('\t\tif 0 < self.dropout < 1 : inputs = tf.keras.backend.in_train_phase(tf.nn.dropout(inputs,self.dropout),inputs,training=training)\n')  
        self.model_file.write('\t\tif 0 < self.recurrentDropout < 1 : state = tf.keras.backend.in_train_phase(tf.nn.dropout(state,self.recurrentDropout),state,training=training)\n')  
        self.model_file.write('\t\tif self.useBias: output = self.activation(tf.matmul(state,self.recurrentKernel) + tf.matmul(inputs,self.kernel) + self.bias)\n')  
        self.model_file.write('\t\telse: output = self.activation(tf.matmul(state,self.recurrentKernel) + tf.matmul(inputs,self.kernel))\n') 
        self.model_file.write('\t\tif self.stateClip is not None:\n')
        self.model_file.write('\t\t\tnewState = tf.clip_by_value(output,-1*self.stateClip,self.stateClip)\n')
        self.model_file.write('\t\t\treturn output,newState\n')
        self.model_file.write('\t\treturn output,output\n\n')
        #_switchBatchTime
        self.model_file.write('\tdef _switchBatchTime(self,inputs):\n')
        self.model_file.write('\t\tperm = list(range(inputs.get_shape().rank))\n')
        self.model_file.write('\t\tperm[0:2] = (1,0)\n')
        self.model_file.write('\t\treturn tf.transpose(inputs,perm)\n\n')
        #_getInitialState
        self.model_file.write('\tdef _getInitialState(self,inputs):\n')
        self.model_file.write('\t\tinitialState = tf.zeros_like(inputs)\n')          
        self.model_file.write('\t\tinitialState = initialState[:,0,0:1]\n')
        self.model_file.write('\t\tmultiples = tf.convert_to_tensor([1,self.units])\n')
        self.model_file.write('\t\tinitialState = tf.tile(initialState,multiples)\n')
        self.model_file.write('\t\treturn initialState\n\n')
        #call
        self.model_file.write('\tdef call(self, inputs ,state = None):\n') 
        self.model_file.write('\t\tbatchSize,timeSteps,_ = inputs.shape.as_list()\n') 
        self.model_file.write('\t\ttimeStepsTensor = tf.shape(inputs)[1]\n')
        self.model_file.write('\t\tinitialState = state if state is not None else self._initialState\n')
        self.model_file.write('\t\tif initialState is None : initialState = self._getInitialState(inputs)\n')
        self.model_file.write('\t\tinputs = self._switchBatchTime(inputs)\n')
        self.model_file.write('\t\tinitialTime = tf.zeros_like(timeStepsTensor,name=\'time\')\n')
        self.model_file.write('\t\tinputsTensorArray = tf.TensorArray(dtype=inputs.dtype,size=timeStepsTensor)\n')
        self.model_file.write('\t\tinputsTensorArray = inputsTensorArray.unstack(tf.reverse(inputs,tf.zeros(1, tf.int32))) if self.goBackwards else inputsTensorArray.unstack(inputs)\n') 
        self.model_file.write('\t\tinitialOutputTensorArray = tf.TensorArray(dtype=inputs.dtype,size=timeStepsTensor)\n\n')
        self.model_file.write('\t\tdef _cond(time,outputTensorArray,state):\n')
        self.model_file.write('\t\t\treturn time<timeStepsTensor\n\n')
        self.model_file.write('\t\tdef _body(time,outputTensorArray,state):\n')
        self.model_file.write('\t\t\tcurrentInput = inputsTensorArray.read(time)\n')
        self.model_file.write('\t\t\toutput,state = self._stepFunction(currentInput,state)\n')
        self.model_file.write('\t\t\toutputTensorArray = outputTensorArray.write(time,output)\n')
        self.model_file.write('\t\t\treturn (time+1,outputTensorArray,state)\n\n')
        self.model_file.write('\t\tfinalOutputs = tf.while_loop(cond=_cond,body=_body,loop_vars=(initialTime,initialOutputTensorArray,initialState))\n')
        self.model_file.write('\t\tnewState = finalOutputs[-1]\n') 
        self.model_file.write('\t\tfinalOutputTensorArray = finalOutputs[1]\n')
        self.model_file.write('\t\tfinalOutput = finalOutputTensorArray.stack()\n') 
        self.model_file.write('\t\tfinalOutput.set_shape([timeSteps,batchSize,self.units])\n')
        self.model_file.write('\t\tif self.statefull : self._initialState = newState\n')
        self.model_file.write('\t\tif self.returnSequenceLen is 1 : finalOutput = finalOutput[-1]\n')
        self.model_file.write('\t\telif self.returnSequenceLen is not None:\n')
        self.model_file.write('\t\t\tfinalOutput = finalOutput[-self.returnSequenceLen:]\n')
        self.model_file.write('\t\t\tfinalOutput = self._switchBatchTime(finalOutput)\n')
        self.model_file.write('\t\telse: finalOutput = self._switchBatchTime(finalOutput)\n')
        self.model_file.write('\t\tif self.returnState : return [finalOutput,newState]\n')
        self.model_file.write('\t\telse: return finalOutput\n\n')
        #compute_output_shape
        self.model_file.write('\tdef compute_output_shape(self,input_shape):\n')
        self.model_file.write('\t\tbatchSize,timeSteps,_ = input_shape\n')
        self.model_file.write('\t\tif self.returnSequenceLen in [1,None]:\n')
        self.model_file.write('\t\t\toutputShape = tf.TensorShape([batchSize,self.units]) if self.returnSequenceLen == 1 else tf.TensorShape([batchSize,timeSteps,self.units])\n')
        self.model_file.write('\t\telse:\n')
        self.model_file.write('\t\t\tif timeSteps is None or timeSteps < self.returnSequenceLen : raise ValueError(\'returnSequenceLen not match to timeSteps\')\n')
        self.model_file.write('\t\t\telse: outputShape = tf.TensorShape([batchSize,self.returnSequenceLen,self.units])\n')
        self.model_file.write('\t\tif not self.returnState : return outputShape\n')
        self.model_file.write('\t\tstateShape = tf.TensorShape([batchSize,self.units])\n')
        self.model_file.write('\t\treturn [outputShape,stateShape]\n\n')
        #get_config
        self.model_file.write('\tdef get_config(self):\n')
        self.model_file.write('\t\tconfig = {\'units\': self.units,\'activation\': tf.keras.activations.serialize(self.activation),\'kernelInitializer\': tf.keras.initializers.serialize(self.kernelInitializer),\n')
        self.model_file.write('\t\t\t\t\t\'recurrentInitializer\': tf.keras.initializers.serialize(self.recurrentInitializer),\'biasInitializer\': tf.keras.initializers.serialize(self.biasInitializer),\n')
        self.model_file.write('\t\t\t\t\t\'kernelRegularizer\': tf.keras.regularizers.serialize(self.kernelRegularizer),\'recurrentRegularizer\': tf.keras.regularizers.serialize(self.recurrentRegularizer),\n')
        self.model_file.write('\t\t\t\t\t\'biasRegularizer\': tf.keras.regularizers.serialize(self.biasRegularizer),\'useBias\': self.useBias,\'dropout\': self.dropout,\'recurrentDropout\': self.recurrentDropout,\n')
        self.model_file.write('\t\t\t\t\t\'stateClip\': self.stateClip,\'statefull\': self.statefull,\'goBackwards\':self.goBackwards,\'returnSequenceLen\':self.returnSequenceLen,\'returnState\':self.returnState}\n')
        self.model_file.write('\t\tbaseConfig = super(RNN, self).get_config()\n')
        self.model_file.write('\t\treturn dict(list(baseConfig.items()) + list(config.items()))\n')
       
    def write_GRU(self):
        '''
			write GRU class to keras_model.py.   --- UPDATED (CYK) 20191115

        '''
        self.model_file.write('\n\n')
        self.model_file.write('class GRU(tf.keras.layers.Layer):\n')
        #Activation
        self.model_file.write('\tclass Activation():\n')
        self.model_file.write('\t\tdef __init__(self,resetGate,updateGate,stateCandidate):\n')
        self.model_file.write('\t\t\tself.resetGate = tf.keras.activations.get(resetGate)\n')
        self.model_file.write('\t\t\tself.updateGate = tf.keras.activations.get(updateGate)\n')
        self.model_file.write('\t\t\tself.stateCandidate = tf.keras.activations.get(stateCandidate)\n\n')
        #Initializer
        self.model_file.write('\tclass Initializer():\n')
        self.model_file.write('\t\tdef __init__(self,resetGate,updateGate,stateCandidate):\n')
        self.model_file.write('\t\t\tself.resetGate = tf.keras.initializers.get(resetGate)\n')
        self.model_file.write('\t\t\tself.updateGate = tf.keras.initializers.get(updateGate)\n')
        self.model_file.write('\t\t\tself.stateCandidate = tf.keras.initializers.get(stateCandidate)\n\n')
        #Regularizer
        self.model_file.write('\tclass Regularizer():\n')
        self.model_file.write('\t\tdef __init__(self,resetGate,updateGate,stateCandidate):\n')
        self.model_file.write('\t\t\tself.resetGate = tf.keras.regularizers.get(resetGate)\n')
        self.model_file.write('\t\t\tself.updateGate = tf.keras.regularizers.get(updateGate)\n')
        self.model_file.write('\t\t\tself.stateCandidate = tf.keras.regularizers.get(stateCandidate)\n\n')          
        #init
        self.model_file.write('\t\t\n')
        self.model_file.write('\tdef __init__(self,units,activation = Activation(\'sigmoid\',\'sigmoid\',\'tanh\'),kernelInitializer = Initializer(\'glorot_uniform\',\'glorot_uniform\',\'glorot_uniform\'),\n')
        self.model_file.write('\t\t\trecurrentInitializer = Initializer(\'orthogonal\',\'orthogonal\',\'orthogonal\'),biasInitializer = Initializer(\'zeros\',\'zeros\',\'zeros\'),kernelRegularizer = Regularizer(None,None,None),\n')
        self.model_file.write('\t\t\trecurrentRegularizer = Regularizer(None,None,None),biasRegularizer = Regularizer(None,None,None,),useBias = True,dropout = 0.,recurrentDropout = 0.,\n')
        self.model_file.write('\t\tstateClip = None,statefull = False,goBackwards = False,returnSequenceLen = 1,returnState = False,**kwargs):\n')
        self.model_file.write('\t\tsuper(GRU, self).__init__(**kwargs)\n')
        self.model_file.write('\t\tself.units = units\n')
        self.model_file.write('\t\tself.activation =  activation\n')
        self.model_file.write('\t\tself.kernelInitializer = kernelInitializer\n')
        self.model_file.write('\t\tself.recurrentInitializer = recurrentInitializer\n')
        self.model_file.write('\t\tself.biasInitializer = biasInitializer\n')
        self.model_file.write('\t\tself.kernelRegularizer = kernelRegularizer\n')
        self.model_file.write('\t\tself.recurrentRegularizer =recurrentRegularizer\n')
        self.model_file.write('\t\tself.biasRegularizer = biasRegularizer\n')
        self.model_file.write('\t\tself.useBias = useBias\n')
        self.model_file.write('\t\tself.dropout = dropout\n')
        self.model_file.write('\t\tself.recurrentDropout = recurrentDropout\n')
        self.model_file.write('\t\tself.stateClip = stateClip\n')
        self.model_file.write('\t\tself.statefull = statefull\n')
        self.model_file.write('\t\tself.goBackwards = goBackwards\n')
        self.model_file.write('\t\tself.returnSequenceLen = returnSequenceLen\n')
        self.model_file.write('\t\tself.returnState = returnState\n')
        self.model_file.write('\t\tself._initialState = None\n\n')
        #build
        self.model_file.write('\tdef build(self, input_shape):\n')
        self.model_file.write('\t\tinput_dim = int(input_shape[-1])\n')
        self.model_file.write('\t\tself.resetGateKernel = self.add_weight(shape=(input_dim, self.units),name=\'resetGateKernel\',initializer=self.kernelInitializer.resetGate,regularizer=self.kernelRegularizer.resetGate)\n')
        self.model_file.write('\t\tself.updateGateKernel = self.add_weight(shape=(input_dim, self.units),name=\'updateGateKernel\',initializer=self.kernelInitializer.updateGate,regularizer=self.kernelRegularizer.updateGate)\n')
        self.model_file.write('\t\tself.stateCandidateKernel = self.add_weight(shape=(input_dim, self.units),name=\'stateCandidateKernel\',initializer=self.kernelInitializer.stateCandidate,regularizer=self.kernelRegularizer.stateCandidate)\n')
        self.model_file.write('\t\tself.resetGateRecurrentKernel = self.add_weight(shape=(self.units, self.units),name=\'resetGateRecurrentKernel\',initializer=self.recurrentInitializer.resetGate,regularizer=self.recurrentRegularizer.resetGate)\n')
        self.model_file.write('\t\tself.updateGateRecurrentKernel = self.add_weight(shape=(self.units, self.units),name=\'updateGateRecurrentKernel\',initializer=self.recurrentInitializer.updateGate,regularizer=self.recurrentRegularizer.updateGate)\n')
        self.model_file.write('\t\tself.stateCandidateRecurrentKernel = self.add_weight(shape=(self.units, self.units),name=\'stateCandidateRecurrentKernel\',initializer=self.recurrentInitializer.stateCandidate,regularizer=self.recurrentRegularizer.stateCandidate)\n')
        self.model_file.write('\t\tif self.useBias:\n')
        self.model_file.write('\t\t\tself.resetGateBias = self.add_weight(shape=(self.units,),name=\'resetGateBias\',initializer=self.biasInitializer.resetGate,regularizer=self.biasRegularizer.resetGate)\n')
        self.model_file.write('\t\t\tself.updateGateBias = self.add_weight(shape=(self.units,),name=\'updateGateBias\',initializer=self.biasInitializer.updateGate,regularizer=self.biasRegularizer.updateGate)\n')
        self.model_file.write('\t\t\tself.stateCandidateBias = self.add_weight(shape=(self.units,),name=\'stateCandidate\',initializer=self.biasInitializer.stateCandidate,regularizer=self.biasRegularizer.stateCandidate)\n')
        self.model_file.write('\t\tself.built = True\n\n')
        #_stepFunction
        self.model_file.write('\tdef _stepFunction(self,inputs,state,training = None):\n') 
        self.model_file.write('\t\tif 0 < self.dropout < 1 : inputs = tf.keras.backend.in_train_phase(tf.nn.dropout(inputs,self.dropout), inputs, training=training)\n')  
        self.model_file.write('\t\tif  0 < self.recurrentDropout < 1 : state =   tf.keras.backend.in_train_phase(tf.nn.dropout(state,self.recurrentDropout), state, training=training)\n')  
        self.model_file.write('\t\tif self.useBias:\n')   
        self.model_file.write('\t\t\tresetGateOutput = self.activation.resetGate(tf.matmul(state,self.resetGateRecurrentKernel) + tf.matmul(inputs,self.resetGateKernel)  + self.resetGateBias)\n')
        self.model_file.write('\t\t\tupdateGateOutput = self.activation.updateGate(tf.matmul(state,self.updateGateRecurrentKernel) + tf.matmul(inputs,self.updateGateKernel) + self.updateGateBias)\n')
        self.model_file.write('\t\t\tstateCandidate = self.activation.stateCandidate(tf.matmul(tf.multiply(resetGateOutput,state),self.stateCandidateRecurrentKernel) + tf.matmul(inputs,self.stateCandidateKernel) + self.stateCandidateBias)\n')
        self.model_file.write('\t\telse:\n')
        self.model_file.write('\t\t\tresetGateOutput = self.activation.resetGate(tf.matmul(state,self.resetGateRecurrentKernel) + tf.matmul(inputs,self.resetGateKernel))\n')
        self.model_file.write('\t\t\tupdateGateOutput = self.activation.updateGate(tf.matmul(state,self.updateGateRecurrentKernel) + tf.matmul(inputs,self.updateGateKernel))\n')
        self.model_file.write('\t\t\tstateCandidate = self.activation.stateCandidate(tf.matmul(tf.multiply(resetGateOutput,state),self.stateCandidateRecurrentKernel) + tf.matmul(inputs,self.stateCandidateKernel))\n')
        self.model_file.write('\t\toutput = tf.multiply(updateGateOutput,state) + tf.multiply(1-updateGateOutput,stateCandidate)\n')
        self.model_file.write('\t\tif self.stateClip is not None:\n')
        self.model_file.write('\t\t\tnewState = tf.clip_by_value(output,-1*self.stateClip,self.stateClip)\n') 
        self.model_file.write('\t\t\treturn output,newState\n')
        self.model_file.write('\t\treturn output,output\n\n') 
        #_switchBatchTime 
        self.model_file.write('\tdef _switchBatchTime(self,inputs):\n')
        self.model_file.write('\t\tperm = list(range(inputs.get_shape().rank))\n') 
        self.model_file.write('\t\tperm[0:2] = (1,0)\n') 
        self.model_file.write('\t\treturn tf.transpose(inputs,perm)\n\n') 
        #_getInitialState
        self.model_file.write('\tdef _getInitialState(self,inputs):\n') 
        self.model_file.write('\t\tinitialState = tf.zeros_like(inputs)\n')          
        self.model_file.write('\t\tinitialState = initialState[:,0,0:1]\n') 
        self.model_file.write('\t\tmultiples = tf.convert_to_tensor([1,self.units])\n') 
        self.model_file.write('\t\tinitialState = tf.tile(initialState,multiples)\n') 
        self.model_file.write('\t\treturn initialState\n\n') 
        #call
        self.model_file.write('\tdef call(self, inputs ,state = None):\n') 
        self.model_file.write('\t\tbatchSize,timeSteps,_ = inputs.shape.as_list()\n') 
        self.model_file.write('\t\ttimeStepsTensor = tf.shape(inputs)[1]\n') 
        self.model_file.write('\t\tinitialState = state if state is not None else self._initialState\n') 
        self.model_file.write('\t\tif initialState is None: initialState = self._getInitialState(inputs)\n') 
        self.model_file.write('\t\tinputs = self._switchBatchTime(inputs)\n') 
        self.model_file.write('\t\tinitialTime = tf.zeros_like(timeStepsTensor,name=\'time\')\n') 
        self.model_file.write('\t\tinputsTensorArray = tf.TensorArray(dtype=inputs.dtype,size=timeStepsTensor)\n') 
        self.model_file.write('\t\tinputsTensorArray = inputsTensorArray.unstack(tf.reverse(inputs,tf.zeros(1, tf.int32))) if self.goBackwards else inputsTensorArray.unstack(inputs)\n') 
        self.model_file.write('\t\tinitialOutputTensorArray = tf.TensorArray(dtype=inputs.dtype,size=timeStepsTensor)\n\n') 
        self.model_file.write('\t\tdef _cond(time,outputTensorArray,state):\n') 
        self.model_file.write('\t\t\treturn time<timeStepsTensor\n\n') 
        self.model_file.write('\t\tdef _body(time,outputTensorArray,state):\n') 
        self.model_file.write('\t\t\tcurrentInput = inputsTensorArray.read(time)\n')
        self.model_file.write('\t\t\toutput,newState = self._stepFunction(currentInput,state)\n') 
        self.model_file.write('\t\t\toutputTensorArray = outputTensorArray.write(time,output)\n')
        self.model_file.write('\t\t\treturn (time+1,outputTensorArray,newState)\n\n') 
        self.model_file.write('\t\tfinalOutputs = tf.while_loop(cond = _cond,body=_body,loop_vars=(initialTime,initialOutputTensorArray,initialState))\n')
        self.model_file.write('\t\tnewState = finalOutputs[-1]\n') 
        self.model_file.write('\t\tfinalOutputTensorArray = finalOutputs[1]\n')
        self.model_file.write('\t\tfinalOutput = finalOutputTensorArray.stack()\n') 
        self.model_file.write('\t\tfinalOutput.set_shape([timeSteps,batchSize,self.units])\n')
        self.model_file.write('\t\tif self.statefull: self._initialState = newState\n')
        self.model_file.write('\t\tif self.returnSequenceLen is 1: finalOutput = finalOutput[-1]\n')
        self.model_file.write('\t\telif self.returnSequenceLen is not None:\n')
        self.model_file.write('\t\t\tfinalOutput = finalOutput[-self.returnSequenceLen:]\n')
        self.model_file.write('\t\t\tfinalOutput = self._switchBatchTime(finalOutput)\n')
        self.model_file.write('\t\telse: finalOutput = self._switchBatchTime(finalOutput)\n')
        self.model_file.write('\t\tif self.returnState: return [finalOutput,newState]\n')
        self.model_file.write('\t\telse: return finalOutput\n\n')        
        #compute_output_shape
        self.model_file.write('\tdef compute_output_shape(self,input_shape):\n')
        self.model_file.write('\t\tbatchSize,timeSteps,_ = input_shape\n')
        self.model_file.write('\t\tif self.returnSequenceLen in [1,None]:\n')
        self.model_file.write('\t\t\tif self.returnSequenceLen == 1: outputShape = tf.TensorShape([batchSize,self.units])\n')
        self.model_file.write('\t\t\telse: outputShape = tf.TensorShape([batchSize,timeSteps,self.units])\n')
        self.model_file.write('\t\telse:\n')
        self.model_file.write('\t\t\tif timeSteps is None or timeSteps < self.returnSequenceLen : raise ValueError(\'returnSequenceLen not match to timeSteps\')\n')
        self.model_file.write('\t\t\telse: outputShape = tf.TensorShape([batchSize,self.returnSequenceLen,self.units])\n')
        self.model_file.write('\t\tif not self.returnState : return outputShape\n')
        self.model_file.write('\t\tstateShape = tf.TensorShape([batchSize,self.units])\n')
        self.model_file.write('\t\treturn [outputShape,stateShape]\n\n')
        #get_config
        self.model_file.write('\tdef get_config(self):\n')
        self.model_file.write('\t\tconfig = {\'units\': self.units,\'activation\' : self.activation,\'kernelInitializer\' : self.kernelInitializer,\'recurrentInitializer\' : self.recurrentInitializer,\n')
        self.model_file.write('\t\t\t\t\t\'biasInitializer\' : self.biasInitializer,\'kernelRegularizer\' : self.kernelRegularizer,\'recurrentRegularizer\' : self.recurrentRegularizer,\'biasRegularizer\' : self.biasRegularizer,\n')
        self.model_file.write('\t\t\t\t\t\'useBias\': self.useBias,\'dropout\': self.dropout,\'recurrentDropout\': self.recurrentDropout,\'stateClip\' : self.stateClip,\'statefull\' : self.statefull,\n')
        self.model_file.write('\t\t\t\t\t\'goBackwards\' : self.goBackwards,\'returnSequenceLen\':self.returnSequenceLen,\'returnState\':self.returnState,}\n')
        self.model_file.write('\t\tbaseConfig = super(GRU, self).get_config()\n')
        self.model_file.write('\t\treturn dict(list(baseConfig.items()) + list(config.items()))\n')

    def write_LSTM(self):
        '''
			write LSTM class to keras_model.py.   --- UPDATED (CYK) 20190925

        '''
        self.model_file.write('\n\n')
        self.model_file.write('class LSTM(tf.keras.layers.Layer):\n')
        #Activation
        self.model_file.write('\tclass Activation():\n')
        self.model_file.write('\t\tdef __init__(self,inputGate,forgetGate,outputGate,memory,shadowGate):\n')
        self.model_file.write('\t\t\tself.inputGate = tf.keras.activations.get(inputGate)\n')
        self.model_file.write('\t\t\tself.forgetGate = tf.keras.activations.get(forgetGate)\n')
        self.model_file.write('\t\t\tself.outputGate = tf.keras.activations.get(outputGate)\n')
        self.model_file.write('\t\t\tself.memory = tf.keras.activations.get(memory)\n')
        self.model_file.write('\t\t\tself.shadowGate = tf.keras.activations.get(shadowGate)\n\n')
        #Initializer
        self.model_file.write('\tclass Initializer():\n')
        self.model_file.write('\t\tdef __init__(self,inputGate,forgetGate,outputGate,memory):\n')
        self.model_file.write('\t\t\tself.inputGate = tf.keras.initializers.get(inputGate)\n')
        self.model_file.write('\t\t\tself.forgetGate = tf.keras.initializers.get(forgetGate)\n')
        self.model_file.write('\t\t\tself.outputGate = tf.keras.initializers.get(outputGate)\n')
        self.model_file.write('\t\t\tself.memory = tf.keras.initializers.get(memory)\n\n')
        #Regularizer
        self.model_file.write('\tclass Regularizer():\n')
        self.model_file.write('\t\tdef __init__(self,inputGate,forgetGate,outputGate,memory):\n')
        self.model_file.write('\t\t\tself.inputGate = tf.keras.regularizers.get(inputGate)\n')
        self.model_file.write('\t\t\tself.forgetGate = tf.keras.regularizers.get(forgetGate)\n')
        self.model_file.write('\t\t\tself.outputGate = tf.keras.regularizers.get(outputGate)\n')
        self.model_file.write('\t\t\tself.memory = tf.keras.regularizers.get(memory)\n\n')            
        #init
        self.model_file.write('\t\t\n')
        self.model_file.write('\tdef __init__(self,units,activation = Activation(\'sigmoid\',\'sigmoid\',\'sigmoid\',\'tanh\',\'tanh\'),kernelInitializer = Initializer(\'glorot_uniform\',\'glorot_uniform\',\'glorot_uniform\',\'glorot_uniform\'),\n')
        self.model_file.write('\t\t\trecurrentInitializer = Initializer(\'orthogonal\',\'orthogonal\',\'orthogonal\',\'orthogonal\'),peepholeInitializer = Initializer(\'orthogonal\',\'orthogonal\',\'orthogonal\',None),\n')
        self.model_file.write('\t\t\tbiasInitializer = Initializer(\'zeros\',\'zeros\',\'zeros\',\'zeros\'),kernelRegularizer = Regularizer(None,None,None,None),recurrentRegularizer = Regularizer(None,None,None,None),\n')
        self.model_file.write('\t\t\tpeepholeRegularizer = Regularizer(None,None,None,None),biasRegularizer = Regularizer(None,None,None,None),useBias = True,usePeephole = False,dropout = 0.,recurrentDropout = 0.,\n')
        self.model_file.write('\t\t\tmemoryCellClip = None,shadowStateClip = None,statefull = False,goBackwards = False,returnSequenceLen = 1,returnState = False,**kwargs):\n')
        self.model_file.write('\t\tsuper(LSTM, self).__init__(**kwargs)\n')
        self.model_file.write('\t\tself.units = units\n')
        self.model_file.write('\t\tself.activation =  activation\n')
        self.model_file.write('\t\tself.kernelInitializer = kernelInitializer\n')
        self.model_file.write('\t\tself.recurrentInitializer = recurrentInitializer\n')
        self.model_file.write('\t\tself.peepholeInitializer = peepholeInitializer\n')
        self.model_file.write('\t\tself.biasInitializer = biasInitializer\n')
        self.model_file.write('\t\tself.kernelRegularizer = kernelRegularizer\n')
        self.model_file.write('\t\tself.recurrentRegularizer =recurrentRegularizer\n')
        self.model_file.write('\t\tself.peepholeRegularizer = peepholeRegularizer\n')
        self.model_file.write('\t\tself.biasRegularizer = biasRegularizer\n')
        self.model_file.write('\t\tself.useBias = useBias\n')
        self.model_file.write('\t\tself.usePeephole = usePeephole\n')
        self.model_file.write('\t\tself.dropout = dropout\n')
        self.model_file.write('\t\tself.recurrentDropout = recurrentDropout\n')
        self.model_file.write('\t\tself.memoryCellClip = memoryCellClip\n')
        self.model_file.write('\t\tself.shadowStateClip = shadowStateClip\n')
        self.model_file.write('\t\tself.statefull = statefull\n')
        self.model_file.write('\t\tself.goBackwards = goBackwards\n')
        self.model_file.write('\t\tself.returnSequenceLen = returnSequenceLen\n')
        self.model_file.write('\t\tself.returnState = returnState\n')
        self.model_file.write('\t\tself._initialMemoryCell = None\n')
        self.model_file.write('\t\tself._initialShadowState = None\n\n')
        #build
        self.model_file.write('\tdef build(self, input_shape):\n')
        self.model_file.write('\t\tinput_dim = int(input_shape[-1])\n')
        self.model_file.write('\t\tself.inputGateKernel = self.add_weight(shape=(input_dim, self.units),name=\'inputGateKernel\',initializer=self.kernelInitializer.inputGate,regularizer=self.kernelRegularizer.inputGate)\n')
        self.model_file.write('\t\tself.forgetGateKernel = self.add_weight(shape=(input_dim, self.units),name=\'forgetGateKernel\',initializer=self.kernelInitializer.forgetGate,regularizer=self.kernelRegularizer.forgetGate)\n')
        self.model_file.write('\t\tself.outputGateKernel = self.add_weight(shape=(input_dim, self.units),name=\'outputGateKernel\',initializer=self.kernelInitializer.outputGate,regularizer=self.kernelRegularizer.outputGate)\n')
        self.model_file.write('\t\tself.memoryKernel = self.add_weight(shape=(input_dim, self.units),name=\'memoryKernel\',initializer=self.kernelInitializer.memory,regularizer=self.kernelRegularizer.memory)\n')
        self.model_file.write('\t\tself.inputGateRecurrentKernel = self.add_weight(shape=(self.units, self.units),name=\'inputGateRecurrentKernel\',initializer=self.recurrentInitializer.inputGate,regularizer=self.recurrentRegularizer.inputGate)\n')
        self.model_file.write('\t\tself.forgetGateRecurrentKernel = self.add_weight(shape=(self.units, self.units),name=\'forgetGateRecurrentKernel\',initializer=self.recurrentInitializer.forgetGate,regularizer=self.recurrentRegularizer.forgetGate)\n')
        self.model_file.write('\t\tself.outputGateRecurrentKernel = self.add_weight(shape=(self.units, self.units),name=\'outputGateRecurrentKernel\',initializer=self.recurrentInitializer.outputGate,regularizer=self.recurrentRegularizer.outputGate)\n')
        self.model_file.write('\t\tself.memoryRecurrentKernel = self.add_weight(shape=(self.units, self.units),name=\'memoryRecurrentKernel\',initializer=self.recurrentInitializer.memory,regularizer=self.recurrentRegularizer.memory)\n')
        self.model_file.write('\t\tif self.usePeephole:\n')
        self.model_file.write('\t\t\tself.inputGatePeepholeKernel = self.add_weight(shape=(self.units, self.units),name=\'inputGatePeepholeKernel\',initializer=self.peepholeInitializer.inputGate,regularizer=self.peepholeRegularizer.inputGate)\n')
        self.model_file.write('\t\t\tself.forgetGatePeepholeKernel = self.add_weight(shape=(self.units, self.units),name=\'forgetGatePeepholeKernel\',initializer=self.peepholeInitializer.forgetGate,regularizer=self.peepholeRegularizer.forgetGate)\n')
        self.model_file.write('\t\t\tself.outputGatePeepholeKernel = self.add_weight(shape=(self.units, self.units),name=\'outputGatePeepholeKernel\',initializer=self.peepholeInitializer.outputGate,regularizer=self.peepholeRegularizer.outputGate)\n')
        self.model_file.write('\t\tif self.useBias:\n')
        self.model_file.write('\t\t\tself.inputGateBias = self.add_weight(shape=(self.units,),name=\'inputGateBias\',initializer=self.biasInitializer.inputGate,regularizer=self.biasRegularizer.inputGate)\n')
        self.model_file.write('\t\t\tself.forgetGateBias = self.add_weight(shape=(self.units,),name=\'forgetGateBias\',initializer=self.biasInitializer.forgetGate,regularizer=self.biasRegularizer.forgetGate)\n')
        self.model_file.write('\t\t\tself.outputGateBias = self.add_weight(shape=(self.units,),name=\'outputGateBias\',initializer=self.biasInitializer.outputGate,regularizer=self.biasRegularizer.outputGate)\n')
        self.model_file.write('\t\t\tself.memoryBias = self.add_weight(shape=(self.units,),name=\'memoryBias\',initializer=self.biasInitializer.memory,regularizer=self.biasRegularizer.memory)\n')
        self.model_file.write('\t\tself.built = True\n\n')
        #_stepFunction
        self.model_file.write('\tdef _stepFunction(self,inputs,states,training = None):\n')
        self.model_file.write('\t\tmemoryCell,shadowState = states\n')  
        self.model_file.write('\t\tif 0 < self.dropout < 1 : inputs = tf.keras.backend.in_train_phase(tf.nn.dropout(inputs,self.dropout), inputs, training=training)\n')  
        self.model_file.write('\t\tif  0 < self.recurrentDropout < 1 : shadowState =   tf.keras.backend.in_train_phase(tf.nn.dropout(shadowState,self.recurrentDropout), shadowState, training=training)\n')  
        self.model_file.write('\t\tif self.usePeephole:\n')  
        self.model_file.write('\t\t\tif self.useBias:\n')
        self.model_file.write('\t\t\t\tinputGateOutput = self.activation.inputGate(tf.matmul(shadowState,self.inputGateRecurrentKernel) + tf.matmul(inputs,self.inputGateKernel) + tf.matmul(memoryCell,self.inputGatePeepholeKernel)  + self.inputGateBias)\n') 
        self.model_file.write('\t\t\t\tforgetGateOutput = self.activation.forgetGate(tf.matmul(shadowState,self.forgetGateRecurrentKernel) + tf.matmul(inputs,self.forgetGateKernel) + tf.matmul(memoryCell,self.forgetGatePeepholeKernel)  + self.forgetGateBias)\n')
        self.model_file.write('\t\t\t\tmemoryCellCandidate = self.activation.memory(tf.matmul(shadowState,self.memoryRecurrentKernel) + tf.matmul(inputs,self.memoryKernel) + self.memoryBias)\n')
        self.model_file.write('\t\t\t\tnewMemoryCell = tf.multiply(forgetGateOutput,memoryCell) + tf.multiply(inputGateOutput,memoryCellCandidate)\n')
        self.model_file.write('\t\t\t\tif self.memoryCellClip is not None: newMemoryCell = tf.clip_by_value(newMemoryCell,-1*self.memoryCellClip,self.memoryCellClip)\n')
        self.model_file.write('\t\t\t\toutputGateOutput = self.activation.outputGate(tf.matmul(shadowState,self.outputGateRecurrentKernel) + tf.matmul(inputs,self.outputGateKernel) + tf.matmul(newMemoryCell,self.outputGatePeepholeKernel) + self.outputGateBias)\n') 
        self.model_file.write('\t\t\telse:\n')
        self.model_file.write('\t\t\t\tinputGateOutput = self.activation.inputGate(tf.matmul(shadowState,self.inputGateRecurrentKernel) + tf.matmul(inputs,self.inputGateKernel) + tf.matmul(memoryCell,self.inputGatePeepholeKernel))\n')
        self.model_file.write('\t\t\t\tforgetGateOutput = self.activation.forgetGate(tf.matmul(shadowState,self.forgetGateRecurrentKernel) + tf.matmul(inputs,self.forgetGateKernel) + tf.matmul(memoryCell,self.forgetGatePeepholeKernel))\n')
        self.model_file.write('\t\t\t\tmemoryCellCandidate = self.activation.memory(tf.matmul(shadowState,self.memoryRecurrentKernel) + tf.matmul(inputs,self.memoryKernel))\n')
        self.model_file.write('\t\t\t\tnewMemoryCell = tf.multiply(forgetGateOutput,memoryCell) + tf.multiply(inputGateOutput,memoryCellCandidate)\n')
        self.model_file.write('\t\t\t\tif self.memoryCellClip is not None: newMemoryCell = tf.clip_by_value(newMemoryCell,-1*self.memoryCellClip,self.memoryCellClip)\n')
        self.model_file.write('\t\t\t\toutputGateOutput = self.activation.outputGate(tf.matmul(shadowState,self.outputGateRecurrentKernel) + tf.matmul(inputs,self.outputGateKernel) + tf.matmul(newMemoryCell,self.outputGatePeepholeKernel))\n')
        self.model_file.write('\t\telse:\n')  
        self.model_file.write('\t\t\tif self.useBias:\n')
        self.model_file.write('\t\t\t\tinputGateOutput = self.activation.inputGate(tf.matmul(shadowState,self.inputGateRecurrentKernel) + tf.matmul(inputs,self.inputGateKernel)  + self.inputGateBias)\n')
        self.model_file.write('\t\t\t\tforgetGateOutput = self.activation.forgetGate(tf.matmul(shadowState,self.forgetGateRecurrentKernel) + tf.matmul(inputs,self.forgetGateKernel) + self.forgetGateBias)\n')
        self.model_file.write('\t\t\t\toutputGateOutput = self.activation.outputGate(tf.matmul(shadowState,self.outputGateRecurrentKernel) + tf.matmul(inputs,self.outputGateKernel) + self.outputGateBias)\n')
        self.model_file.write('\t\t\t\tmemoryCellCandidate = self.activation.memory(tf.matmul(shadowState,self.memoryRecurrentKernel) + tf.matmul(inputs,self.memoryKernel) + self.memoryBias)\n')
        self.model_file.write('\t\t\t\tnewMemoryCell = tf.multiply(forgetGateOutput,memoryCell) + tf.multiply(inputGateOutput,memoryCellCandidate)\n')
        self.model_file.write('\t\t\telse:\n')
        self.model_file.write('\t\t\t\tinputGateOutput = self.activation.inputGate(tf.matmul(shadowState,self.inputGateRecurrentKernel) + tf.matmul(inputs,self.inputGateKernel))\n')
        self.model_file.write('\t\t\t\tforgetGateOutput = self.activation.forgetGate(tf.matmul(shadowState,self.forgetGateRecurrentKernel) + tf.matmul(inputs,self.forgetGateKernel))\n')
        self.model_file.write('\t\t\t\toutputGateOutput = self.activation.outputGate(tf.matmul(shadowState,self.outputGateRecurrentKernel) + tf.matmul(inputs,self.outputGateKernel))\n')
        self.model_file.write('\t\t\t\tmemoryCellCandidate = self.activation.memory(tf.matmul(shadowState,self.memoryRecurrentKernel) + tf.matmul(inputs,self.memoryKernel)) \n')
        self.model_file.write('\t\t\t\tnewMemoryCell = tf.multiply(forgetGateOutput,memoryCell) + tf.multiply(inputGateOutput,memoryCellCandidate)\n')
        self.model_file.write('\t\t\tif self.memoryCellClip is not None: newMemoryCell = tf.clip_by_value(newMemoryCell,-1*self.memoryCellClip,self.memoryCellClip)\n')
        self.model_file.write('\t\tnewShadowState = tf.multiply(outputGateOutput,self.activation.shadowGate(newMemoryCell))\n') 
        self.model_file.write('\t\toutput = newShadowState\n')
        self.model_file.write('\t\tif self.shadowStateClip is not None: newShadowState = tf.clip_by_value(newShadowState,-1*self.shadowStateClip,self.shadowStateClip)\n') 
        self.model_file.write('\t\treturn output,(newMemoryCell,newShadowState)\n\n') 
        #_switchBatchTime 
        self.model_file.write('\tdef _switchBatchTime(self,inputs):\n')
        self.model_file.write('\t\tperm = list(range(inputs.get_shape().rank))\n') 
        self.model_file.write('\t\tperm[0:2] = (1,0)\n') 
        self.model_file.write('\t\treturn tf.transpose(inputs,perm)\n\n') 
        #_getInitialState
        self.model_file.write('\tdef _getInitialState(self,inputs):\n') 
        self.model_file.write('\t\tinitialState = tf.zeros_like(inputs)\n')          
        self.model_file.write('\t\tinitialState = initialState[:,0,0:1]\n') 
        self.model_file.write('\t\tmultiples = tf.convert_to_tensor([1,self.units])\n') 
        self.model_file.write('\t\tinitialState = tf.tile(initialState,multiples)\n') 
        self.model_file.write('\t\treturn initialState\n\n') 
        #call
        self.model_file.write('\tdef call(self, inputs ,states = None):\n') 
        self.model_file.write('\t\tbatchSize,timeSteps,_ = inputs.shape.as_list()\n') 
        self.model_file.write('\t\ttimeStepsTensor = tf.shape(inputs)[1]\n') 
        self.model_file.write('\t\tinitialMemoryCell,initialShadowState = states if states is not None else self._initialMemoryCell,self._initialShadowState\n') 
        self.model_file.write('\t\tif initialMemoryCell is None: initialMemoryCell = self._getInitialState(inputs)\n') 
        self.model_file.write('\t\tif initialShadowState is None: initialShadowState = self._getInitialState(inputs)\n') 
        self.model_file.write('\t\tinputs = self._switchBatchTime(inputs)\n') 
        self.model_file.write('\t\tinitialTime = tf.zeros_like(timeStepsTensor,name=\'time\')\n') 
        self.model_file.write('\t\tinputsTensorArray = tf.TensorArray(dtype=inputs.dtype,size=timeStepsTensor)\n') 
        self.model_file.write('\t\tinputsTensorArray = inputsTensorArray.unstack(tf.reverse(inputs,tf.zeros(1, tf.int32))) if self.goBackwards else inputsTensorArray.unstack(inputs)\n') 
        self.model_file.write('\t\tinitialOutputTensorArray = tf.TensorArray(dtype=inputs.dtype,size=timeStepsTensor)\n\n') 
        self.model_file.write('\t\tdef _cond(time,outputTensorArray,memoryCell,shadowState):\n') 
        self.model_file.write('\t\t\treturn time<timeStepsTensor\n\n') 
        self.model_file.write('\t\tdef _body(time,outputTensorArray,memoryCell,shadowState):\n') 
        self.model_file.write('\t\t\tcurrentInput = inputsTensorArray.read(time)\n')
        self.model_file.write('\t\t\toutput,(newMemoryCell,newShadowState) = self._stepFunction(currentInput,(memoryCell,shadowState))\n') 
        self.model_file.write('\t\t\toutputTensorArray = outputTensorArray.write(time,output)\n')
        self.model_file.write('\t\t\treturn (time+1,outputTensorArray,newMemoryCell,newShadowState)\n\n') 
        self.model_file.write('\t\tfinalOutputs = tf.while_loop(cond = _cond,body=_body,loop_vars=(initialTime,initialOutputTensorArray,initialMemoryCell,initialShadowState))\n')
        self.model_file.write('\t\tnewMemoryCell,newShadowState = finalOutputs[-2],finalOutputs[-1]\n') 
        self.model_file.write('\t\tfinalOutputTensorArray = finalOutputs[1]\n')
        self.model_file.write('\t\tfinalOutput = finalOutputTensorArray.stack()\n') 
        self.model_file.write('\t\tfinalOutput.set_shape([timeSteps,batchSize,self.units])\n')
        self.model_file.write('\t\tif self.statefull: self._initialMemoryCell,self._initialShadowState = newMemoryCell,newShadowState\n')
        self.model_file.write('\t\tif self.returnSequenceLen is 1: finalOutput = finalOutput[-1]\n')
        self.model_file.write('\t\telif self.returnSequenceLen is not None:\n')
        self.model_file.write('\t\t\tfinalOutput = finalOutput[-self.returnSequenceLen:]\n')
        self.model_file.write('\t\t\tfinalOutput = self._switchBatchTime(finalOutput)\n')
        self.model_file.write('\t\telse: finalOutput = self._switchBatchTime(finalOutput)\n')
        self.model_file.write('\t\tif self.returnState: return [finalOutput,newMemoryCell,newShadowState]\n')
        self.model_file.write('\t\telse: return finalOutput\n\n')        
        #compute_output_shape
        self.model_file.write('\tdef compute_output_shape(self,input_shape):\n')
        self.model_file.write('\t\tbatchSize,timeSteps,_ = input_shape\n')
        self.model_file.write('\t\tif self.returnSequenceLen in [1,None]:\n')
        self.model_file.write('\t\t\tif self.returnSequenceLen == 1: outputShape = tf.TensorShape([batchSize,self.units])\n')
        self.model_file.write('\t\t\telse: outputShape = tf.TensorShape([batchSize,timeSteps,self.units])\n')
        self.model_file.write('\t\telse:\n')
        self.model_file.write('\t\t\tif timeSteps is None or timeSteps < self.returnSequenceLen : raise ValueError(\'returnSequenceLen not match to timeSteps\')\n')
        self.model_file.write('\t\t\telse: outputShape = tf.TensorShape([batchSize,self.returnSequenceLen,self.units])\n')
        self.model_file.write('\t\tif not self.returnState : return outputShape\n')
        self.model_file.write('\t\tmemoryCellShape = tf.TensorShape([batchSize,self.units])\n')
        self.model_file.write('\t\tshadowStateShape = tf.TensorShape([batchSize,self.units])\n')
        self.model_file.write('\t\treturn [outputShape,(memoryCellShape,shadowStateShape)]\n\n')
        #get_config
        self.model_file.write('\tdef get_config(self):\n')
        self.model_file.write('\t\tconfig = {\'units\': self.units,\'activation\' : self.activation,\'kernelInitializer\' : self.kernelInitializer,\'recurrentInitializer\' : self.recurrentInitializer,\n')
        self.model_file.write('\t\t\t\t\t\'peepholeInitializer\' : self.peepholeInitializer,\'biasInitializer\' : self.biasInitializer,\'kernelRegularizer\' : self.kernelRegularizer,\n')
        self.model_file.write('\t\t\t\t\t\'recurrentRegularizer\' : self.recurrentRegularizer,\'peepholeRegularizer\' : self.peepholeRegularizer,\'biasRegularizer\' : self.biasRegularizer,\n')
        self.model_file.write('\t\t\t\t\t\'useBias\': self.useBias,\'usePeephole\' : self.usePeephole,\'dropout\': self.dropout,\'recurrentDropout\': self.recurrentDropout,\'memoryCellClip\' : self.memoryCellClip,\n')
        self.model_file.write('\t\t\t\t\t\'shadowStateClip\' : self.shadowStateClip,\'statefull\' : self.statefull,\'goBackwards\' : self.goBackwards,\'returnSequenceLen\':self.returnSequenceLen,\'returnState\':self.returnState,}\n')
        self.model_file.write('\t\tbaseConfig = super(LSTM, self).get_config()\n')
        self.model_file.write('\t\treturn dict(list(baseConfig.items()) + list(config.items()))\n')

    def write_BiRNN(self):
        '''
			write BiRNN class to keras_model.py.   --- UPDATED (CYK) 20191115

        '''
        self.model_file.write('\n\n')
        self.model_file.write('class BiRNN(tf.keras.layers.Layer):\n')
        #init
        self.model_file.write('\tdef __init__(self,units,activation = \'tanh\',kernelInitializer = \'glorot_uniform\',recurrentInitializer = \'orthogonal\',biasInitializer = \'zero\',\n')
        self.model_file.write('\t\t\tkernelRegularizer = None,recurrentRegularizer = None,biasRegularizer = None,kernelConstraint = None,recurrentConstraint = None,biasConstraint = None,\n')
        self.model_file.write('\t\t\tuseBias = True,dropout = 0.,recurrentDropout = 0.,stateClip = None,statefull = False,mergeMode = \'concat\',**kwargs):\n')
        self.model_file.write('\t\tsuper(BiRNN, self).__init__(**kwargs)\n')
        self.model_file.write('\t\tself.units = units\n')
        self.model_file.write('\t\tself.activation =  tf.keras.activations.get(activation)\n')
        self.model_file.write('\t\tself.kernelInitializer = tf.keras.initializers.get(kernelInitializer)\n')
        self.model_file.write('\t\tself.recurrentInitializer = tf.keras.initializers.get(recurrentInitializer)\n')
        self.model_file.write('\t\tself.biasInitializer = tf.keras.initializers.get(biasInitializer)\n')
        self.model_file.write('\t\tself.kernelRegularizer = tf.keras.regularizers.get(kernelRegularizer)\n')
        self.model_file.write('\t\tself.recurrentRegularizer =tf.keras.regularizers.get(recurrentRegularizer)\n')
        self.model_file.write('\t\tself.biasRegularizer = tf.keras.regularizers.get(biasRegularizer)\n')
        self.model_file.write('\t\tself.kernelConstraint = tf.keras.constraints.get(kernelConstraint)\n')
        self.model_file.write('\t\tself.recurrentConstraint = tf.keras.constraints.get(recurrentConstraint)\n')
        self.model_file.write('\t\tself.biasConstraint = tf.keras.constraints.get(biasConstraint)\n')
        self.model_file.write('\t\tself.useBias = useBias\n')
        self.model_file.write('\t\tself.dropout = dropout\n')
        self.model_file.write('\t\tself.recurrentDropout = recurrentDropout\n')
        self.model_file.write('\t\tself.stateClip = stateClip\n')
        self.model_file.write('\t\tself.statefull = statefull\n')
        self.model_file.write('\t\tself.mergeMode = mergeMode\n')
        self.model_file.write('\t\tself._initialStateForward = None\n')
        self.model_file.write('\t\tself._initialStateBackward = None\n\n')
        #build
        self.model_file.write('\tdef build(self, input_shape):\n')
        self.model_file.write('\t\tinput_dim = int(input_shape[-1])\n')
        self.model_file.write('\t\tself.kernelForward = self.add_weight(shape=(input_dim,self.units),name=\'kernel\',initializer=self.kernelInitializer,regularizer=self.kernelRegularizer,constraint=self.kernelConstraint)\n')
        self.model_file.write('\t\tself.recurrentKernelForward = self.add_weight(shape=(self.units,self.units),name=\'recurrentKernel\',initializer=self.recurrentInitializer,regularizer=self.recurrentRegularizer,constraint=self.recurrentConstraint)\n')
        self.model_file.write('\t\tself.kernelBackward = self.add_weight(shape=(input_dim,self.units),name=\'kernel\',initializer=self.kernelInitializer,regularizer=self.kernelRegularizer,constraint=self.kernelConstraint)\n')
        self.model_file.write('\t\tself.recurrentKernelBackward = self.add_weight(shape=(self.units,self.units),name=\'recurrentKernel\',initializer=self.recurrentInitializer,regularizer=self.recurrentRegularizer,constraint=self.recurrentConstraint)\n')
        self.model_file.write('\t\tif self.useBias:\n')
        self.model_file.write('\t\t\tself.biasForward = self.add_weight(shape=(self.units,),name=\'bias\',initializer=self.biasInitializer,regularizer=self.biasRegularizer,constraint=self.biasConstraint)\n')
        self.model_file.write('\t\t\tself.biasBackward = self.add_weight(shape=(self.units,),name=\'bias\',initializer=self.biasInitializer,regularizer=self.biasRegularizer,constraint=self.biasConstraint)\n')
        self.model_file.write('\t\tself.built = True\n\n')
        #_stepFunctionForward
        self.model_file.write('\tdef _stepFunctionForward(self,inputs,state,training = None):\n')
        self.model_file.write('\t\tif 0 < self.dropout < 1 : inputs = tf.keras.backend.in_train_phase(tf.nn.dropout(inputs,self.dropout),inputs,training=training)\n')  
        self.model_file.write('\t\tif 0 < self.recurrentDropout < 1 : state = tf.keras.backend.in_train_phase(tf.nn.dropout(state,self.recurrentDropout),state,training=training)\n')  
        self.model_file.write('\t\tif self.useBias: output = self.activation(tf.matmul(state,self.recurrentKernelForward) + tf.matmul(inputs,self.kernelForward) + self.biasForward)\n')  
        self.model_file.write('\t\telse: output = self.activation(tf.matmul(state,self.recurrentKernelForward) + tf.matmul(inputs,self.kernelForward))\n') 
        self.model_file.write('\t\tif self.stateClip is not None:\n')
        self.model_file.write('\t\t\tnewState = tf.clip_by_value(output,-1*self.stateClip,self.stateClip)\n')
        self.model_file.write('\t\t\treturn output,newState\n')
        self.model_file.write('\t\treturn output,output\n\n')
        #_stepFunctionBackward
        self.model_file.write('\tdef _stepFunctionBackward(self,inputs,state,training = None):\n')
        self.model_file.write('\t\tif 0 < self.dropout < 1 : inputs = tf.keras.backend.in_train_phase(tf.nn.dropout(inputs,self.dropout),inputs,training=training)\n')  
        self.model_file.write('\t\tif 0 < self.recurrentDropout < 1 : state = tf.keras.backend.in_train_phase(tf.nn.dropout(state,self.recurrentDropout),state,training=training)\n')  
        self.model_file.write('\t\tif self.useBias: output = self.activation(tf.matmul(state,self.recurrentKernelBackward) + tf.matmul(inputs,self.kernelBackward) + self.biasBackward)\n')  
        self.model_file.write('\t\telse: output = self.activation(tf.matmul(state,self.recurrentKernelBackward) + tf.matmul(inputs,self.kernelBackward))\n') 
        self.model_file.write('\t\tif self.stateClip is not None:\n')
        self.model_file.write('\t\t\tnewState = tf.clip_by_value(output,-1*self.stateClip,self.stateClip)\n')
        self.model_file.write('\t\t\treturn output,newState\n')
        self.model_file.write('\t\treturn output,output\n\n')
        #_switchBatchTime
        self.model_file.write('\tdef _switchBatchTime(self,inputs):\n')
        self.model_file.write('\t\tperm = list(range(inputs.get_shape().rank))\n')
        self.model_file.write('\t\tperm[0:2] = (1,0)\n')
        self.model_file.write('\t\treturn tf.transpose(inputs,perm)\n\n')
        #_getInitialState
        self.model_file.write('\tdef _getInitialState(self,inputs):\n')
        self.model_file.write('\t\tinitialState = tf.zeros_like(inputs)\n')          
        self.model_file.write('\t\tinitialState = initialState[:,0,0:1]\n')
        self.model_file.write('\t\tmultiples = tf.convert_to_tensor([1,self.units])\n')
        self.model_file.write('\t\tinitialState = tf.tile(initialState,multiples)\n')
        self.model_file.write('\t\treturn initialState\n\n')
        #call
        self.model_file.write('\tdef call(self, inputs ,state = None):\n') 
        self.model_file.write('\t\tbatchSize,timeSteps,_ = inputs.shape.as_list()\n') 
        self.model_file.write('\t\ttimeStepsTensor = tf.shape(inputs)[1]\n')
        self.model_file.write('\t\tinitialStateForward,initialStateBackward = self._initialStateForward,self._initialStateBackward\n')
        self.model_file.write('\t\tif initialStateForward is None: initialStateForward = self._getInitialState(inputs)\n')
        self.model_file.write('\t\tif initialStateBackward is None: initialStateBackward = self._getInitialState(inputs)\n')
        self.model_file.write('\t\tinputs = self._switchBatchTime(inputs)\n')
        self.model_file.write('\t\tinitialTime = tf.zeros_like(timeStepsTensor,name=\'time\')\n')
        self.model_file.write('\t\tinputsTensorArrayForward = tf.TensorArray(dtype=inputs.dtype,size=timeStepsTensor)\n')
        self.model_file.write('\t\tinputsTensorArrayBackward = tf.TensorArray(dtype=inputs.dtype,size=timeStepsTensor)\n')
        self.model_file.write('\t\tinputsTensorArrayForward = inputsTensorArrayForward.unstack(inputs)\n')
        self.model_file.write('\t\tinputsTensorArrayBackward = inputsTensorArrayBackward.unstack(tf.reverse(inputs,tf.zeros(1, tf.int32)))\n') 
        self.model_file.write('\t\tinitialOutputTensorArrayForward = tf.TensorArray(dtype=inputs.dtype,size=timeStepsTensor)\n\n')
        self.model_file.write('\t\tinitialOutputTensorArrayBackward = tf.TensorArray(dtype=inputs.dtype,size=timeStepsTensor)\n\n')
        self.model_file.write('\t\tdef _cond(time,outputTensorArray,state):\n')
        self.model_file.write('\t\t\treturn time<timeStepsTensor\n\n')
        self.model_file.write('\t\tdef _bodyForward(time,outputTensorArray,state):\n')
        self.model_file.write('\t\t\tcurrentInput = inputsTensorArrayForward.read(time)\n')
        self.model_file.write('\t\t\toutput,state = self._stepFunctionForward(currentInput,state)\n')
        self.model_file.write('\t\t\toutputTensorArray = outputTensorArray.write(time,output)\n')
        self.model_file.write('\t\t\treturn (time+1,outputTensorArray,state)\n\n')
        self.model_file.write('\t\tdef _bodyBackward(time,outputTensorArray,state):\n')
        self.model_file.write('\t\t\tcurrentInput = inputsTensorArrayBackward.read(time)\n')
        self.model_file.write('\t\t\toutput,state = self._stepFunctionBackward(currentInput,state)\n')
        self.model_file.write('\t\t\toutputTensorArray = outputTensorArray.write(time,output)\n')
        self.model_file.write('\t\t\treturn (time+1,outputTensorArray,state)\n\n')
        self.model_file.write('\t\tfinalOutputsForward = tf.while_loop(cond=_cond,body=_bodyForward,loop_vars=(initialTime,initialOutputTensorArrayForward,initialStateForward))\n')
        self.model_file.write('\t\tfinalOutputsBackward = tf.while_loop(cond=_cond,body=_bodyBackward,loop_vars=(initialTime,initialOutputTensorArrayBackward,initialStateBackward))\n')
        self.model_file.write('\t\tfinalOutputTensorArrayForward = finalOutputsForward[1]\n')
        self.model_file.write('\t\tfinalOutputTensorArrayBackward = finalOutputsBackward[1]\n')
        self.model_file.write('\t\tfinalOutputForward = finalOutputTensorArrayForward.stack()\n') 
        self.model_file.write('\t\tfinalOutputBackward = finalOutputTensorArrayBackward.stack()\n') 
        self.model_file.write('\t\tfinalOutputForward.set_shape([timeSteps,batchSize,self.units])\n')
        self.model_file.write('\t\tfinalOutputBackward.set_shape([timeSteps,batchSize,self.units])\n')
        self.model_file.write('\t\tif self.statefull : self._initialStatForwarde,self._initialStateBackward = finalOutputsForward[-1],finalOutputsBackward[-1]\n')
        self.model_file.write('\t\tfinalOutputForward = self._switchBatchTime(finalOutputForward)\n')
        self.model_file.write('\t\tfinalOutputBackward = self._switchBatchTime(finalOutputBackward)\n')
        self.model_file.write('\t\tif self.mergeMode == \'concat\': mergeOutput = tf.concat([finalOutputForward, finalOutputBackward],-1)\n')
        self.model_file.write('\t\telif self.mergeMode == \'sum\': mergeOutput = finalOutputForward + finalOutputBackward\n')
        self.model_file.write('\t\telif self.mergeMode == \'ave\': mergeOutput = (finalOutputForward + finalOutputBackward) / 2\n')
        self.model_file.write('\t\telif self.mergeMode == \'mul\': mergeOutput = finalOutputForward * finalOutputBackward\n')
        self.model_file.write('\t\telif self.mergeMode is None: mergeOutput = [finalOutputForward, finalOutputBackward]\n')
        self.model_file.write('\t\telse: raise ValueError(\'mergeMode wrong\')\n')
        self.model_file.write('\t\treturn mergeOutput\n\n')
        #compute_output_shape
        self.model_file.write('\tdef compute_output_shape(self,input_shape):\n')
        self.model_file.write('\t\tbatchSize,timeSteps,dim = input_shape\n')
        self.model_file.write('\t\toutputShape = tf.TensorShape([batchSize,timeSteps,self.units*2]) if self.mergeMode == \'concat\' else tf.TensorShape([batchSize,timeSteps,self.units])\n')
        self.model_file.write('\t\tif self.mergeMode is None : return [outputShape,outputShape]\n')
        self.model_file.write('\t\treturn outputShape\n\n')
        #get_config
        self.model_file.write('\tdef get_config(self):\n')
        self.model_file.write('\t\tconfig = {\'units\': self.units,\'activation\': tf.keras.activations.serialize(self.activation),\'kernelInitializer\': tf.keras.initializers.serialize(self.kernelInitializer),\n')
        self.model_file.write('\t\t\t\t\t\'recurrentInitializer\': tf.keras.initializers.serialize(self.recurrentInitializer),\'biasInitializer\': tf.keras.initializers.serialize(self.biasInitializer),\n')
        self.model_file.write('\t\t\t\t\t\'kernelRegularizer\': tf.keras.regularizers.serialize(self.kernelRegularizer),\'recurrentRegularizer\': tf.keras.regularizers.serialize(self.recurrentRegularizer),\n')
        self.model_file.write('\t\t\t\t\t\'biasRegularizer\': tf.keras.regularizers.serialize(self.biasRegularizer),\'useBias\': self.useBias,\'dropout\': self.dropout,\'recurrentDropout\': self.recurrentDropout,\n')
        self.model_file.write('\t\t\t\t\t\'stateClip\': self.stateClip,\'statefull\': self.statefull,\'mergeMode\':self.mergeMode}\n')
        self.model_file.write('\t\tbaseConfig = super(BiRNN, self).get_config()\n')
        self.model_file.write('\t\treturn dict(list(baseConfig.items()) + list(config.items()))\n')
    
    def write_BiGRU(self):
        '''
			write BiGRU class to keras_model.py.   --- UPDATED (CYK) 20191115

        '''
        self.model_file.write('\n\n')
        self.model_file.write('class BiGRU(tf.keras.layers.Layer):\n')
        #Activation
        self.model_file.write('\tclass Activation():\n')
        self.model_file.write('\t\tdef __init__(self,resetGate,updateGate,stateCandidate):\n')
        self.model_file.write('\t\t\tself.resetGate = tf.keras.activations.get(resetGate)\n')
        self.model_file.write('\t\t\tself.updateGate = tf.keras.activations.get(updateGate)\n')
        self.model_file.write('\t\t\tself.stateCandidate = tf.keras.activations.get(stateCandidate)\n\n')
        #Initializer
        self.model_file.write('\tclass Initializer():\n')
        self.model_file.write('\t\tdef __init__(self,resetGate,updateGate,stateCandidate):\n')
        self.model_file.write('\t\t\tself.resetGate = tf.keras.initializers.get(resetGate)\n')
        self.model_file.write('\t\t\tself.updateGate = tf.keras.initializers.get(updateGate)\n')
        self.model_file.write('\t\t\tself.stateCandidate = tf.keras.initializers.get(stateCandidate)\n\n')
        #Regularizer
        self.model_file.write('\tclass Regularizer():\n')
        self.model_file.write('\t\tdef __init__(self,resetGate,updateGate,stateCandidate):\n')
        self.model_file.write('\t\t\tself.resetGate = tf.keras.regularizers.get(resetGate)\n')
        self.model_file.write('\t\t\tself.updateGate = tf.keras.regularizers.get(updateGate)\n')
        self.model_file.write('\t\t\tself.stateCandidate = tf.keras.regularizers.get(stateCandidate)\n\n')          
        #init
        self.model_file.write('\t\t\n')
        self.model_file.write('\tdef __init__(self,units,activation = Activation(\'sigmoid\',\'sigmoid\',\'tanh\'),kernelInitializer = Initializer(\'glorot_uniform\',\'glorot_uniform\',\'glorot_uniform\'),\n')
        self.model_file.write('\t\t\trecurrentInitializer = Initializer(\'orthogonal\',\'orthogonal\',\'orthogonal\'),biasInitializer = Initializer(\'zeros\',\'zeros\',\'zeros\'),kernelRegularizer = Regularizer(None,None,None),\n')
        self.model_file.write('\t\t\trecurrentRegularizer = Regularizer(None,None,None),biasRegularizer = Regularizer(None,None,None,),useBias = True,dropout = 0.,recurrentDropout = 0.,\n')
        self.model_file.write('\t\t\tstateClip = None,statefull = False,mergeMode = \'concat\',**kwargs):\n')
        self.model_file.write('\t\tsuper(BiGRU, self).__init__(**kwargs)\n')
        self.model_file.write('\t\tself.units = units\n')
        self.model_file.write('\t\tself.activation =  activation\n')
        self.model_file.write('\t\tself.kernelInitializer = kernelInitializer\n')
        self.model_file.write('\t\tself.recurrentInitializer = recurrentInitializer\n')
        self.model_file.write('\t\tself.biasInitializer = biasInitializer\n')
        self.model_file.write('\t\tself.kernelRegularizer = kernelRegularizer\n')
        self.model_file.write('\t\tself.recurrentRegularizer =recurrentRegularizer\n')
        self.model_file.write('\t\tself.biasRegularizer = biasRegularizer\n')
        self.model_file.write('\t\tself.useBias = useBias\n')
        self.model_file.write('\t\tself.dropout = dropout\n')
        self.model_file.write('\t\tself.recurrentDropout = recurrentDropout\n')
        self.model_file.write('\t\tself.stateClip = stateClip\n')
        self.model_file.write('\t\tself.statefull = statefull\n')
        self.model_file.write('\t\tself.mergeMode = mergeMode\n\n')
        self.model_file.write('\t\tself._initialStateForward = None\n')
        self.model_file.write('\t\tself._initialStateBackward = None\n\n')
        #build
        self.model_file.write('\tdef build(self, input_shape):\n')
        self.model_file.write('\t\tinput_dim = int(input_shape[-1])\n')
        self.model_file.write('\t\tself.resetGateKernelForward = self.add_weight(shape=(input_dim, self.units),name=\'resetGateKernelForward\',initializer=self.kernelInitializer.resetGate,regularizer=self.kernelRegularizer.resetGate)\n')
        self.model_file.write('\t\tself.updateGateKernelForward = self.add_weight(shape=(input_dim, self.units),name=\'updateGateKernelForward\',initializer=self.kernelInitializer.updateGate,regularizer=self.kernelRegularizer.updateGate)\n')
        self.model_file.write('\t\tself.stateCandidateKernelForward = self.add_weight(shape=(input_dim, self.units),name=\'stateCandidateKernelForward\',initializer=self.kernelInitializer.stateCandidate,regularizer=self.kernelRegularizer.stateCandidate)\n')
        self.model_file.write('\t\tself.resetGateRecurrentKernelForward = self.add_weight(shape=(self.units, self.units),name=\'resetGateRecurrentKernelForward\',initializer=self.recurrentInitializer.resetGate,regularizer=self.recurrentRegularizer.resetGate)\n')
        self.model_file.write('\t\tself.updateGateRecurrentKernelForward = self.add_weight(shape=(self.units, self.units),name=\'updateGateRecurrentKernelForward\',initializer=self.recurrentInitializer.updateGate,regularizer=self.recurrentRegularizer.updateGate)\n')
        self.model_file.write('\t\tself.stateCandidateRecurrentKernelForward = self.add_weight(shape=(self.units, self.units),name=\'stateCandidateRecurrentKernelForward\',initializer=self.recurrentInitializer.stateCandidate,regularizer=self.recurrentRegularizer.stateCandidate)\n')
        self.model_file.write('\t\tself.resetGateKernelBackward = self.add_weight(shape=(input_dim, self.units),name=\'resetGateKernelBackward\',initializer=self.kernelInitializer.resetGate,regularizer=self.kernelRegularizer.resetGate)\n')
        self.model_file.write('\t\tself.updateGateKernelBackward = self.add_weight(shape=(input_dim, self.units),name=\'updateGateKernelBackward\',initializer=self.kernelInitializer.updateGate,regularizer=self.kernelRegularizer.updateGate)\n')
        self.model_file.write('\t\tself.stateCandidateKernelBackward = self.add_weight(shape=(input_dim, self.units),name=\'stateCandidateKernelBackward\',initializer=self.kernelInitializer.stateCandidate,regularizer=self.kernelRegularizer.stateCandidate)\n')
        self.model_file.write('\t\tself.resetGateRecurrentKernelBackward = self.add_weight(shape=(self.units, self.units),name=\'resetGateRecurrentKernelBackward\',initializer=self.recurrentInitializer.resetGate,regularizer=self.recurrentRegularizer.resetGate)\n')
        self.model_file.write('\t\tself.updateGateRecurrentKernelBackward = self.add_weight(shape=(self.units, self.units),name=\'updateGateRecurrentKernelBackward\',initializer=self.recurrentInitializer.updateGate,regularizer=self.recurrentRegularizer.updateGate)\n')
        self.model_file.write('\t\tself.stateCandidateRecurrentKernelBackward = self.add_weight(shape=(self.units, self.units),name=\'stateCandidateRecurrentKernelBackward\',initializer=self.recurrentInitializer.stateCandidate,regularizer=self.recurrentRegularizer.stateCandidate)\n')
        self.model_file.write('\t\tif self.useBias:\n')
        self.model_file.write('\t\t\tself.resetGateBiasForward = self.add_weight(shape=(self.units,),name=\'resetGateBiasForward\',initializer=self.biasInitializer.resetGate,regularizer=self.biasRegularizer.resetGate)\n')
        self.model_file.write('\t\t\tself.updateGateBiasForward = self.add_weight(shape=(self.units,),name=\'updateGateBiasForward\',initializer=self.biasInitializer.updateGate,regularizer=self.biasRegularizer.updateGate)\n')
        self.model_file.write('\t\t\tself.stateCandidateBiasForward = self.add_weight(shape=(self.units,),name=\'stateCandidate\',initializer=self.biasInitializer.stateCandidate,regularizer=self.biasRegularizer.stateCandidate)\n')
        self.model_file.write('\t\t\tself.resetGateBiasBackward = self.add_weight(shape=(self.units,),name=\'resetGateBiasBackward\',initializer=self.biasInitializer.resetGate,regularizer=self.biasRegularizer.resetGate)\n')
        self.model_file.write('\t\t\tself.updateGateBiasBackward = self.add_weight(shape=(self.units,),name=\'updateGateBiasBackward\',initializer=self.biasInitializer.updateGate,regularizer=self.biasRegularizer.updateGate)\n')
        self.model_file.write('\t\t\tself.stateCandidateBiasBackward = self.add_weight(shape=(self.units,),name=\'stateCandidate\',initializer=self.biasInitializer.stateCandidate,regularizer=self.biasRegularizer.stateCandidate)\n')
        self.model_file.write('\t\tself.built = True\n\n')
        #_stepFunctionForward
        self.model_file.write('\tdef _stepFunctionForward(self,inputs,state,training = None):\n') 
        self.model_file.write('\t\tif 0 < self.dropout < 1 : inputs = tf.keras.backend.in_train_phase(tf.nn.dropout(inputs,self.dropout), inputs, training=training)\n')  
        self.model_file.write('\t\tif  0 < self.recurrentDropout < 1 : state =   tf.keras.backend.in_train_phase(tf.nn.dropout(state,self.recurrentDropout), state, training=training)\n')  
        self.model_file.write('\t\tif self.useBias:\n')   
        self.model_file.write('\t\t\tresetGateOutput = self.activation.resetGate(tf.matmul(state,self.resetGateRecurrentKernelForward) + tf.matmul(inputs,self.resetGateKernelForward)  + self.resetGateBiasForward)\n')
        self.model_file.write('\t\t\tupdateGateOutput = self.activation.updateGate(tf.matmul(state,self.updateGateRecurrentKernelForward) + tf.matmul(inputs,self.updateGateKernelForward) + self.updateGateBiasForward)\n')
        self.model_file.write('\t\t\tstateCandidate = self.activation.stateCandidate(tf.matmul(tf.multiply(resetGateOutput,state),self.stateCandidateRecurrentKernelForward) + tf.matmul(inputs,self.stateCandidateKernelForward) + self.stateCandidateBiasForward)\n')
        self.model_file.write('\t\telse:\n')
        self.model_file.write('\t\t\tresetGateOutput = self.activation.resetGate(tf.matmul(state,self.resetGateRecurrentKernelForward) + tf.matmul(inputs,self.resetGateKernelForward))\n')
        self.model_file.write('\t\t\tupdateGateOutput = self.activation.updateGate(tf.matmul(state,self.updateGateRecurrentKernelForward) + tf.matmul(inputs,self.updateGateKernelForward))\n')
        self.model_file.write('\t\t\tstateCandidate = self.activation.stateCandidate(tf.matmul(tf.multiply(resetGateOutput,state),self.stateCandidateRecurrentKernelForward) + tf.matmul(inputs,self.stateCandidateKernelForward))\n')
        self.model_file.write('\t\toutput = tf.multiply(updateGateOutput,state) + tf.multiply(1-updateGateOutput,stateCandidate)\n')
        self.model_file.write('\t\tif self.stateClip is not None:\n')
        self.model_file.write('\t\t\tnewState = tf.clip_by_value(output,-1*self.stateClip,self.stateClip)\n') 
        self.model_file.write('\t\t\treturn output,newState\n')
        self.model_file.write('\t\treturn output,output\n\n') 
        #_stepFunctionBackward
        self.model_file.write('\tdef _stepFunctionBackward(self,inputs,state,training = None):\n') 
        self.model_file.write('\t\tif 0 < self.dropout < 1 : inputs = tf.keras.backend.in_train_phase(tf.nn.dropout(inputs,self.dropout), inputs, training=training)\n')  
        self.model_file.write('\t\tif  0 < self.recurrentDropout < 1 : state =   tf.keras.backend.in_train_phase(tf.nn.dropout(state,self.recurrentDropout), state, training=training)\n')  
        self.model_file.write('\t\tif self.useBias:\n')   
        self.model_file.write('\t\t\tresetGateOutput = self.activation.resetGate(tf.matmul(state,self.resetGateRecurrentKernelBackward) + tf.matmul(inputs,self.resetGateKernelBackward)  + self.resetGateBiasBackward)\n')
        self.model_file.write('\t\t\tupdateGateOutput = self.activation.updateGate(tf.matmul(state,self.updateGateRecurrentKernelBackward) + tf.matmul(inputs,self.updateGateKernelBackward) + self.updateGateBiasBackward)\n')
        self.model_file.write('\t\t\tstateCandidate = self.activation.stateCandidate(tf.matmul(tf.multiply(resetGateOutput,state),self.stateCandidateRecurrentKernelBackward) + tf.matmul(inputs,self.stateCandidateKernelBackward) + self.stateCandidateBiasBackward)\n')
        self.model_file.write('\t\telse:\n')
        self.model_file.write('\t\t\tresetGateOutput = self.activation.resetGate(tf.matmul(state,self.resetGateRecurrentKernelBackward) + tf.matmul(inputs,self.resetGateKernelBackward))\n')
        self.model_file.write('\t\t\tupdateGateOutput = self.activation.updateGate(tf.matmul(state,self.updateGateRecurrentKernelBackward) + tf.matmul(inputs,self.updateGateKernelBackward))\n')
        self.model_file.write('\t\t\tstateCandidate = self.activation.stateCandidate(tf.matmul(tf.multiply(resetGateOutput,state),self.stateCandidateRecurrentKernelBackward) + tf.matmul(inputs,self.stateCandidateKernelBackward))\n')
        self.model_file.write('\t\toutput = tf.multiply(updateGateOutput,state) + tf.multiply(1-updateGateOutput,stateCandidate)\n')
        self.model_file.write('\t\tif self.stateClip is not None:\n')
        self.model_file.write('\t\t\tnewState = tf.clip_by_value(output,-1*self.stateClip,self.stateClip)\n') 
        self.model_file.write('\t\t\treturn output,newState\n')
        self.model_file.write('\t\treturn output,output\n\n') 
        #_switchBatchTime 
        self.model_file.write('\tdef _switchBatchTime(self,inputs):\n')
        self.model_file.write('\t\tperm = list(range(inputs.get_shape().rank))\n') 
        self.model_file.write('\t\tperm[0:2] = (1,0)\n') 
        self.model_file.write('\t\treturn tf.transpose(inputs,perm)\n\n') 
        #_getInitialState
        self.model_file.write('\tdef _getInitialState(self,inputs):\n') 
        self.model_file.write('\t\tinitialState = tf.zeros_like(inputs)\n')          
        self.model_file.write('\t\tinitialState = initialState[:,0,0:1]\n') 
        self.model_file.write('\t\tmultiples = tf.convert_to_tensor([1,self.units])\n') 
        self.model_file.write('\t\tinitialState = tf.tile(initialState,multiples)\n') 
        self.model_file.write('\t\treturn initialState\n\n') 
        #call
        self.model_file.write('\tdef call(self, inputs ,state = None):\n') 
        self.model_file.write('\t\tbatchSize,timeSteps,_ = inputs.shape.as_list()\n') 
        self.model_file.write('\t\ttimeStepsTensor = tf.shape(inputs)[1]\n')
        self.model_file.write('\t\tinitialStateForward,initialStateBackward = self._initialStateForward,self._initialStateBackward\n')
        self.model_file.write('\t\tif initialStateForward is None: initialStateForward = self._getInitialState(inputs)\n')
        self.model_file.write('\t\tif initialStateBackward is None: initialStateBackward = self._getInitialState(inputs)\n')
        self.model_file.write('\t\tinputs = self._switchBatchTime(inputs)\n')
        self.model_file.write('\t\tinitialTime = tf.zeros_like(timeStepsTensor,name=\'time\')\n')
        self.model_file.write('\t\tinputsTensorArrayForward = tf.TensorArray(dtype=inputs.dtype,size=timeStepsTensor)\n')
        self.model_file.write('\t\tinputsTensorArrayBackward = tf.TensorArray(dtype=inputs.dtype,size=timeStepsTensor)\n')
        self.model_file.write('\t\tinputsTensorArrayForward = inputsTensorArrayForward.unstack(inputs)\n')
        self.model_file.write('\t\tinputsTensorArrayBackward = inputsTensorArrayBackward.unstack(tf.reverse(inputs,tf.zeros(1, tf.int32)))\n') 
        self.model_file.write('\t\tinitialOutputTensorArrayForward = tf.TensorArray(dtype=inputs.dtype,size=timeStepsTensor)\n\n')
        self.model_file.write('\t\tinitialOutputTensorArrayBackward = tf.TensorArray(dtype=inputs.dtype,size=timeStepsTensor)\n\n')
        self.model_file.write('\t\tdef _cond(time,outputTensorArray,state):\n')
        self.model_file.write('\t\t\treturn time<timeStepsTensor\n\n')
        self.model_file.write('\t\tdef _bodyForward(time,outputTensorArray,state):\n')
        self.model_file.write('\t\t\tcurrentInput = inputsTensorArrayForward.read(time)\n')
        self.model_file.write('\t\t\toutput,state = self._stepFunctionForward(currentInput,state)\n')
        self.model_file.write('\t\t\toutputTensorArray = outputTensorArray.write(time,output)\n')
        self.model_file.write('\t\t\treturn (time+1,outputTensorArray,state)\n\n')
        self.model_file.write('\t\tdef _bodyBackward(time,outputTensorArray,state):\n')
        self.model_file.write('\t\t\tcurrentInput = inputsTensorArrayBackward.read(time)\n')
        self.model_file.write('\t\t\toutput,state = self._stepFunctionBackward(currentInput,state)\n')
        self.model_file.write('\t\t\toutputTensorArray = outputTensorArray.write(time,output)\n')
        self.model_file.write('\t\t\treturn (time+1,outputTensorArray,state)\n\n')
        self.model_file.write('\t\tfinalOutputsForward = tf.while_loop(cond=_cond,body=_bodyForward,loop_vars=(initialTime,initialOutputTensorArrayForward,initialStateForward))\n')
        self.model_file.write('\t\tfinalOutputsBackward = tf.while_loop(cond=_cond,body=_bodyBackward,loop_vars=(initialTime,initialOutputTensorArrayBackward,initialStateBackward))\n')
        self.model_file.write('\t\tfinalOutputTensorArrayForward = finalOutputsForward[1]\n')
        self.model_file.write('\t\tfinalOutputTensorArrayBackward = finalOutputsBackward[1]\n')
        self.model_file.write('\t\tfinalOutputForward = finalOutputTensorArrayForward.stack()\n') 
        self.model_file.write('\t\tfinalOutputBackward = finalOutputTensorArrayBackward.stack()\n') 
        self.model_file.write('\t\tfinalOutputForward.set_shape([timeSteps,batchSize,self.units])\n')
        self.model_file.write('\t\tfinalOutputBackward.set_shape([timeSteps,batchSize,self.units])\n')
        self.model_file.write('\t\tif self.statefull : self._initialStatForwarde,self._initialStateBackward = finalOutputsForward[-1],finalOutputsBackward[-1]\n')
        self.model_file.write('\t\tfinalOutputForward = self._switchBatchTime(finalOutputForward)\n')
        self.model_file.write('\t\tfinalOutputBackward = self._switchBatchTime(finalOutputBackward)\n')
        self.model_file.write('\t\tif self.mergeMode == \'concat\': mergeOutput = tf.concat([finalOutputForward, finalOutputBackward],-1)\n')
        self.model_file.write('\t\telif self.mergeMode == \'sum\': mergeOutput = finalOutputForward + finalOutputBackward\n')
        self.model_file.write('\t\telif self.mergeMode == \'ave\': mergeOutput = (finalOutputForward + finalOutputBackward) / 2\n')
        self.model_file.write('\t\telif self.mergeMode == \'mul\': mergeOutput = finalOutputForward * finalOutputBackward\n')
        self.model_file.write('\t\telif self.mergeMode is None: mergeOutput = [finalOutputForward, finalOutputBackward]\n')
        self.model_file.write('\t\telse: raise ValueError(\'mergeMode wrong\')\n')
        self.model_file.write('\t\treturn mergeOutput\n\n')
        #compute_output_shape
        self.model_file.write('\tdef compute_output_shape(self,input_shape):\n')
        self.model_file.write('\t\tbatchSize,timeSteps,dim = input_shape\n')
        self.model_file.write('\t\toutputShape = tf.TensorShape([batchSize,timeSteps,self.units*2]) if self.mergeMode == \'concat\' else tf.TensorShape([batchSize,timeSteps,self.units])\n')
        self.model_file.write('\t\tif self.mergeMode is None : return [outputShape,outputShape]\n')
        self.model_file.write('\t\treturn outputShape\n\n')
        #get_config
        self.model_file.write('\tdef get_config(self):\n')
        self.model_file.write('\t\tconfig = {\'units\': self.units,\'activation\': tf.keras.activations.serialize(self.activation),\'kernelInitializer\': tf.keras.initializers.serialize(self.kernelInitializer),\n')
        self.model_file.write('\t\t\t\t\t\'recurrentInitializer\': tf.keras.initializers.serialize(self.recurrentInitializer),\'biasInitializer\': tf.keras.initializers.serialize(self.biasInitializer),\n')
        self.model_file.write('\t\t\t\t\t\'kernelRegularizer\': tf.keras.regularizers.serialize(self.kernelRegularizer),\'recurrentRegularizer\': tf.keras.regularizers.serialize(self.recurrentRegularizer),\n')
        self.model_file.write('\t\t\t\t\t\'biasRegularizer\': tf.keras.regularizers.serialize(self.biasRegularizer),\'useBias\': self.useBias,\'dropout\': self.dropout,\'recurrentDropout\': self.recurrentDropout,\n')
        self.model_file.write('\t\t\t\t\t\'stateClip\': self.stateClip,\'statefull\': self.statefull,\'mergeMode\':self.mergeMode}\n')
        self.model_file.write('\t\tbaseConfig = super(BiGRU, self).get_config()\n')
        self.model_file.write('\t\treturn dict(list(baseConfig.items()) + list(config.items()))\n')

    def write_BiLSTM(self):
        '''
			write BiLSTM class to keras_model.py.   --- UPDATED (CYK) 20191115

        '''
        self.model_file.write('\n\n')
        self.model_file.write('class BiLSTM(tf.keras.layers.Layer):\n')
        #Activation
        self.model_file.write('\tclass Activation():\n')
        self.model_file.write('\t\tdef __init__(self,inputGate,forgetGate,outputGate,memory,shadowGate):\n')
        self.model_file.write('\t\t\tself.inputGate = tf.keras.activations.get(inputGate)\n')
        self.model_file.write('\t\t\tself.forgetGate = tf.keras.activations.get(forgetGate)\n')
        self.model_file.write('\t\t\tself.outputGate = tf.keras.activations.get(outputGate)\n')
        self.model_file.write('\t\t\tself.memory = tf.keras.activations.get(memory)\n')
        self.model_file.write('\t\t\tself.shadowGate = tf.keras.activations.get(shadowGate)\n\n')
        #Initializer
        self.model_file.write('\tclass Initializer():\n')
        self.model_file.write('\t\tdef __init__(self,inputGate,forgetGate,outputGate,memory):\n')
        self.model_file.write('\t\t\tself.inputGate = tf.keras.initializers.get(inputGate)\n')
        self.model_file.write('\t\t\tself.forgetGate = tf.keras.initializers.get(forgetGate)\n')
        self.model_file.write('\t\t\tself.outputGate = tf.keras.initializers.get(outputGate)\n')
        self.model_file.write('\t\t\tself.memory = tf.keras.initializers.get(memory)\n\n')
        #Regularizer
        self.model_file.write('\tclass Regularizer():\n')
        self.model_file.write('\t\tdef __init__(self,inputGate,forgetGate,outputGate,memory):\n')
        self.model_file.write('\t\t\tself.inputGate = tf.keras.regularizers.get(inputGate)\n')
        self.model_file.write('\t\t\tself.forgetGate = tf.keras.regularizers.get(forgetGate)\n')
        self.model_file.write('\t\t\tself.outputGate = tf.keras.regularizers.get(outputGate)\n')
        self.model_file.write('\t\t\tself.memory = tf.keras.regularizers.get(memory)\n\n')            
        #init
        self.model_file.write('\t\t\n')
        self.model_file.write('\tdef __init__(self,units,activation = Activation(\'sigmoid\',\'sigmoid\',\'sigmoid\',\'tanh\',\'tanh\'),kernelInitializer = Initializer(\'glorot_uniform\',\'glorot_uniform\',\'glorot_uniform\',\'glorot_uniform\'),\n')
        self.model_file.write('\t\t\trecurrentInitializer = Initializer(\'orthogonal\',\'orthogonal\',\'orthogonal\',\'orthogonal\'),peepholeInitializer = Initializer(\'orthogonal\',\'orthogonal\',\'orthogonal\',None),\n')
        self.model_file.write('\t\t\tbiasInitializer = Initializer(\'zeros\',\'zeros\',\'zeros\',\'zeros\'),kernelRegularizer = Regularizer(None,None,None,None),recurrentRegularizer = Regularizer(None,None,None,None),\n')
        self.model_file.write('\t\t\tpeepholeRegularizer = Regularizer(None,None,None,None),biasRegularizer = Regularizer(None,None,None,None),useBias = True,usePeephole = False,dropout = 0.,recurrentDropout = 0.,\n')
        self.model_file.write('\t\t\tmemoryCellClip = None,shadowStateClip = None,statefull = False,mergeMode = \'concat\',**kwargs):\n')
        self.model_file.write('\t\tsuper(BiLSTM, self).__init__(**kwargs)\n')
        self.model_file.write('\t\tself.units = units\n')
        self.model_file.write('\t\tself.activation =  activation\n')
        self.model_file.write('\t\tself.kernelInitializer = kernelInitializer\n')
        self.model_file.write('\t\tself.recurrentInitializer = recurrentInitializer\n')
        self.model_file.write('\t\tself.peepholeInitializer = peepholeInitializer\n')
        self.model_file.write('\t\tself.biasInitializer = biasInitializer\n')
        self.model_file.write('\t\tself.kernelRegularizer = kernelRegularizer\n')
        self.model_file.write('\t\tself.recurrentRegularizer =recurrentRegularizer\n')
        self.model_file.write('\t\tself.peepholeRegularizer = peepholeRegularizer\n')
        self.model_file.write('\t\tself.biasRegularizer = biasRegularizer\n')
        self.model_file.write('\t\tself.useBias = useBias\n')
        self.model_file.write('\t\tself.usePeephole = usePeephole\n')
        self.model_file.write('\t\tself.dropout = dropout\n')
        self.model_file.write('\t\tself.recurrentDropout = recurrentDropout\n')
        self.model_file.write('\t\tself.memoryCellClip = memoryCellClip\n')
        self.model_file.write('\t\tself.shadowStateClip = shadowStateClip\n')
        self.model_file.write('\t\tself.statefull = statefull\n')
        self.model_file.write('\t\tself.mergeMode = mergeMode\n\n')
        self.model_file.write('\t\tself._initialMemoryCellForward = None\n')
        self.model_file.write('\t\tself._initialShadowStateForward = None\n\n')
        self.model_file.write('\t\tself._initialMemoryCellBackward = None\n')
        self.model_file.write('\t\tself._initialShadowStateBackward = None\n\n')
        #build
        self.model_file.write('\tdef build(self, input_shape):\n')
        self.model_file.write('\t\tinput_dim = int(input_shape[-1])\n')
        self.model_file.write('\t\tself.inputGateKernelForward = self.add_weight(shape=(input_dim, self.units),name=\'inputGateKernelForward\',initializer=self.kernelInitializer.inputGate,regularizer=self.kernelRegularizer.inputGate)\n')
        self.model_file.write('\t\tself.forgetGateKernelForward = self.add_weight(shape=(input_dim, self.units),name=\'forgetGateKernelForward\',initializer=self.kernelInitializer.forgetGate,regularizer=self.kernelRegularizer.forgetGate)\n')
        self.model_file.write('\t\tself.outputGateKernelForward = self.add_weight(shape=(input_dim, self.units),name=\'outputGateKernelForward\',initializer=self.kernelInitializer.outputGate,regularizer=self.kernelRegularizer.outputGate)\n')
        self.model_file.write('\t\tself.memoryKernelForward = self.add_weight(shape=(input_dim, self.units),name=\'memoryKernelForward\',initializer=self.kernelInitializer.memory,regularizer=self.kernelRegularizer.memory)\n')
        self.model_file.write('\t\tself.inputGateRecurrentKernelForward = self.add_weight(shape=(self.units, self.units),name=\'inputGateRecurrentKernelForward\',initializer=self.recurrentInitializer.inputGate,regularizer=self.recurrentRegularizer.inputGate)\n')
        self.model_file.write('\t\tself.forgetGateRecurrentKernelForward = self.add_weight(shape=(self.units, self.units),name=\'forgetGateRecurrentKernelForward\',initializer=self.recurrentInitializer.forgetGate,regularizer=self.recurrentRegularizer.forgetGate)\n')
        self.model_file.write('\t\tself.outputGateRecurrentKernelForward = self.add_weight(shape=(self.units, self.units),name=\'outputGateRecurrentKernelForward\',initializer=self.recurrentInitializer.outputGate,regularizer=self.recurrentRegularizer.outputGate)\n')
        self.model_file.write('\t\tself.memoryRecurrentKernelForward = self.add_weight(shape=(self.units, self.units),name=\'memoryRecurrentKernelForward\',initializer=self.recurrentInitializer.memory,regularizer=self.recurrentRegularizer.memory)\n')
        self.model_file.write('\t\tself.inputGateKernelBackward = self.add_weight(shape=(input_dim, self.units),name=\'inputGateKernelBackward\',initializer=self.kernelInitializer.inputGate,regularizer=self.kernelRegularizer.inputGate)\n')
        self.model_file.write('\t\tself.forgetGateKernelBackward = self.add_weight(shape=(input_dim, self.units),name=\'forgetGateKernelBackward\',initializer=self.kernelInitializer.forgetGate,regularizer=self.kernelRegularizer.forgetGate)\n')
        self.model_file.write('\t\tself.outputGateKernelBackward = self.add_weight(shape=(input_dim, self.units),name=\'outputGateKernelBackward\',initializer=self.kernelInitializer.outputGate,regularizer=self.kernelRegularizer.outputGate)\n')
        self.model_file.write('\t\tself.memoryKernelBackward = self.add_weight(shape=(input_dim, self.units),name=\'memoryKernelBackward\',initializer=self.kernelInitializer.memory,regularizer=self.kernelRegularizer.memory)\n')
        self.model_file.write('\t\tself.inputGateRecurrentKernelBackward = self.add_weight(shape=(self.units, self.units),name=\'inputGateRecurrentKernelBackward\',initializer=self.recurrentInitializer.inputGate,regularizer=self.recurrentRegularizer.inputGate)\n')
        self.model_file.write('\t\tself.forgetGateRecurrentKernelBackward = self.add_weight(shape=(self.units, self.units),name=\'forgetGateRecurrentKernelBackward\',initializer=self.recurrentInitializer.forgetGate,regularizer=self.recurrentRegularizer.forgetGate)\n')
        self.model_file.write('\t\tself.outputGateRecurrentKernelBackward = self.add_weight(shape=(self.units, self.units),name=\'outputGateRecurrentKernelBackward\',initializer=self.recurrentInitializer.outputGate,regularizer=self.recurrentRegularizer.outputGate)\n')
        self.model_file.write('\t\tself.memoryRecurrentKernelBackward = self.add_weight(shape=(self.units, self.units),name=\'memoryRecurrentKernelBackward\',initializer=self.recurrentInitializer.memory,regularizer=self.recurrentRegularizer.memory)\n')
        self.model_file.write('\t\tif self.usePeephole:\n')
        self.model_file.write('\t\t\tself.inputGatePeepholeKernelForward = self.add_weight(shape=(self.units, self.units),name=\'inputGatePeepholeKernelForward\',initializer=self.peepholeInitializer.inputGate,regularizer=self.peepholeRegularizer.inputGate)\n')
        self.model_file.write('\t\t\tself.forgetGatePeepholeKernelForward = self.add_weight(shape=(self.units, self.units),name=\'forgetGatePeepholeKernelForward\',initializer=self.peepholeInitializer.forgetGate,regularizer=self.peepholeRegularizer.forgetGate)\n')
        self.model_file.write('\t\t\tself.outputGatePeepholeKernelForward = self.add_weight(shape=(self.units, self.units),name=\'outputGatePeepholeKernelForward\',initializer=self.peepholeInitializer.outputGate,regularizer=self.peepholeRegularizer.outputGate)\n')
        self.model_file.write('\t\t\tself.inputGatePeepholeKernelBackward = self.add_weight(shape=(self.units, self.units),name=\'inputGatePeepholeKernelBackward\',initializer=self.peepholeInitializer.inputGate,regularizer=self.peepholeRegularizer.inputGate)\n')
        self.model_file.write('\t\t\tself.forgetGatePeepholeKernelBackward = self.add_weight(shape=(self.units, self.units),name=\'forgetGatePeepholeKernelBackward\',initializer=self.peepholeInitializer.forgetGate,regularizer=self.peepholeRegularizer.forgetGate)\n')
        self.model_file.write('\t\t\tself.outputGatePeepholeKernelBackward = self.add_weight(shape=(self.units, self.units),name=\'outputGatePeepholeKernelBackward\',initializer=self.peepholeInitializer.outputGate,regularizer=self.peepholeRegularizer.outputGate)\n')
        self.model_file.write('\t\tif self.useBias:\n')
        self.model_file.write('\t\t\tself.inputGateBiasForward = self.add_weight(shape=(self.units,),name=\'inputGateBiasForward\',initializer=self.biasInitializer.inputGate,regularizer=self.biasRegularizer.inputGate)\n')
        self.model_file.write('\t\t\tself.forgetGateBiasForward = self.add_weight(shape=(self.units,),name=\'forgetGateBiasForward\',initializer=self.biasInitializer.forgetGate,regularizer=self.biasRegularizer.forgetGate)\n')
        self.model_file.write('\t\t\tself.outputGateBiasForward = self.add_weight(shape=(self.units,),name=\'outputGateBiasForward\',initializer=self.biasInitializer.outputGate,regularizer=self.biasRegularizer.outputGate)\n')
        self.model_file.write('\t\t\tself.memoryBiasForward = self.add_weight(shape=(self.units,),name=\'memoryBiasForward\',initializer=self.biasInitializer.memory,regularizer=self.biasRegularizer.memory)\n')
        self.model_file.write('\t\t\tself.inputGateBiasBackward = self.add_weight(shape=(self.units,),name=\'inputGateBiasBackward\',initializer=self.biasInitializer.inputGate,regularizer=self.biasRegularizer.inputGate)\n')
        self.model_file.write('\t\t\tself.forgetGateBiasBackward = self.add_weight(shape=(self.units,),name=\'forgetGateBiasBackward\',initializer=self.biasInitializer.forgetGate,regularizer=self.biasRegularizer.forgetGate)\n')
        self.model_file.write('\t\t\tself.outputGateBiasBackward = self.add_weight(shape=(self.units,),name=\'outputGateBiasBackward\',initializer=self.biasInitializer.outputGate,regularizer=self.biasRegularizer.outputGate)\n')
        self.model_file.write('\t\t\tself.memoryBiasBackward = self.add_weight(shape=(self.units,),name=\'memoryBiasBackward\',initializer=self.biasInitializer.memory,regularizer=self.biasRegularizer.memory)\n')
        self.model_file.write('\t\tself.built = True\n\n')
        #_stepFunctionForward
        self.model_file.write('\tdef _stepFunctionForward(self,inputs,states,training = None):\n')
        self.model_file.write('\t\tmemoryCell,shadowState = states\n')  
        self.model_file.write('\t\tif 0 < self.dropout < 1 : inputs = tf.keras.backend.in_train_phase(tf.nn.dropout(inputs,self.dropout), inputs, training=training)\n')  
        self.model_file.write('\t\tif  0 < self.recurrentDropout < 1 : shadowState =   tf.keras.backend.in_train_phase(tf.nn.dropout(shadowState,self.recurrentDropout), shadowState, training=training)\n')  
        self.model_file.write('\t\tif self.usePeephole:\n')  
        self.model_file.write('\t\t\tif self.useBias:\n')
        self.model_file.write('\t\t\t\tinputGateOutput = self.activation.inputGate(tf.matmul(shadowState,self.inputGateRecurrentKernelForward) + tf.matmul(inputs,self.inputGateKernelForward) + tf.matmul(memoryCell,self.inputGatePeepholeKernelForward)  + self.inputGateBiasForward)\n') 
        self.model_file.write('\t\t\t\tforgetGateOutput = self.activation.forgetGate(tf.matmul(shadowState,self.forgetGateRecurrentKernelForward) + tf.matmul(inputs,self.forgetGateKernelForward) + tf.matmul(memoryCell,self.forgetGatePeepholeKernelForward)  + self.forgetGateBiasForward)\n')
        self.model_file.write('\t\t\t\tmemoryCellCandidate = self.activation.memory(tf.matmul(shadowState,self.memoryRecurrentKernelForward) + tf.matmul(inputs,self.memoryKernelForward) + self.memoryBiasForward)\n')
        self.model_file.write('\t\t\t\tnewMemoryCell = tf.multiply(forgetGateOutput,memoryCell) + tf.multiply(inputGateOutput,memoryCellCandidate)\n')
        self.model_file.write('\t\t\t\tif self.memoryCellClip is not None: newMemoryCell = tf.clip_by_value(newMemoryCell,-1*self.memoryCellClip,self.memoryCellClip)\n')
        self.model_file.write('\t\t\t\toutputGateOutput = self.activation.outputGate(tf.matmul(shadowState,self.outputGateRecurrentKernelForward) + tf.matmul(inputs,self.outputGateKernelForward) + tf.matmul(newMemoryCell,self.outputGatePeepholeKernelForward) + self.outputGateBiasForward)\n') 
        self.model_file.write('\t\t\telse:\n')
        self.model_file.write('\t\t\t\tinputGateOutput = self.activation.inputGate(tf.matmul(shadowState,self.inputGateRecurrentKernelForward) + tf.matmul(inputs,self.inputGateKernelForward) + tf.matmul(memoryCell,self.inputGatePeepholeKernelForward))\n')
        self.model_file.write('\t\t\t\tforgetGateOutput = self.activation.forgetGate(tf.matmul(shadowState,self.forgetGateRecurrentKernelForward) + tf.matmul(inputs,self.forgetGateKernelForward) + tf.matmul(memoryCell,self.forgetGatePeepholeKernelForward))\n')
        self.model_file.write('\t\t\t\tmemoryCellCandidate = self.activation.memory(tf.matmul(shadowState,self.memoryRecurrentKernelForward) + tf.matmul(inputs,self.memoryKernelForward))\n')
        self.model_file.write('\t\t\t\tnewMemoryCell = tf.multiply(forgetGateOutput,memoryCell) + tf.multiply(inputGateOutput,memoryCellCandidate)\n')
        self.model_file.write('\t\t\t\tif self.memoryCellClip is not None: newMemoryCell = tf.clip_by_value(newMemoryCell,-1*self.memoryCellClip,self.memoryCellClip)\n')
        self.model_file.write('\t\t\t\toutputGateOutput = self.activation.outputGate(tf.matmul(shadowState,self.outputGateRecurrentKernelForward) + tf.matmul(inputs,self.outputGateKernelForward) + tf.matmul(newMemoryCell,self.outputGatePeepholeKernelForward))\n')
        self.model_file.write('\t\telse:\n')  
        self.model_file.write('\t\t\tif self.useBias:\n')
        self.model_file.write('\t\t\t\tinputGateOutput = self.activation.inputGate(tf.matmul(shadowState,self.inputGateRecurrentKernelForward) + tf.matmul(inputs,self.inputGateKernelForward)  + self.inputGateBiasForward)\n')
        self.model_file.write('\t\t\t\tforgetGateOutput = self.activation.forgetGate(tf.matmul(shadowState,self.forgetGateRecurrentKernelForward) + tf.matmul(inputs,self.forgetGateKernelForward) + self.forgetGateBiasForward)\n')
        self.model_file.write('\t\t\t\toutputGateOutput = self.activation.outputGate(tf.matmul(shadowState,self.outputGateRecurrentKernelForward) + tf.matmul(inputs,self.outputGateKernelForward) + self.outputGateBiasForward)\n')
        self.model_file.write('\t\t\t\tmemoryCellCandidate = self.activation.memory(tf.matmul(shadowState,self.memoryRecurrentKernelForward) + tf.matmul(inputs,self.memoryKernelForward) + self.memoryBiasForward)\n')
        self.model_file.write('\t\t\t\tnewMemoryCell = tf.multiply(forgetGateOutput,memoryCell) + tf.multiply(inputGateOutput,memoryCellCandidate)\n')
        self.model_file.write('\t\t\telse:\n')
        self.model_file.write('\t\t\t\tinputGateOutput = self.activation.inputGate(tf.matmul(shadowState,self.inputGateRecurrentKernelForward) + tf.matmul(inputs,self.inputGateKernelForward))\n')
        self.model_file.write('\t\t\t\tforgetGateOutput = self.activation.forgetGate(tf.matmul(shadowState,self.forgetGateRecurrentKernelForward) + tf.matmul(inputs,self.forgetGateKernelForward))\n')
        self.model_file.write('\t\t\t\toutputGateOutput = self.activation.outputGate(tf.matmul(shadowState,self.outputGateRecurrentKernelForward) + tf.matmul(inputs,self.outputGateKernelForward))\n')
        self.model_file.write('\t\t\t\tmemoryCellCandidate = self.activation.memory(tf.matmul(shadowState,self.memoryRecurrentKernelForward) + tf.matmul(inputs,self.memoryKernelForward)) \n')
        self.model_file.write('\t\t\t\tnewMemoryCell = tf.multiply(forgetGateOutput,memoryCell) + tf.multiply(inputGateOutput,memoryCellCandidate)\n')
        self.model_file.write('\t\t\tif self.memoryCellClip is not None: newMemoryCell = tf.clip_by_value(newMemoryCell,-1*self.memoryCellClip,self.memoryCellClip)\n')
        self.model_file.write('\t\tnewShadowState = tf.multiply(outputGateOutput,self.activation.shadowGate(newMemoryCell))\n') 
        self.model_file.write('\t\toutput = newShadowState\n')
        self.model_file.write('\t\tif self.shadowStateClip is not None: newShadowState = tf.clip_by_value(newShadowState,-1*self.shadowStateClip,self.shadowStateClip)\n') 
        self.model_file.write('\t\treturn output,(newMemoryCell,newShadowState)\n\n') 
        #_stepFunctionBackward
        self.model_file.write('\tdef _stepFunctionBackward(self,inputs,states,training = None):\n')
        self.model_file.write('\t\tmemoryCell,shadowState = states\n')  
        self.model_file.write('\t\tif 0 < self.dropout < 1 : inputs = tf.keras.backend.in_train_phase(tf.nn.dropout(inputs,self.dropout), inputs, training=training)\n')  
        self.model_file.write('\t\tif  0 < self.recurrentDropout < 1 : shadowState =   tf.keras.backend.in_train_phase(tf.nn.dropout(shadowState,self.recurrentDropout), shadowState, training=training)\n')  
        self.model_file.write('\t\tif self.usePeephole:\n')  
        self.model_file.write('\t\t\tif self.useBias:\n')
        self.model_file.write('\t\t\t\tinputGateOutput = self.activation.inputGate(tf.matmul(shadowState,self.inputGateRecurrentKernelBackward) + tf.matmul(inputs,self.inputGateKernelBackward) + tf.matmul(memoryCell,self.inputGatePeepholeKernelBackward)  + self.inputGateBiasBackward)\n') 
        self.model_file.write('\t\t\t\tforgetGateOutput = self.activation.forgetGate(tf.matmul(shadowState,self.forgetGateRecurrentKernelBackward) + tf.matmul(inputs,self.forgetGateKernelBackward) + tf.matmul(memoryCell,self.forgetGatePeepholeKernelBackward)  + self.forgetGateBiasBackward)\n')
        self.model_file.write('\t\t\t\tmemoryCellCandidate = self.activation.memory(tf.matmul(shadowState,self.memoryRecurrentKernelBackward) + tf.matmul(inputs,self.memoryKernelBackward) + self.memoryBiasBackward)\n')
        self.model_file.write('\t\t\t\tnewMemoryCell = tf.multiply(forgetGateOutput,memoryCell) + tf.multiply(inputGateOutput,memoryCellCandidate)\n')
        self.model_file.write('\t\t\t\tif self.memoryCellClip is not None: newMemoryCell = tf.clip_by_value(newMemoryCell,-1*self.memoryCellClip,self.memoryCellClip)\n')
        self.model_file.write('\t\t\t\toutputGateOutput = self.activation.outputGate(tf.matmul(shadowState,self.outputGateRecurrentKernelBackward) + tf.matmul(inputs,self.outputGateKernelBackward) + tf.matmul(newMemoryCell,self.outputGatePeepholeKernelBackward) + self.outputGateBiasBackward)\n') 
        self.model_file.write('\t\t\telse:\n')
        self.model_file.write('\t\t\t\tinputGateOutput = self.activation.inputGate(tf.matmul(shadowState,self.inputGateRecurrentKernelBackward) + tf.matmul(inputs,self.inputGateKernelBackward) + tf.matmul(memoryCell,self.inputGatePeepholeKernelBackward))\n')
        self.model_file.write('\t\t\t\tforgetGateOutput = self.activation.forgetGate(tf.matmul(shadowState,self.forgetGateRecurrentKernelBackward) + tf.matmul(inputs,self.forgetGateKernelBackward) + tf.matmul(memoryCell,self.forgetGatePeepholeKernelBackward))\n')
        self.model_file.write('\t\t\t\tmemoryCellCandidate = self.activation.memory(tf.matmul(shadowState,self.memoryRecurrentKernelBackward) + tf.matmul(inputs,self.memoryKernelBackward))\n')
        self.model_file.write('\t\t\t\tnewMemoryCell = tf.multiply(forgetGateOutput,memoryCell) + tf.multiply(inputGateOutput,memoryCellCandidate)\n')
        self.model_file.write('\t\t\t\tif self.memoryCellClip is not None: newMemoryCell = tf.clip_by_value(newMemoryCell,-1*self.memoryCellClip,self.memoryCellClip)\n')
        self.model_file.write('\t\t\t\toutputGateOutput = self.activation.outputGate(tf.matmul(shadowState,self.outputGateRecurrentKernelBackward) + tf.matmul(inputs,self.outputGateKernelBackward) + tf.matmul(newMemoryCell,self.outputGatePeepholeKernelBackward))\n')
        self.model_file.write('\t\telse:\n')  
        self.model_file.write('\t\t\tif self.useBias:\n')
        self.model_file.write('\t\t\t\tinputGateOutput = self.activation.inputGate(tf.matmul(shadowState,self.inputGateRecurrentKernelBackward) + tf.matmul(inputs,self.inputGateKernelBackward)  + self.inputGateBiasBackward)\n')
        self.model_file.write('\t\t\t\tforgetGateOutput = self.activation.forgetGate(tf.matmul(shadowState,self.forgetGateRecurrentKernelBackward) + tf.matmul(inputs,self.forgetGateKernelBackward) + self.forgetGateBiasBackward)\n')
        self.model_file.write('\t\t\t\toutputGateOutput = self.activation.outputGate(tf.matmul(shadowState,self.outputGateRecurrentKernelBackward) + tf.matmul(inputs,self.outputGateKernelBackward) + self.outputGateBiasBackward)\n')
        self.model_file.write('\t\t\t\tmemoryCellCandidate = self.activation.memory(tf.matmul(shadowState,self.memoryRecurrentKernelBackward) + tf.matmul(inputs,self.memoryKernelBackward) + self.memoryBiasBackward)\n')
        self.model_file.write('\t\t\t\tnewMemoryCell = tf.multiply(forgetGateOutput,memoryCell) + tf.multiply(inputGateOutput,memoryCellCandidate)\n')
        self.model_file.write('\t\t\telse:\n')
        self.model_file.write('\t\t\t\tinputGateOutput = self.activation.inputGate(tf.matmul(shadowState,self.inputGateRecurrentKernelBackward) + tf.matmul(inputs,self.inputGateKernelBackward))\n')
        self.model_file.write('\t\t\t\tforgetGateOutput = self.activation.forgetGate(tf.matmul(shadowState,self.forgetGateRecurrentKernelBackward) + tf.matmul(inputs,self.forgetGateKernelBackward))\n')
        self.model_file.write('\t\t\t\toutputGateOutput = self.activation.outputGate(tf.matmul(shadowState,self.outputGateRecurrentKernelBackward) + tf.matmul(inputs,self.outputGateKernelBackward))\n')
        self.model_file.write('\t\t\t\tmemoryCellCandidate = self.activation.memory(tf.matmul(shadowState,self.memoryRecurrentKernelBackward) + tf.matmul(inputs,self.memoryKernelBackward)) \n')
        self.model_file.write('\t\t\t\tnewMemoryCell = tf.multiply(forgetGateOutput,memoryCell) + tf.multiply(inputGateOutput,memoryCellCandidate)\n')
        self.model_file.write('\t\t\tif self.memoryCellClip is not None: newMemoryCell = tf.clip_by_value(newMemoryCell,-1*self.memoryCellClip,self.memoryCellClip)\n')
        self.model_file.write('\t\tnewShadowState = tf.multiply(outputGateOutput,self.activation.shadowGate(newMemoryCell))\n') 
        self.model_file.write('\t\toutput = newShadowState\n')
        self.model_file.write('\t\tif self.shadowStateClip is not None: newShadowState = tf.clip_by_value(newShadowState,-1*self.shadowStateClip,self.shadowStateClip)\n') 
        self.model_file.write('\t\treturn output,(newMemoryCell,newShadowState)\n\n') 
        #_switchBatchTime 
        self.model_file.write('\tdef _switchBatchTime(self,inputs):\n')
        self.model_file.write('\t\tperm = list(range(inputs.get_shape().rank))\n') 
        self.model_file.write('\t\tperm[0:2] = (1,0)\n') 
        self.model_file.write('\t\treturn tf.transpose(inputs,perm)\n\n') 
        #_getInitialState
        self.model_file.write('\tdef _getInitialState(self,inputs):\n') 
        self.model_file.write('\t\tinitialState = tf.zeros_like(inputs)\n')          
        self.model_file.write('\t\tinitialState = initialState[:,0,0:1]\n') 
        self.model_file.write('\t\tmultiples = tf.convert_to_tensor([1,self.units])\n') 
        self.model_file.write('\t\tinitialState = tf.tile(initialState,multiples)\n') 
        self.model_file.write('\t\treturn initialState\n\n') 
        #call
        self.model_file.write('\tdef call(self, inputs ,state = None):\n') 
        self.model_file.write('\t\tbatchSize,timeSteps,_ = inputs.shape.as_list()\n') 
        self.model_file.write('\t\ttimeStepsTensor = tf.shape(inputs)[1]\n')
        self.model_file.write('\t\tinitialMemoryCellForward,initialShadowStateForward = self._initialMemoryCellForward, self._initialShadowStateForward\n')
        self.model_file.write('\t\tinitialMemoryCellBackward,initialShadowStateBackward = self._initialMemoryCellBackward,self._initialShadowStateBackward\n')
        self.model_file.write('\t\tif initialMemoryCellForward is None: initialMemoryCellForward = self._getInitialState(inputs)\n')
        self.model_file.write('\t\tif initialShadowStateForward is None: initialShadowStateForward = self._getInitialState(inputs)\n')
        self.model_file.write('\t\tif initialMemoryCellBackward is None: initialMemoryCellBackward = self._getInitialState(inputs)\n')
        self.model_file.write('\t\tif initialShadowStateBackward is None: initialShadowStateBackward = self._getInitialState(inputs)\n')
        self.model_file.write('\t\tinputs = self._switchBatchTime(inputs)\n')
        self.model_file.write('\t\tinitialTime = tf.zeros_like(timeStepsTensor,name=\'time\')\n')
        self.model_file.write('\t\tinputsTensorArrayForward = tf.TensorArray(dtype=inputs.dtype,size=timeStepsTensor)\n')
        self.model_file.write('\t\tinputsTensorArrayBackward = tf.TensorArray(dtype=inputs.dtype,size=timeStepsTensor)\n')
        self.model_file.write('\t\tinputsTensorArrayForward = inputsTensorArrayForward.unstack(inputs)\n')
        self.model_file.write('\t\tinputsTensorArrayBackward = inputsTensorArrayBackward.unstack(tf.reverse(inputs,tf.zeros(1, tf.int32)))\n') 
        self.model_file.write('\t\tinitialOutputTensorArrayForward = tf.TensorArray(dtype=inputs.dtype,size=timeStepsTensor)\n\n')
        self.model_file.write('\t\tinitialOutputTensorArrayBackward = tf.TensorArray(dtype=inputs.dtype,size=timeStepsTensor)\n\n')
        self.model_file.write('\t\tdef _cond(time,outputTensorArray,memoryCell,shadowState):\n')
        self.model_file.write('\t\t\treturn time<timeStepsTensor\n\n')
        self.model_file.write('\t\tdef _bodyForward(time,outputTensorArray,memoryCell,shadowState):\n')
        self.model_file.write('\t\t\tcurrentInput = inputsTensorArrayForward.read(time)\n')
        self.model_file.write('\t\t\toutput,(newMemoryCell,newShadowState) = self._stepFunctionForward(currentInput,(memoryCell,shadowState))\n')
        self.model_file.write('\t\t\toutputTensorArray = outputTensorArray.write(time,output)\n')
        self.model_file.write('\t\t\treturn (time+1,outputTensorArray,newMemoryCell,newShadowState)\n\n')
        self.model_file.write('\t\tdef _bodyBackward(time,outputTensorArray,memoryCell,shadowState):\n')
        self.model_file.write('\t\t\tcurrentInput = inputsTensorArrayBackward.read(time)\n')
        self.model_file.write('\t\t\toutput,(newMemoryCell,newShadowState) = self._stepFunctionBackward(currentInput,(memoryCell,shadowState))\n')
        self.model_file.write('\t\t\toutputTensorArray = outputTensorArray.write(time,output)\n')
        self.model_file.write('\t\t\treturn (time+1,outputTensorArray,newMemoryCell,newShadowState)\n\n')
        self.model_file.write('\t\tfinalOutputsForward = tf.while_loop(cond = _cond,body=_bodyForward,loop_vars=(initialTime,initialOutputTensorArrayForward,initialMemoryCellForward,initialShadowStateForward))\n')
        self.model_file.write('\t\tfinalOutputsBackward = tf.while_loop(cond = _cond,body=_bodyBackward,loop_vars=(initialTime,initialOutputTensorArrayBackward,initialMemoryCellBackward,initialShadowStateBackward))\n')
        self.model_file.write('\t\tfinalOutputTensorArrayForward = finalOutputsForward[1]\n')
        self.model_file.write('\t\tfinalOutputTensorArrayBackward = finalOutputsBackward[1]\n')
        self.model_file.write('\t\tfinalOutputForward = finalOutputTensorArrayForward.stack()\n') 
        self.model_file.write('\t\tfinalOutputBackward = finalOutputTensorArrayBackward.stack()\n') 
        self.model_file.write('\t\tfinalOutputForward.set_shape([timeSteps,batchSize,self.units])\n')
        self.model_file.write('\t\tfinalOutputBackward.set_shape([timeSteps,batchSize,self.units])\n')
        self.model_file.write('\t\tif self.statefull : self._initialStatForwarde,self._initialStateBackward = finalOutputsForward[-1],finalOutputsBackward[-1]\n')
        self.model_file.write('\t\tfinalOutputForward = self._switchBatchTime(finalOutputForward)\n')
        self.model_file.write('\t\tfinalOutputBackward = self._switchBatchTime(finalOutputBackward)\n')
        self.model_file.write('\t\tif self.mergeMode == \'concat\': mergeOutput = tf.concat([finalOutputForward, finalOutputBackward],-1)\n')
        self.model_file.write('\t\telif self.mergeMode == \'sum\': mergeOutput = finalOutputForward + finalOutputBackward\n')
        self.model_file.write('\t\telif self.mergeMode == \'ave\': mergeOutput = (finalOutputForward + finalOutputBackward) / 2\n')
        self.model_file.write('\t\telif self.mergeMode == \'mul\': mergeOutput = finalOutputForward * finalOutputBackward\n')
        self.model_file.write('\t\telif self.mergeMode is None: mergeOutput = [finalOutputForward, finalOutputBackward]\n')
        self.model_file.write('\t\telse: raise ValueError(\'mergeMode wrong\')\n')
        self.model_file.write('\t\treturn mergeOutput\n\n')
        #compute_output_shape
        self.model_file.write('\tdef compute_output_shape(self,input_shape):\n')
        self.model_file.write('\t\tbatchSize,timeSteps,dim = input_shape\n')
        self.model_file.write('\t\toutputShape = tf.TensorShape([batchSize,timeSteps,self.units*2]) if self.mergeMode == \'concat\' else tf.TensorShape([batchSize,timeSteps,self.units])\n')
        self.model_file.write('\t\tif self.mergeMode is None : return [outputShape,outputShape]\n')
        self.model_file.write('\t\treturn outputShape\n\n')
        #get_config
        self.model_file.write('\tdef get_config(self):\n')
        self.model_file.write('\t\tconfig = {\'units\': self.units,\'activation\' : self.activation,\'kernelInitializer\' : self.kernelInitializer,\'recurrentInitializer\' : self.recurrentInitializer,\n')
        self.model_file.write('\t\t\t\t\t\'peepholeInitializer\' : self.peepholeInitializer,\'biasInitializer\' : self.biasInitializer,\'kernelRegularizer\' : self.kernelRegularizer,\n')
        self.model_file.write('\t\t\t\t\t\'recurrentRegularizer\' : self.recurrentRegularizer,\'peepholeRegularizer\' : self.peepholeRegularizer,\'biasRegularizer\' : self.biasRegularizer,\n')
        self.model_file.write('\t\t\t\t\t\'useBias\': self.useBias,\'usePeephole\' : self.usePeephole,\'dropout\': self.dropout,\'recurrentDropout\': self.recurrentDropout,\'memoryCellClip\' : self.memoryCellClip,\n')
        self.model_file.write('\t\t\t\t\t\'shadowStateClip\' : self.shadowStateClip,\'statefull\' : self.statefull,\'mergeMode\' : self.mergeMode}\n')
        self.model_file.write('\t\tbaseConfig = super(BiLSTM, self).get_config()\n')
        self.model_file.write('\t\treturn dict(list(baseConfig.items()) + list(config.items()))\n')

    def write_init(self, buildNo = 0):
        '''
			write __init__ function to keras_model.py.   --- UPDATED (Kai Hsiang) 20190717

            Parameters
            ------------------------------

            optimizer   `str`       - The optimizer name.

            buildNo         `int`   - The build no.
        '''
        for obj in self.layer_obj:
            self.model_file.write('\t\t%s\n'%(obj))
        optimizer_dict = {'adam':'tf.keras.optimizers.Adam',
                          'rmsprop':'tf.keras.optimizers.RMSprop',
                          'grad':'tf.keras.optimizers.SGD',
                          'momentum':'tf.compat.v1.train.MomentumOptimizer',
                          'adagrad':'tf.keras.optimizers.Adagrad',
                          'adadelta':'tf.keras.optimizers.Adadelta',
                          'ftrl':'tf.keras.optimizers.Ftrl'}

        '''if optimizer == 'momentum':
            self.model_file.write('\t\tself.optimizer = ' + '%s(self.get_lr,momentum=0.9)'%(optimizer_dict[optimizer]) + '\n')
        else:
            try:
                self.model_file.write('\t\tself.optimizer = ' + '%s(self.get_lr)'%(optimizer_dict[optimizer]) + '\n')
            except:
                self.model_file.write('\t\tself.optimizer = ' + '%s(self.get_lr)'%(optimizer_dict['adam']) + '\n')
        '''
        self.model_file.write('\n')

    def write_call(self):
        '''
			write call function to keras_model.py.   --- UPDATED (Kai Hsiang) 20190717
        '''
        for obj in self.call_obj:
            self.model_file.write('\t\t%s\n'%(obj))
        self.model_file.write('\t\treturn [' + ','.join(self.return_obj) + ']')
        self.model_file.write('\n')

    def write_loss(self):
        '''
			write loss function to keras_model.py.   --- UPDATED (Kai Hsiang) 20190717
        '''
        for obj in self.loss_obj:
            self.model_file.write('\t\t' + obj + '\n')
        # self.model_file.write('\t\t' + self.loss_obj + '\n')
        # self.loss_var = list(map(lambda x: x.split(' = ')[0], self.loss_obj))
        # self.loss_var = list(map(lambda x: 'tf.reduce_mean(' + x + ')', self.loss_var))
        # self.loss_var.append('tf.reduce_sum(self.losses)')
        self.model_file.write('\t\treturn ' + ' + '.join(self.loss_ret) + '\n')

        '''self.model_file.write('\t\tself.total_loss = ' + '+'.join(self.return_loss) + '\n')
        self.model_file.write('\t\treturn ' + 'self.total_loss' + '\n')'''
    
    def write_optimizer(self):
        '''
			write optimizer function to keras_model.py.   --- UPDATED (Kai Hsiang) 20190717
        '''            
        for obj in self.update_obj:
            self.model_file.write('\t\t%s\n'%(obj))
        '''self.model_file.write('\t\tgrads = tape.gradient(self.total_loss, self.trainable_variables)\n')
        self.model_file.write('\t\tself.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n')
        '''
    
    def keras_fullModelTrain(self):
        '''
			Perform a full keras model train with multi-build settings.   --- UPDATED (Kai Hsiang) 20190717
        '''

        # If there is a restore path, find the from-globaleStep.   --- BETA
        if self.restorePath:
            if any([tp.crossValidationType is not None for tp in self.buildConfigs]):
                raise ValueError("Train Resuming is not allowed for cross-validating trainings.")
            
            step = 0
            try:
                with open(self.restorePath + '/tmpStep.txt', "r", encoding = "utf-8", newline="") as f:
                    step = f.read().trim()
            except:
                raise ValueError("No model is restored from: " + str(self.restorePath))
                
            self.localStep = self.globalStep = int(step)

        # Train the model for every training profiles, loop by each run.
        for runNo in range(0, self.runCount):
            self._runNo += 1
            self.globalStep = 0

            # Loop by each build.
            for toBuildNo,x in enumerate(self.buildConfigs):
                if (x.crossValidationType is None or x.validationRuns == 1):

                    self._buildNo = toBuildNo
                    self.keras_train(buildNo = toBuildNo)
                else:
                    # Create temporary global step.
                    startGlobalStep = self.globalStep

                    # Loop by each cross validation and overall performance.
                    for cv in range(0, x.validationRuns+1):
                        self.globalStep = startGlobalStep
                        self._cvNo += 1
                        if (self._cvNo == x.validationRuns):
                            self._cvNo = -1
                        self._buildNo = toBuildNo
                        self.keras_train(buildNo = toBuildNo)
        
        # Reset Run No.
        self._runNo = -1

    def keras_fullModelTrain2(self):
        '''
			Perform a full keras model train with multi-build settings.   --- UPDATED (Kai Hsiang, Dexter) 20190717
        '''

        # If there is a restore path, find the from-globaleStep.   --- BETA
        if self.restorePath:
            if any([tp.crossValidationType is not None for tp in self.buildConfigs]):
                raise ValueError("Train Resuming is not allowed for cross-validating trainings.")
            
            step = 0
            try:
                with open(self.restorePath + '/tmpStep.txt', "r", encoding = "utf-8", newline="") as f:
                    step = f.read().trim()
            except:
                raise ValueError("No model is restored from: " + str(self.restorePath))
                
            self.localStep = self.globalStep = int(step)

        # Train the model for every training profiles, loop by each run.
        for runNo in range(0, self.runCount):
            self._runNo += 1
            self.globalStep = 0

            # Loop by each build.
            for toBuildNo,x in enumerate(self.buildConfigs):
                validationRuns = x.validationRuns
                if (x.crossValidationType is None or validationRuns == 1):
                    
                    self.keras_train2(buildNo = toBuildNo)
                else:
                    # Create temporary global step.
                    startGlobalStep = self.globalStep

                    # Loop by each cross validation and overall performance.
                    for cv in range(0, validationRuns+1):
                        self.globalStep = startGlobalStep
                        self._cvNo += 1
                        if (self._cvNo == validationRuns):
                            self._cvNo = -1
                        self.keras_train2(buildNo = toBuildNo)
        
        # Reset Run No.
        self._runNo = -1

    def keras_train(self, buildNo = 0):
        '''
			Train keras model.   --- UPDATED (Kai Hsiang) 20200115
        
            Parameters
            ------------------------------

            buildNo    `int`   - The build number of this training build.
        '''
        ### Create the results folder.
        os.makedirs(self.folder + "outputLogs/" + self.trainTime + "/", exist_ok=True)
        os.makedirs(self.folder + "builds/" + self.trainTime + "/", exist_ok=True)
        os.makedirs(self.folder + "tmpLogs/" + self.trainTime + "/", exist_ok=True)
        
        #   1.  Collect current training profile.
        nowBuildConfig = self.buildConfigs[buildNo]
        optimizer = nowBuildConfig.optimizer
        
        # create keras_model file
        self.connect_keras_layer()
        self.create_model_file(optimizer, buildNo = 0)
        from keras_model import eagerModel
        self.k_model = eagerModel()
        
        
        #   2.  Basic Step Counting
        primaryTrainingSource = self.sources[0]
        coreBatchSize = primaryTrainingSource.batchSize
        batchCountPerEpoch = max(primaryTrainingSource.epochSize // coreBatchSize, 1)
        print(batchCountPerEpoch)
        if nowBuildConfig.numEpochsPerDecay is not None:
            decaySteps = int(batchCountPerEpoch * nowBuildConfig.numEpochsPerDecay)
        
        #   3. Validation Setup
        validationType = nowBuildConfig.crossValidationType
        isCrossVal = validationType is not None and self._cvNo >= 0
        if (isCrossVal):
            if (nowBuildConfig._cvCount < nowBuildConfig.validationRuns):
                self._nextCrossValidation(nowBuildConfig)
                nowBuildConfig._cvCount += 1
        elif (nowBuildConfig._cvCount == nowBuildConfig.validationRuns):
            nowBuildConfig._cvCount = 0
        
        #   4.  Start Train Looping
        finalStep = nowBuildConfig.noOfEpoch * batchCountPerEpoch

        #   5A.  No actions to be taken if the current local step is greater than the final step (usually from a recovered state)
        if (self.localStep >= finalStep):
            self.localStep -= finalStep

        #   5B. Continue for training if the local step is within final step.
        else:
            #   6.  Build the model if it has not built yet.

            #   7.  Fire trainbuild event
                #   8.  Define the learning rate
            '''if nowBuildConfig.learningRateDecayFactor is not None and (nowBuildConfig.learningRateDecayFactor != 1 and nowBuildConfig.learningRateDecayFactor > 0):
                learningRate = tf.train.exponential_decay(nowBuildConfig.initialLearningRate, self.localStepTensor, decaySteps, nowBuildConfig.learningRateDecayFactor, staircase=True)
            else:
                learningRate = tf.constant(nowBuildConfig.initialLearningRate)'''
                
                #   9A.  Collect all the losses.

                #   9B. Apply moving average loss decay.
                # not implement

                #   10.  Training Operation
                
                #   11. Update Global Step
                
                
                #   12. Take Moving AVerage Decay on Trainable Variables
                # not implement
                
                #   13. Grab the final training operation
                
            #   14. Create session
            
            #   15. Restore previous training states or previous build
            '''sess.run(tf.global_variables_initializer())'''
            print("Training Initialized.\n")
            '''if (self.restorePath):
                self._restoreHistory(self.restorePath)
            elif (self._buildNo > 0):
                self._restoreBuild()'''
            
            #   16. Fire trainstart event
            recorder = TimeHelper.Recorder()
            recorderStep = self.localStep-1
            '''tb = TrainEvent("trainstart", {"target": self, "buildNo": self._buildNo, "runNo": self._runNo, "cvNo": self._cvNo, "buildConfig": nowBuildConfig})
            self.dispatchEvent(tb)'''
            
            # call model to get weight name
            self.localStep += 1
            self.globalStep += 1
            batch_input = self.get_batch_data()
            with tf.GradientTape() as tape:
                self.k_model(batch_input, training=True)
                loss_value = self.k_model.loss_value()
                self.k_model.update(tape)
            weight_names = []
            for layer in self.k_model.layers:
                for w in layer.weights:
                    weight_names.append(w.name)

            #   17. Prepare Train Log and Test Log
            toLog = True if self.logFreq > 0 and self.logFreq <= math.inf else False
            #print(self.logFreq)
            if (toLog):
                trainLogger = CSVLogger(self.folder+"outputLogs/"+self.trainTime+"/", "trainLog_"+str(self._buildNo), ["Timestamp", "Run", "Cross Validation Step", "Global Step", "Local Step", "Total Loss", "Average Loss", "Learning Rate", "Examples per Second", "Seconds per Step"])
            
            toTest = True if self.testFreq > 0 and self.testFreq <= math.inf else False
            if (toTest):
            #if True:
                self.testLogger = CSVLogger(self.folder+"outputLogs/"+self.trainTime+"/", "testLog_"+str(self._buildNo), ["Timestamp", "Run", "Cross Validation Step", "Global Step", "Local Step", "Test Type", *[(l.name + ": " + l.measurement) for l in self.getFinalNodes(buildNo=buildNo)]])
            
            toLogWeight = True if self.weightLogFreq > 0 and self.weightLogFreq <= math.inf else False
            if (toLogWeight):
                weightLogger = CSVLogger(self.folder+"outputLogs/"+self.trainTime+"/", "weightLog_"+str(self._buildNo), ["Timestamp", "Run", "Cross Validation Step", "Global Step", "Local Step", *weight_names])
            toLogTrace = True if self.traceFreq > 0 and self.traceFreq <= math.inf else False
            if (toLogTrace):
                traceLogger = CSVLogger(self.folder+"outputLogs/"+self.trainTime+"/","traceLog_"+str(self._buildNo), ["Timestamp", "Run", "Cross Validation Step", "Global Step", "Local Step", "Task Name", "Task Type", "Source ID", "Source Config", "Item ID", "Data Type", "Data"])
                self._initTraceItems(traceLogger, buildNo = buildNo)

            #   misc. cache
            '''toFireEpochStart = len(self._eventFtns["epochstart"])
            toFireEpochEnd = len(self._eventFtns["epochend"])
            toFireStepPrepare = len(self._eventFtns["stepprepare"])
            toFireStepStart = len(self._eventFtns["stepstart"])
            toFireStepEnd = len(self._eventFtns["stepend"])'''
            for i in range(self.localStep, finalStep):
                self.localStep += 1
                self.globalStep += 1
                ii = self.localStep
                #   18E.  Print debugging tensors
       
                #   18F.  Run the loss function
                '''[totalLoss, avgLoss] =  sess.run([totalLossOP, avgLossOP], feed_dict=feedDictObj)'''
                batch_input = self.get_batch_data()

                if nowBuildConfig.learningRateDecayFactor is None:
                    logLR = nowBuildConfig.initialLearningRate
                else:
                    logLR = nowBuildConfig.initialLearningRate * nowBuildConfig.learningRateDecayFactor**(self.localStep//decaySteps)
                    self.k_model.lr = logLR
                             
                with tf.GradientTape() as tape:
                    self.k_model(batch_input, training=True)
                    loss_value = self.k_model.loss_value()
                    self.k_model.update(tape)
                    
                #   18G.  Log if needed
                if (toLog and (ii%self.logFreq == 0 or i == finalStep - 1)):
                    duration = recorder.logAndRestart()
                    nowTime = datetime.now()
                    examplesPerSec = self.logFreq * self.sources[0].batchSize / duration
                    secPerStep = float(duration / (self.localStep - recorderStep))
                    recorderStep = self.localStep
                    avgLoss = loss_value.numpy()
                    totalLoss = avgLoss*coreBatchSize
                    trainLogger.logAndSave([nowTime, self._runNo, self._cvNo, self.globalStep, ii, escapeNaN(totalLoss), escapeNaN(avgLoss), logLR, examplesPerSec, secPerStep])
                    if (self._cvNo == -1):
                        print('%s: Run #%d, Step %d (%d) --- loss: %f; learning rate: %f; %.1f examples/s; %.3f s/step' % (nowTime, self._runNo, self.globalStep, ii, avgLoss, logLR, examplesPerSec, secPerStep))
                    else:
                        print('%s: Run #%d - Cross Validation %d, Step %d (%d) --- loss: %f; learning rate: %f; %.1f examples/s; %.3f s/step' % (nowTime, self._runNo, self._cvNo, self.globalStep, ii, avgLoss, logLR, examplesPerSec, secPerStep))
                    
                #   18H.  Print Weights if needed
                if (toLogWeight and (ii%self.weightLogFreq == 0 or ii == finalStep)):
                    nowTime = datetime.now()
                    weight_list = []
                    for layer in self.k_model.layers:
                        for w in layer.weights:
                            w = w.numpy()
                            w[np.isnan(w)] = None
                            weight_list.append(w.tolist())
                    weightLogger.logAndSave([nowTime, self._runNo, self._cvNo, self.globalStep, ii, *weight_list])

                #   18I.  Save if needed
                if (self.saveFreq > 0 and (ii%self.saveFreq == 0 or ii == finalStep)):
                    #   18I-1.  Save the trained graph.
                    #self._saveTempTrain()
                    
                    #   18-2.  Flush Training Log
                    if (toLog):
                        trainLogger.flush()
                
                #   18J.  Log trace items if needed
                if (toLogTrace and (ii % self.traceFreq == 0 or ii == finalStep)):
                    #   10J-1.  Predict acoording to the saved model.
                    predictedValues = self.keras_predict(x=self.traceItems, buildNo = buildNo)
                    nowTime = datetime.now()
                    for l in self.getFinalNodes(buildNo = buildNo):
                        predictedItems = l.getTraceItems(predictedValues, buildNo = buildNo)
                        if predictedItems is not None:
                            traceLogger.log(*[[nowTime, self._runNo, self._cvNo, self.globalStep, self.localStep, l.name, l.__class__.__name__, *escapeNaNList(pi)] for pi in predictedItems])
                    traceLogger.flush()
                
                #   18K.  Test if needed, using cross validation data as testing if needed
                if (toTest and (ii%self.testFreq == 0 or ii == finalStep)):
                #if True:
                    #   18K-1.  Evaluate acoording to the saved model.
                    self.keras_evaluate(validation = isCrossVal, event="Test Log", buildNo = buildNo)
                #   18L.  Fire stepend event
                
                #   18M.  Fire epochend event
            
            # 19B. Flush Training Log
            '''if (toLog):
                trainLogger.flush()'''

            # 20. Save current training graph   --- BETA
            '''with self._graph.as_default():
                tmpSaver = tf.train.Saver()
                tmpSaver.save(sess, self.folder + 'tmpLogs/' + self.trainTime + '/tmpLog.ckpt')
                tmpSaver.save(sess, self.folder + 'builds/' + self.trainTime + '/build_' + str(self._buildNo) + '/buildLog.ckpt')
            '''
            # 21. Dispatch trainend event.
            
            # 21. Close and reset TensorFlow training graph
            self.close()

    def keras_train2(self, buildNo = 0):
        '''
			Train keras model.   --- UPDATED (Kai Hsiang, Dexter) 20200501
        
            Parameters
            ------------------------------

            buildNo    `int`   - The build number of this training build.
        '''
        ### Create the results folder.
        os.makedirs(self.folder + "outputLogs/" + self.trainTime + "/", exist_ok=True)
        os.makedirs(self.folder + "builds/" + self.trainTime + "/", exist_ok=True)
        os.makedirs(self.folder + "tmpLogs/" + self.trainTime + "/", exist_ok=True)
        
        #   1. Cache the root source linkages
        #   1-a. Get the actively-using data preprocessing nodes.
        nowTrainBuildConfig: Train.BuildConfig = self.currentBuildConfig
        # optimizer = nowTrainBuildConfig.optimizer
        activeDppNodes: List[DataPreprocessing.Node.Config] = self.getDataPreprocessingNodesUsedByLayers()
        
        #   1-b. Get the root sources with unique and used sources.
        uniqueSources: set(Source.Config) = set()
        for dppNode in activeDppNodes:
            for s in dppNode.getRootSources(rawGraph = False):
                uniqueSources.add(s)
        self._rootSources = Train.RootSources(self, [*uniqueSources])

        #   1-c. Split Test Datasets
        if (self.testDatasetType == DataGenerator.Dataset.TestSource.Split and self.testRatio > 0):
            self.rootSources.splitTestDataset(self.testRatio, self.testShuffle)
            self.testRatio = 0
        
        #   1-d. create keras_model file
        self.connect_keras_layer()
        self.create_model_file(buildNo = 0)
        
        from keras_model import eagerModel
        self.k_model = eagerModel()

        # get learning rate config(s)
        learning_list = list(self.k_model.learning_list.values())
        
        #   2. Validation Setup
        validationType: str = nowTrainBuildConfig.crossValidationType
        isCrossVal: bool = validationType is not None and self._cvNo >= 0
        validationRuns: int = nowTrainBuildConfig.validationRuns
        if (isCrossVal):
            if (nowTrainBuildConfig._cvCount < validationRuns):
                self.rootSources.nextCrossValidation(nowTrainBuildConfig)
                nowTrainBuildConfig._cvCount += 1
                self.assignCurrentSourceDataset(DataGenerator.Dataset.Types.ValidationTrain)
            elif (nowTrainBuildConfig._cvCount == validationRuns):
                nowTrainBuildConfig._cvCount = 0
                self.assignCurrentSourceDataset(DataGenerator.Dataset.Types.Train)
        else:
            self.assignCurrentSourceDataset(DataGenerator.Dataset.Types.Train)
        
        #   3.  Basic Step Counting
        batchCountPerEpoch: int = self.rootSources.batchCountPerEpoch
        # decaySteps: int = int(batchCountPerEpoch * nowTrainBuildConfig.numEpochsPerDecay) if (nowTrainBuildConfig.numEpochsPerDecay is not None) else None
        
        decay_steps = []
        for index, learning_config in enumerate(learning_list):
            num_epochs_per_decay = 0 if (learning_config['numEpochsPerDecay'] is None) else learning_config['numEpochsPerDecay']
            decay_steps.append(int(batchCountPerEpoch * int(num_epochs_per_decay)) if (learning_config['learningRateDecay'] == True) else None)
        
        #   4.  Start Train Looping
        finalStep: int = nowTrainBuildConfig.noOfEpoch * batchCountPerEpoch

        #   5A.  No actions to be taken if the current local step is greater than the final step (usually from a recovered state)
        if (self.localStep >= finalStep):
            self.localStep -= finalStep

        #   5B. Continue for training if the local step is within final step.
        else:
            #   6.  Build the model if it has not built yet.
            self._buildNo = buildNo

            #   7.  Fire trainbuild event
                #   8.  Define the learning rate
            '''if nowTrainBuildConfig.learningRateDecayFactor is not None and (nowTrainBuildConfig.learningRateDecayFactor != 1 and nowTrainBuildConfig.learningRateDecayFactor > 0):
                learningRate = tf.train.exponential_decay(nowTrainBuildConfig.initialLearningRate, self.localStepTensor, decaySteps, nowTrainBuildConfig.learningRateDecayFactor, staircase=True)
            else:
                learningRate = tf.constant(nowTrainBuildConfig.initialLearningRate)'''
                
                #   9A.  Collect all the losses.

                #   9B. Apply moving average loss decay.
                # not implement

                #   10.  Training Operation
                
                #   11. Update Global Step
                
                #   12. Take Moving AVerage Decay on Trainable Variables
                # not implement
                
                #   13. Grab the final training operation
                
            #   14. Create session
            
            #   15. Restore previous training states or previous build
            '''sess.run(tf.global_variables_initializer())'''
            print("Training Initialized.\n")
            '''if (self.restorePath):
                self._restoreHistory(self.restorePath)
            elif (self._buildNo > 0):
                self._restoreBuild()'''
            
            #   16. Fire trainstart event
            recorder = TimeHelper.Recorder()
            recorderStep = self.localStep-1
            '''tb = TrainEvent("trainstart", {"target": self, "buildNo": self._buildNo, "runNo": self._runNo, "cvNo": self._cvNo, "buildConfig": nowTrainBuildConfig})
            self.dispatchEvent(tb)'''
            
            # call model to get weight name
            self.localStep += 1
            self.globalStep += 1
            batch_input = next(self)

            for order in self.k_model.learning_list.keys():
                with tf.GradientTape() as tape:
                    self.k_model(batch_input, training=True)
                    loss_value = self.k_model.loss_value()
                    self.k_model.update(tape, order)

            weight_names = []

            # TODO: Check l.weightLogging
            for layer in self.k_model.layers:
                for w in layer.weights:
                    weight_names.append(w.name)

            #   17. Prepare Train Log and Test Log
            toLog: bool = True if self.logFreq > 0 and self.logFreq <= math.inf else False
            if (toLog):
                trainLogger: CSVLogger = CSVLogger(self.folder+"outputLogs/"+self.trainTime+"/", "trainLog_"+str(self._buildNo), ["Timestamp", "Run", "Cross Validation Step", "Global Step", "Local Step", "Total Loss", "Average Loss", "Learning Rate", "Examples per Second", "Seconds per Step"])
            toTest: bool = True if self.testFreq > 0 and self.testFreq <= math.inf else False
            if (toTest):
                test_list = []
                for l in self.getFinalNodes(buildNo=buildNo):
                    for comparison in l.comparisons:
                        test_list.append((l.name+'_'+comparison["input"]+'_'+comparison["target"] + ": " + comparison["metrics"]))
                self.testLogger: CSVLogger = CSVLogger(self.folder+"outputLogs/"+self.trainTime+"/", "testLog_"+str(self._buildNo), ["Timestamp", "Run", "Cross Validation Step", "Global Step", "Local Step", "Test Type", *test_list])
            toLogWeight: bool = True if self.weightLogFreq > 0 and self.weightLogFreq <= math.inf else False
            if (toLogWeight):
                weightLogger: CSVLogger = CSVLogger(self.folder+"outputLogs/"+self.trainTime+"/", "weightLog_"+str(self._buildNo), ["Timestamp", "Run", "Cross Validation Step", "Global Step", "Local Step", *weight_names])
            toLogTrace: bool = True if self.traceFreq > 0 and self.traceRecord > 0 and self.traceFreq <= math.inf else False
            if (toLogTrace):
                traceLogger: CSVLogger = CSVLogger(self.folder+"outputLogs/"+self.trainTime+"/","traceLog_"+str(self._buildNo), ["Timestamp", "Run", "Cross Validation Step", "Global Step", "Local Step", "Task Name", "Task Type", "Data Preprocessing Node Key", "Item ID", "Data Type", "Data"])
                self._initTraceItems(traceLogger, buildNo = buildNo)
            
            #   misc. cache
            '''toFireEpochStart = len(self._eventFtns["epochstart"])
            toFireEpochEnd = len(self._eventFtns["epochend"])
            toFireStepPrepare = len(self._eventFtns["stepprepare"])
            toFireStepStart = len(self._eventFtns["stepstart"])
            toFireStepEnd = len(self._eventFtns["stepend"])'''
            for i in range(self.localStep, finalStep):
                self.localStep += 1
                self.globalStep += 1
                ii: int = self.localStep
                #   18E.  Print debugging tensors
       
                #   18F.  Run the loss function
                '''[totalLoss, avgLoss] =  sess.run([totalLossOP, avgLossOP], feed_dict=feedDictObj)'''
                batch_input = next(self)
                
                # update learning rate
                for index, learning_config in enumerate(learning_list):
                    if learning_config['learningRateDecay'] == True:
                        init_learning_rate = float(learning_config['initialLearningRate'])
                        learning_decay = float(0) if learning_config['learningRateDecayFactor'] is None else float(learning_config['learningRateDecayFactor'])
                        learning_decay_factor = float((self.localStep) // decay_steps[index])
                        logLR = init_learning_rate * learning_decay ** learning_decay_factor
                        learning_config['learningRate'] = logLR
                        # print(logLR, init_learning_rate, learning_decay, learning_decay_factor, self.localStep, decay_steps[index])
                    else:
                        logLR = learning_config['learningRate']

                for order in self.k_model.learning_list.keys():
                    with tf.GradientTape() as tape:
                        self.k_model(batch_input, training=True)
                        loss_value = self.k_model.loss_value()
                        self.k_model.update(tape, order)
                    
                #   18G.  Log if needed
                if (toLog and (ii%self.logFreq == 0 or i == finalStep - 1)):
                    duration = recorder.logAndRestart()
                    nowTime = datetime.now()
                    examplesPerSec = self.logFreq * self.sources[0].batchSize / duration
                    secPerStep = float(duration / (self.localStep - recorderStep))
                    recorderStep = self.localStep
                    avgLoss = np.average(loss_value.numpy())
                    totalLoss = avgLoss * nowTrainBuildConfig.batchSize

                    # print('loss_value', loss_value, 'total_loss', totalLoss, 'sum_loss:', np.sum(loss_value.numpy()))
                    
                    trainLogger.logAndSave([nowTime, self._runNo, self._cvNo, self.globalStep, ii, escapeNaN(totalLoss), escapeNaN(avgLoss), logLR, examplesPerSec, secPerStep])
                    if (self._cvNo == -1):
                        print('%s: Run #%d, Step %d (%d) --- loss: %f; learning rate: %f; %.1f examples/s; %.3f s/step' % (nowTime, self._runNo, self.globalStep, ii, avgLoss, logLR, examplesPerSec, secPerStep))
                    else:
                        print('%s: Run #%d - Cross Validation %d, Step %d (%d) --- loss: %f; learning rate: %f; %.1f examples/s; %.3f s/step' % (nowTime, self._runNo, self._cvNo, self.globalStep, ii, avgLoss, logLR, examplesPerSec, secPerStep))
                    
                #   18H.  Print Weights if needed
                if (toLogWeight and (ii%self.weightLogFreq == 0 or ii == finalStep)):
                    nowTime = datetime.now()
                    weight_list = []
                    for layer in self.k_model.layers:
                        for w in layer.weights:
                            w = w.numpy()
                            w[np.isnan(w)] = None
                            weight_list.append(w.tolist())
                    weightLogger.logAndSave([nowTime, self._runNo, self._cvNo, self.globalStep, ii, *weight_list])

                #   18I.  Save if needed
                if (self.saveFreq > 0 and (ii%self.saveFreq == 0 or ii == finalStep)):
                    #   18I-1.  Save the trained graph.
                    #self._saveTempTrain()
                    
                    #   18-2.  Flush Training Log
                    if (toLog):
                        trainLogger.flush()
                
                #   18J.  Log trace items if needed
                if (toLogTrace and (ii % self.traceFreq == 0 or ii == finalStep)):
                    #   10J-1.  Predict acoording to the saved model.
                    predictedValues = self.keras_predict(x=self.traceItems, buildNo = buildNo)
                    nowTime = datetime.now()
                    for l in self.getFinalNodes(buildNo = buildNo):
                        for comparison in l.comparisons:
                            if comparison["target"] in self.traceItems:
                                predictedItems = l.getTraceItems(predictedValues, buildNo = buildNo, comparison=comparison)
                                if predictedItems is not None:
                                    traceLogger.log(*[[nowTime, self._runNo, self._cvNo, self.globalStep, self.localStep, l.name+'_'+comparison["input"]+'_'+comparison["target"], l.__class__.__name__, *escapeNaNList(pi)] for pi in predictedItems])
                    traceLogger.flush()
                
                #   18K.  Test if needed, using cross validation data as testing if needed
                if (toTest and (ii%self.testFreq == 0 or ii == finalStep)):
                #if True:
                    #   18K-1.  Evaluate acoording to the saved model.
                    self.keras_evaluate(validation = isCrossVal, event="Test Log", buildNo = buildNo)
                #   18L.  Fire stepend event
                
                #   18M.  Fire epochend event
            
            # 19B. Flush Training Log
            '''if (toLog):
                trainLogger.flush()'''

            # 20. Save current training graph   --- BETA
            '''with self._graph.as_default():
                tmpSaver = tf.train.Saver()
                tmpSaver.save(sess, self.folder + 'tmpLogs/' + self.trainTime + '/tmpLog.ckpt')
                tmpSaver.save(sess, self.folder + 'builds/' + self.trainTime + '/build_' + str(self._buildNo) + '/buildLog.ckpt')
            '''
            # 21. Dispatch trainend event.
            
            # 21. Close and reset TensorFlow training graph
            self.close()

    def keras_evaluate(self, x: Optional[Union['Source.Config', 'np.ndarray', List[List[Any]], List['Source.Config'], 'Source.Data']] = None, 
                batchSize: int = -1, shuffle: bool = False, validation: bool = False, isPredict: bool = False, event: str = "", saveOriginal: bool = False, saveResults: bool = False, buildNo: int = 0):
        '''
			Evaluation keras model (A user-oriented API).   --- UPDATED (Kai Hsiang, Dexter) 20200119

            Parameters
            ------------------------------

            x               `Source.Config|np.ndarray[np.ndarray]|list[list]|list[Source.Config]|Source.Data`     - Input data of a model.

            batchSize       `int`   - Batch size of the test dataset. -1: Full epoch evaluation will be processed.

            shuffle         `bool`  - Whether shuffling is needed.

            validation      `bool`  - Whether this evaluation is taken on validation dataset.

            isPredict       `bool`  - Whether this evaluation requires a prediction.
            
            event           `str`   - An event name for this evaluation.

            saveOriginal    `bool`  - Whether to save original data. (Reserved for future use)

            saveResults     `bool`  - Whether to save result data. (Reserved for future use)

            buildNo     `int`       - The build number to be built.
        '''
        #   1A.  If there is evaluation data, set it as test data source.
        if (x is not None):
            if (validation):
                raise ValueError("NOM Trainer Internal Error - Validation triggered with customized source data.")

            #   1A-1.    Dependiing on the training source and set up the test sources.
            ## TODO: Remove redundant codes.
            if (isinstance(x, Source.Config)):
                self._evalSources = [x]
            elif (isinstance(x, Train.UsingData)):
                self._evalSources = x
            elif (isinstance(x, list) and all([isinstance(x, Source.Config) for t in x])):
                self._evalSources = x
                '''elif (len(self.sources) == 0 and isinstance(self.sources[0], Source.Table)):
                    self._evalSources = [Source.Table(x,training=False)]'''
            else:
                raise ValueError("Evaluation source is not supported.")
            
            #   1A-2.    Check the compatibility of the original source configurations.
            if (not isinstance(x, Train.UsingData)):
                if (len(self._evalSources) != len(self.sources)):
                    raise ValueError("Evaluation sources length does not match originally desinged model.")
                elif (any([not isinstance(s, self._evalSources[idx].__class__) for idx,s in enumerate(self.sources)])):
                    raise ValueError("Evaluation sources class does not match originally desinged model.")

        #   2. If there are available evaluation source, evaluate the data.
        if (isPredict):
            #   Calculate the prediction results.
            predictionResults = self._keras_evaluate(validation, isPredict = isPredict, event=event, saveOriginal=saveOriginal, saveResults=saveResults, buildNo = buildNo)
            
            #   Clear evaluation sources.
            self._evalSources = None

            return predictionResults
        else:
            #   Evaluate the data.
            self._keras_evaluate(validation, event=event, saveOriginal=saveOriginal, saveResults=saveResults, buildNo = buildNo)
    
            #   Clear evaluation sources.
            self._evalSources = None

    def _keras_evaluate(self, validation: bool = False, isPredict: bool = False, event: str = "", saveOriginal: bool = False, saveResults: bool = False, buildNo: int = 0) -> Any:
        '''
			Evaluate the keras model.   --- UPDATED (Kai Hsiang, Dexter) 20200311

            Parameters
            ------------------------------

            validation  `bool`  - Whether this evaluation is taken on validation dataset.

            isPredict   `bool`  - Whether this evaluation requires a prediction.

            event       `str`   - An event name for this evaluation. If it is "Test Log", it is an evaluation during training.

            saveOriginal    `bool`  - Whether to save original data. (Reserved)

            saveResults     `bool`  - Whether to save result data. (Reserved)

            buildNo     `int`   - The build number to be built.

            Returns
            ------------------------------

            `*`     - Prediction results if needed.
        '''
        # Define the training stage.
        oriSourceDataset = self.currentSourceDataset
        if (validation):
            self.assignCurrentSourceDataset(DataGenerator.Dataset.Types.Validation)    
        else:
            self.assignCurrentSourceDataset(DataGenerator.Dataset.Types.Test)

        #   1.  Confirm the test data are with same batch-size as source data
        predictSources = self._evalSources
        useExternalSource = predictSources is not None
        isProcessedData = isinstance(predictSources, Train.UsingData)
        if isProcessedData:
            if len(predictSources) == 0:
                raise ValueError("There is no test or validation data sources.")
            elif len(self.dppNodes) != len(predictSources):
                raise ValueError("Sources and test sources are not matched.")

        #   2.  Define the epoch size and relevant learning rate, not all data would be looped through depending on previously defined batch size 
        rootSources = self.rootSources
        totalStepCount = math.ceil(predictSources.getCount()/self.buildConfigs[self.buildNo].batchSize) if useExternalSource else rootSources.batchCountPerEpoch

        #   3.  Loop the test source in several batches for testing
        finalTensors = self.getFinalNodes(buildNo=buildNo)# layer profile
        savedResults = {}
        
        #   3-0. Initialize dataSize, and all evaluation measurements.
        dataSize = 0
        for lt in finalTensors:
            lt._clearEvalInfo()

        for i in range(0, totalStepCount):
            #   3-1.    Get all test data and count the test data size.
            allTestData = predictSources if isProcessedData else {dppKey: dppData for dppKey,dppData in next(self).items()}
            dataSize += max([len(colData) for colData in allTestData.values()])

            #   3-4.    Run the model.
            self.k_model.call(allTestData, training=False)
            # allPredictedResults = {lt.name: getattr(self.k_model, lt.name + str('_output')) for lt in finalTensors}
            allPredictedResults = {}
            allCompareData = {}
            for lt in finalTensors:
                for comparison in lt.comparisons:
                    comparison_name = lt.name+'_'+comparison["input"]+'_'+comparison["target"]
                    allPredictedResults[comparison_name] = getattr(self.k_model, comparison["input"].replace(' ', '') + str('_output'))
                    allCompareData[comparison_name] = getattr(self.k_model, comparison["target"].replace(' ', '') + str('_output'))
            
            #   3-5.    Recover the predicted results from reversed data transformations or circular range bounding.
            for lt in finalTensors:
                # Dexter 0803 NOTES: Put the class-specific algorithms back to the class methods.
                # Dexter 0119 NOTES: Removed.
                # if isinstance(lt, ModelNode.Layer.Task.Classifier):
                # if lt.metrics == "accuracy":
                #     predictionResults = np.reshape(np.argmax(allPredictedResults[lt.name], axis = 1), (-1,1))
                # else:
                #     predictionResults = allPredictedResults[lt.name]
                # allPredictedResults[lt.name] = lt.recoverPredictedResults(predictionResults)
                for comparison in lt.comparisons:
                    comparison_name = lt.name+'_'+comparison["input"]+'_'+comparison["target"]
                    if comparison["metrics"] == "accuracy" or comparison["metrics"] == "sparse_categorical_crossentropy":
                        predictionResults = np.reshape(np.argmax(allPredictedResults[comparison_name], axis = 1), (-1,1))
                        allCompareData[comparison_name] = np.reshape(np.argmax(allCompareData[comparison_name], axis = 1), (-1,1))
                    else:
                        predictionResults = allPredictedResults[comparison_name]
                    
                    if comparison["target"] == "input" or comparison["target"] == "target":
                        # Collect the test data and recover to the original range.
                        allPredictedResults[comparison_name] = lt.recoverPredictedResults(predictionResults, comparison["target"])
                        allCompareData[comparison_name] = lt.recoverPredictedResults(np.array(allTestData[comparison["target"]]), comparison["target"])
            
            #   3-6.    If it's an evaluation, partially evaluate this batch of test data for each final layers.
            if not isPredict:
                #   3-6-1.  Partial evaluate each final tensor.
                for lt in finalTensors:
                    for comparison in lt.comparisons:
                        lt.partialEvaluate(allCompareData[lt.name+'_'+comparison["input"]+'_'+comparison["target"]], allPredictedResults[lt.name+'_'+comparison["input"]+'_'+comparison["target"]], comparison)

            #   3-7.  Save all predicted results for all final layers.
            if isPredict:
                for lt in finalTensors:
                    for comparison in lt.comparisons:
                        taskName = lt.name+'_'+comparison["input"]+'_'+comparison["target"]
                        try:
                            # Concatenate the results on the first axis (batch axis [looping on batched testing]).
                            savedResults[taskName] = np.concatenate((savedResults[taskName], allPredictedResults[taskName]), axis=0)
                        except:
                            # If it's the first batch testing, initialte with the results.
                            savedResults[taskName] = allPredictedResults[taskName]
        
        if (isPredict):
            # Revert the dataset reference.
            self.assignCurrentSourceDataset(oriSourceDataset)
            
            #   4X. Return the results if it's a prediction.
            return savedResults
        else:
            #   4A.  Get the final score by aggregating all previously partially evaluated information.
            toAns = []
            for lt in finalTensors:
                for comparison in lt.comparisons:
                    if comparison["metrics"] != "None":
                        toAns.append([lt.name+'_'+comparison["input"]+'_'+comparison["target"]+' '+comparison["metrics"], lt.getTestScore(comparison)])

            #   5.  Log the test results.
            if toAns:
                nowTime = datetime.now()
                testLog = [nowTime, self._runNo, self._cvNo, self.globalStep, self.localStep, event, *escapeNaNList([ans[1] for ans in toAns])]
                if (self._cvNo == -1):
                    print('\nTEST (Validation: %s - %s) Test Data Size: %d\n%s: Run #%d, Step %d (%d) --- \n' % (validation, event, dataSize, nowTime, self._runNo, self.globalStep, self.localStep), *[ans[0] + ": " + ("%f\n" % ans[1]) for ans in toAns], "\n")
                else:
                    print('\nTEST (Validation: %s - %s) Test Data Size: %d\n%s: Run #%d - Cross Validation %d, Step %d (%d) --- \n' % (validation, event, dataSize, nowTime, self._runNo, self._cvNo, self.globalStep, self.localStep), *[ans[0] + ": " + ("%f\n" % ans[1]) for ans in toAns], "\n")
                if (self.testLogger is not None):
                    self.testLogger.logAndSave(testLog)
            
            #   6.  Close Test Sources
            if not isProcessedData:
                if predictSources is not None:
                    # 6A. Case of external sources that are not UsingData.
                    for s in reversed(predictSources):
                        if isinstance(s, Source.Config):
                            s.close()
                else:
                    # 6B. Case of using internal dataset.
                    self.rootSources.closeCurrentDatasets()
            
            # Revert the dataset reference.
            self.assignCurrentSourceDataset(oriSourceDataset)

    def keras_predict(self, x: Optional[Union['Source.Config', 'np.ndarray', List[List[Any]], List['Source.Config'], 'Source.Data']] = None, 
                batchSize: int = -1, shuffle: bool = False, validation: bool = False, event: str = "", saveOriginal: bool = False, saveResults: bool = False, buildNo: int = 0):
        '''
			Predict some data using keras model (A user-oriented API).   --- UPDATED (Kai Hsiang) 20190717

            Parameters
            ------------------------------

            x               `Source.Config|np.ndarray[np.ndarray]|list[list]|list[Source.Config]|Source.Data`     - Input data of a model.

            batchSize       `int`   - Batch size of the test dataset. -1: Full epoch evaluation will be processed.

            shuffle         `bool`  - Whether shuffling is needed.

            validation      `bool`  - Whether this evaluation is taken on validation dataset.

            event           `str`   - An event name for this evaluation.

            saveOriginal    `bool`  - Whether to save original data. (Reserved for future use)

            saveResults     `bool`  - Whether to save result data. (Reserved for future use)

            buildNo     `int`   - The build number to be built.
        '''
        return self.keras_evaluate(x=x, batchSize=batchSize, shuffle=shuffle, validation=validation, isPredict=True, event=event, saveOriginal=saveOriginal, saveResults=saveResults, buildNo = buildNo)

    @staticmethod
    def getPrioritizedKeys(obj: Dict[str, Any], prioritizedKeys: List[str]) -> List[str]:
        """
            Get the keys of the object with a prefered prioritized order.   --- UPDATED (Dexter) 20191013

            Parameters
            ------------------------------

            `dict<str,*>` obj - The object.

            `list<str>` prioritizedKeys - A prioritized key list in order.

            Returns
            ------------------------------

            `list<str>` - The list of keys with the requested prioritized order.
        """
        # Get a set of the appeared keys.
        jsonKeys: Set[str] = set(obj.keys())

        # For every given prioritized keys, push into the new list and remove from the appearing set.
        returnList: List[str] = []
        for pKey in prioritizedKeys:
            if pKey in jsonKeys:
                returnList.append(pKey)
                jsonKeys.remove(pKey)
        
        # Return the prioritized key list.
        return [*returnList, *jsonKeys]

class RegisterManger():
    '''
			Class representing a Register Manger.   --- UPDATED (Kai Hsiang) 20190829
    '''
    def __init__(self):
        '''
			Create a RegisterManger tensors object.   --- UPDATED (Kai Hsiang) 20190829
        '''
        self.counter = 0
        self.reg_list = [] # 0:free ,1:used
        self.name_to_reg = {} # dict[layer name] => register name
    
    def get_register(self, layer_name):
        '''
			Ccreate a Register for layer.   --- UPDATED (Kai Hsiang) 20190829

            Parameters
            ------------------------------

            layer_name    `str`   - The layer name.
        '''
        try:
            return self.name_to_reg[layer_name]
        except:
            reg = -1
            for idx, r in enumerate(self.reg_list):
                if r == 0:
                    reg = idx
                    self.name_to_reg[layer_name] = 'register_' + str(reg)
                    self.reg_list[reg] = 1
                    break
            if reg == -1:#create new register
                self.reg_list.append(0)
                reg = self.counter
                self.counter += 1
                self.name_to_reg[layer_name] = 'register_' + str(reg)
                self.reg_list[reg] = 1
            return self.name_to_reg[layer_name]

    def free_register(self, layer_name):
        '''
			Delete a Register of Layer.   --- UPDATED (Kai Hsiang) 20190829

            Parameters
            ------------------------------

            layer_name    `str`   - The layer name.
        '''
        try:
            register_name = self.name_to_reg.pop(layer_name)
            reg = int(register_name.split('_')[1])
            self.reg_list[reg] = 0
        except:
            pass
      
class LinearTransformTensors():
    '''
			Class representing a linear transformation results and it's corresponding weights.   --- UPDATED (Dexter) 20181003
    '''
    def __init__(self, results: 'tf.Tensor', weights: List[tf.Tensor]):
        '''
			Ccreate a linear transformation tensors object, wrapping the results and it's corresponding weights as an object.   --- UPDATED (Dexter) 20181003
        '''
        self.results = results
        self.weights = weights

class RNNConfig():
    '''
			Class representing a RNN configuration on a given tensor, controlling the behavior of the RNN cell operation.   --- UPDATED (CYK) 20190819
    '''
    def __init__(self, weightConfigs: 'VarConfig' = Train.Variable.Config(), recurrentWeightConfigs: 'VarConfig' = Train.Variable.Config(), 
                   biasConfigs: 'VarConfig' = Train.Variable.Config()):
        '''
			Create a RNN option using variable configurations.

            Parameters
            ------------------------------

            weightConfigs       `VarConfig` -  The weight configurations.

            recurrentWeightConfigs      `VarConfig` -  The recurrent weight configurations.

            biasConfigs          `VarConfig` -  The bias configurations.

        '''
        self.weightConfig = weightConfigs        
        self.recurrentWeightConfig = recurrentWeightConfigs       
        self.biasConfig = biasConfigs
    
    @staticmethod
    def createBasicConfig() -> 'RNNConfig':
        '''
			Create a basic RNN config.   --- UPDATED (CYK) 20190916

            Parameters
            ------------------------------

            Returns
            ------------------------------

            `RNNConfig` - A RNNConfig object from the given basic configuration settings.
        '''

        weightConfigs = Train.Variable.Config(initializer = Train.Variable.Initializer.GlorotUniform())
        recurrentWeightConfigs = Train.Variable.Config(initializer = Train.Variable.Initializer.Orthogonal())
        biasConfigs = Train.Variable.Config(initializer = Train.Variable.Initializer.Zeros())
                 
        return RNNConfig(weightConfigs = weightConfigs, recurrentWeightConfigs = recurrentWeightConfigs, 
                            biasConfigs = biasConfigs)

class GRUConfig():
    '''
			Class representing a GRU configuration on a given tensor, controlling the behavior of the GRU cell operation.   --- UPDATED (CYK) 20191101
    '''

    class GRUVarConfigs():
        '''
            Class representing  3 VarConfigs, they are about resetGate, updateGate, and stateCandidate.
        '''

        def __init__(self,resetGate: 'VarConfig' = None, updateGate: 'VarConfig' = None, stateCandidate: 'VarConfig' = None):
            '''
                create a GRUVarConfigs

                Parameters
                ------------------------------

                resetGate       `VarConfig`     - VarConfig about resetGate.

                updateGate      `VarConfig`     - VarConfig about updateGate.

                stateCandidate      `VarConfig`     - VarConfig about stateCandidate.
            '''
            self.resetGate = resetGate
            self.updateGate = updateGate           
            self.stateCandidate = stateCandidate

    def __init__(self, weightConfigs: 'GRUVarConfigs' = GRUVarConfigs(), recurrentWeightConfigs: 'GRUVarConfigs' = GRUVarConfigs(), 
                    biasConfigs: 'GRUVarConfigs' = GRUVarConfigs()):
        '''
			Create a GRU option using variable configurations.

            Parameters
            ------------------------------

            weightConfigs       `GRUVarConfigs` -  The weight configurations of resetGate, updateGate, and stateCandidate.

            recurrentWeightConfigs      `GRUVarConfigs` -  The recurrent weight configurations of resetGate, updateGate, and stateCandidate.

            biasConfigs          `GRUVarConfigs` -  The bias configurations of resetGate, updateGate, and stateCandidate.

        '''
        self.resetGateWeightConfig = weightConfigs.resetGate
        self.updateGateWeightConfig = weightConfigs.updateGate
        self.stateCandidateWeightConfig = weightConfigs.stateCandidate
        self.resetGateRecurrentWeightConfig = recurrentWeightConfigs.resetGate
        self.updateGateRecurrentWeightConfig = recurrentWeightConfigs.updateGate
        self.stateCandidateRecurrentWeightConfig = recurrentWeightConfigs.stateCandidate
        self.resetGateBiasConfig = biasConfigs.resetGate
        self.updateGateBiasConfig = biasConfigs.updateGate
        self.stateCandidateBiasConfig = biasConfigs.stateCandidate

    
    @staticmethod
    def createBasicConfig() -> 'GRUConfig':
        '''
			Create a basic GRU config.   --- UPDATED (CYK) 20191101

            Parameters
            ------------------------------

            Returns
            ------------------------------

            `GRUConfig` - A GRUConfig object from the given basic configuration settings.
        '''

        VarConfigOfGlorotUniform = Train.Variable.Config(initializer = Train.Variable.Initializer.GlorotUniform())
        VarConfigOfOrthogonal = Train.Variable.Config(initializer = Train.Variable.Initializer.Orthogonal())
        VarConfigOfZeros = Train.Variable.Config(initializer = Train.Variable.Initializer.Zeros())

        weightConfigs = LSTMConfig.LSTMVarConfigs(VarConfigOfGlorotUniform,VarConfigOfGlorotUniform,VarConfigOfGlorotUniform) 
        recurrentWeightConfigs = LSTMConfig.LSTMVarConfigs(VarConfigOfOrthogonal,VarConfigOfOrthogonal,VarConfigOfOrthogonal) 
        biasConfigs = LSTMConfig.LSTMVarConfigs(VarConfigOfZeros,VarConfigOfZeros,VarConfigOfZeros)

                 
        return LSTMConfig(weightConfigs = weightConfigs, recurrentWeightConfigs = recurrentWeightConfigs, 
                    biasConfigs = biasConfigs)  

class LSTMConfig():
    '''
			Class representing a LSTM configuration on a given tensor, controlling the behavior of the LSTM cell operation.   --- UPDATED (CYK) 20190819
    '''

    class LSTMVarConfigs():
        '''
            Class representing  4 VarConfigs, they are about inputGate, forgetGate, outputGate, and memory.
        '''

        def __init__(self,inputGate: 'VarConfig' = None, forgetGate: 'VarConfig' = None, outputGate: 'VarConfig' = None, memory: 'VarConfig' = None):
            '''
                create a LSTMVarConfigs

                Parameters
                ------------------------------

                inputGate       `VarConfig`     - VarConfig about inputGate.

                forgetGate      `VarConfig`     - VarConfig about forgetGate.

                outputGate      `VarConfig`     - VarConfig about outputGate.

                memory          `VarConfig`     - VarConfig about memory.
            '''
            self.inputGate = inputGate
            self.forgetGate = forgetGate           
            self.outputGate = outputGate
            self.memory = memory

    def __init__(self, weightConfigs: 'LSTMVarConfigs' = LSTMVarConfigs(), recurrentWeightConfigs: 'LSTMVarConfigs' = LSTMVarConfigs(), 
                    peepholeWeightConfigs: 'LSTMVarConfigs' = LSTMVarConfigs(), biasConfigs: 'LSTMVarConfigs' = LSTMVarConfigs()):
        '''
			Create a LSTM option using variable configurations.

            Parameters
            ------------------------------

            weightConfigs       `LSTMVarConfigs` -  The weight configurations of inputGate,forgetGate,outputGate,and memory.

            recurrentWeightConfigs      `LSTMVarConfigs` -  The recurrent weight configurations of inputGate,forgetGate,outputGate,and memory.

            peepholeWeightConfigs       `LSTMVarConfigs` -  The peephole weight configurations of inputGate,forgetGate,outputGate,and memory.

            biasConfigs          `LSTMVarConfigs` -  The bias configurations of inputGate,forgetGate,outputGate,and memory.

        '''
        self.inputGateWeightConfig = weightConfigs.inputGate
        self.forgetGateWeightConfig = weightConfigs.forgetGate
        self.outputGateWeightConfig = weightConfigs.outputGate
        self.memoryWeightConfig = weightConfigs.memory
        self.inputGateRecurrentWeightConfig = recurrentWeightConfigs.inputGate
        self.forgetGateRecurrentWeightConfig = recurrentWeightConfigs.forgetGate
        self.outputGateRecurrentWeightConfig = recurrentWeightConfigs.outputGate
        self.memoryRecurrentWeightConfig = recurrentWeightConfigs.memory
        self.inputGatePeepholeWeightConfig = peepholeWeightConfigs.inputGate
        self.forgetGatePeepholeWeightConfig = peepholeWeightConfigs.forgetGate
        self.outputGatePeepholeWeightConfig = peepholeWeightConfigs
        self.inputGateBiasConfig = biasConfigs.inputGate
        self.forgetGateBiasConfig = biasConfigs.forgetGate
        self.outputGateBiasConfig = biasConfigs.outputGate
        self.memoryBiasConfig = biasConfigs.memory
    
    @staticmethod
    def createBasicConfig() -> 'LSTMConfig':
        '''
			Create a basic LSTM config.   --- UPDATED (CYK) 20190819

            Parameters
            ------------------------------

            Returns
            ------------------------------

            `LSTMConfig` - A LSTMConfig object from the given basic configuration settings.
        '''

        VarConfigOfGlorotUniform = Train.Variable.Config(initializer = Train.Variable.Initializer.GlorotUniform())
        VarConfigOfOrthogonal = Train.Variable.Config(initializer = Train.Variable.Initializer.Orthogonal())
        VarConfigOfZeros = Train.Variable.Config(initializer = Train.Variable.Initializer.Zeros())

        weightConfigs = LSTMConfig.LSTMVarConfigs(VarConfigOfGlorotUniform,VarConfigOfGlorotUniform,VarConfigOfGlorotUniform,VarConfigOfGlorotUniform) 
        recurrentWeightConfigs = LSTMConfig.LSTMVarConfigs(VarConfigOfOrthogonal,VarConfigOfOrthogonal,VarConfigOfOrthogonal,VarConfigOfOrthogonal) 
        peepholeWeightConfigs = LSTMConfig.LSTMVarConfigs(VarConfigOfOrthogonal,VarConfigOfOrthogonal,VarConfigOfOrthogonal,None)
        biasConfigs = LSTMConfig.LSTMVarConfigs(VarConfigOfZeros,VarConfigOfZeros,VarConfigOfZeros,VarConfigOfZeros)

                 
        return LSTMConfig(weightConfigs = weightConfigs, recurrentWeightConfigs = recurrentWeightConfigs, 
                    peepholeWeightConfigs = peepholeWeightConfigs, biasConfigs = biasConfigs)    

class _ModelNodeLayerIncoming():
    '''
			Class representing an incoming configurations of a node.   --- UPDATED (Dexter) 20180921
    '''
    class Types(Enumeration):
        '''
			Enumeration defining the available methods for incoming configuration.   --- UPDATED (Dexter) 20180919
        '''
        # Concatenate all incoming nodes. Nodes should have either the same number of dimensions, or higher dimension than the core incoming node. (Ref: @ModelNode.Layer.Incoming.Concat )
        Concat = 1

        # Elemenally sum all incoming nodes. Nodes should be same in dimensions, or the pre-feature dimensions should be the multiple of the core incoming node. Implementation of ResNet or Highway Network may consider using this method. (Ref: @ModelNode.Layer.Incoming.Sum )
        Sum = 3

        # Elementally multiply all incoming nodes. Nodes should be same in dimensions, or the pre-feature dimensions should be the multiple of the core incoming node. (Ref: @ModelNode.Layer.Incoming.Multiply )
        Multiply = 4
        
        # Elementally sum and multiply the elemenental min of all incoming nodes with weighted proportion on summation and multiplication. Nodes should be same in dimensions, or the pre-feature dimensions should be the multiple of the core incoming node. (Ref: @ModelNode.Layer.Incoming.Blend )
        Blend = 5
    
    class MergeDimMethods(Enumeration):
        '''
			Enumeration defining the type of handling merging higher dimensioned nodes to the lower dimensioned core node.   --- BETA --- UPDATED (Dexter) 20180923
        '''
        # SubSample: A subsampling is used.
        SubSample = 1

        # SpaceToDepth: Pre-operation axis are stacked in blocks of multiple of the core node dimensions as the operation dimension.
        SpaceToDepth = 2

        # Multiply: Adjacent features in the dimensions before the operation dimension will be max-pooled to the dimensions as the core node.
        MaxPool = 3
        
        # MeanPool: Adjacent features in the dimensions before the operation dimension will be mean-pooled to the dimensions as the core node.
        MeanPool = 4
    
    @staticmethod
    def createFromJSON(obj: Dict[str, Any]) -> 'ModelNode.Layer.Incoming.Config':
        """
            Parse a previously saved object into a new @ModelNode.Layer.Incoming.Config object. This will auto-determine the sub-class of the object, and pass the JSON object to the inner method to continue to parse.   --- UPDATED (Dexter) 20190731

            Parameters
            ------------------------------

            obj `dict<str,*>` - JSON object from Project file.

            Returns
            ------------------------------
            
            `ModelNode.Layer.Incoming.Config`  - A @ModelNode.Layer.Incoming.Config object.
        """
        # Parse this @ModelNode.Layer.Incoming.Config object.
        incomingConfig = getattr(ModelNode.Layer.Incoming, ModelNode.Layer.Incoming.Types.getName(obj["_instanceClass"]))()
        incomingConfig.parseJSON(obj)
        return incomingConfig

    class Config():
        '''
			A instance class representing an incoming configruation object.   --- UPDATED (Dexter) 20181128
        '''
        def __init__(self, method: 'ModelNode.Layer.Incoming.Types' = None, axis: int = -1, coreNode: 'ModelNode.Layer.Config' = None, mergeDim: 'ModelNode.Layer.Incoming.MergeDimMethods' = None):
            '''
			    Create an incoming configuration object.  --- UPDATED (Dexter) 20180921

                Parameters
                ------------------------------

                method      `ModelNode.Layer.Incoming.Types`            - A numerical representation for the incoming option method type.

                axis        `int`                                   - The operation axis.

                coreNode    `ModelNode.Layer.Config`                          - The reference index of the ModelNode.Layer.Config.fromNode list. If `None`, an automated selection will be used.

                mergeDim    `ModelNode.Layer.Incoming.MergeDimMethods`    - The type of dimension-merging methods if needed.
            '''
            if method is None or method not in _ModelNodeLayerIncoming.Types:
                raise ValueError("Required incoming configurations is not supported.")
            # `ModelNode.Layer.Incoming.Types` - A numerical representation for the incoming option method type.
            self._instanceClass = method
            # `ModelNode.Layer.Config` - The reference index of the ModelNode.Layer.Config.fromNode list. If `None`, an automated selection will be used.
            self.coreNode = coreNode

            if mergeDim is None:
                mergeDim = _ModelNodeLayerIncoming.MergeDimMethods.SubSample
            elif mergeDim not in _ModelNodeLayerIncoming.MergeDimMethods:
                raise ValueError("Required incoming configurations is not supported.")
            # `ModelNode.Layer.Incoming.MergeDimMethods` - The type of dimension-merging methods if needed.
            self.mergeDim = mergeDim

            if not (isinstance(axis, int) and axis >= -1):
                raise ValueError("axis should be an integer greater than or equal to -1.")
            # `int` - The operation axis.
            self.axis = axis
        
        @property
        def instanceClass(self) -> 'ModelNode.Layer.Incoming.Types':
            """
                An enumeration representation for the incoming option method type.   --- UPDATED (Dexter) 20190822

                Returns
                ------------------------------
                
                `ModelNode.Layer.Incoming.Types`  - An enumeration representation for the incoming option method type.
            """
            return self._instanceClass

        def __repr__(self) -> str:
            '''
			Get a string representation of this incoming configuration.   --- UPDATED (Dexter) 20180919

                Returns
                ------------------------------

                `str`   - The representation of this method.
            '''
            return "<ModelNode.Layer.Incoming: method: \"" + str(self) + "\">"

        def __str__(self) -> str:
            '''
			Returns the method string of this incoming configuration.   --- UPDATED (Dexter) 20180919

                Returns
                ------------------------------

                `str`   - The name of this method.
            '''
            return self.instanceClass.name

        def parseJSON(self, obj: Dict[str, Any]):
            """
                Parse a previously saved object into this class of @ModelNode.Layer.Incoming.Config.   --- UPDATED (Dexter) 20190221

                Returns
                ------------------------------
                
                `dict<str,*>`  - JSON object from Project file
            """
            # Iterate the object keys and take actions on mapping back to the ModelNode.Layer.Incoming.Config Class.
            for (k, v) in obj.items():
                if (k not in ["_instanceClass", "mergeDim", "method"]):
                    setattr(self, k, v)

    class Concat(Config):
        '''
			Class representing an option of concatenating incoming nodes.   --- UPDATED (Dexter) 20180920
        '''
        def __init__(self, coreNode: 'ModelNode.Layer.Config' = None, axis: int = -1, mergeDim: int = None):
            '''
			Create an incoming concatenation option.   --- UPDATED (Dexter) 20180919

                Parameters
                ------------------------------

                coreNode    `ModelNode.Layer.Config`      - The reference index of the ModelNode.Layer.Config.fromNode list. If None, a selection on nearest and smallest dimensioned node will be used.

                axis        `int`               - Incoming node will be flattened to this axis for concatenation.

                mergeDim    `int`               - The type of dimension-merging methods if needed.
            '''
            super().__init__(_ModelNodeLayerIncoming.Types.Concat, axis, coreNode, mergeDim)

    class ElementWiseConfig(Config):
        '''
			Class representing an option of summing incoming nodes element-wise.   --- UPDATED (Dexter) 20180920
        '''
        def __init__(self, method: int = None, coreNode: 'ModelNode.Layer.Config' = None, axis: int = -1, mergeDim: int = None, transformGate: int = None, 
                    transformConfig: 'Train.Variable.LinearTransform' = Train.Variable.LinearTransform(weightConfig = Train.Variable.Config(initializer = Train.Variable.Initializer.TruncatedNormal(mean = 0.0, stddev = 0.05))),
                    dimensionMappingConfig: 'Train.Variable.LinearTransform' = Train.Variable.LinearTransform(weightConfig = Train.Variable.Config(initializer = Train.Variable.Initializer.TruncatedNormal(mean = 0.0, stddev = 0.05)))):
            '''
			Create an incoming configuration object with element-wise operation.   --- UPDATED (Dexter) 20190123

                Parameters
                ------------------------------

                method              `int`           - A numerical representation for the incoming option method type.

                coreNode            `ModelNode.Layer.Config`  - The reference index of the @ModelNode.Layer.Config.fromNode list. If None, a selection on the node with smallest dimensioned and longest path from the root source will be used.

                axis                `int`           - The operation axis.
                
                mergeDim            `int`           - The type of dimension-merging methods if needed.

                transformGate       `ModelNode.Layer.Config|int`          - The reference layer of the @ModelNode.Layer.Config.fromNode list of the node on which a transform gate will apply with distributed total proportion of nodes as 1. If -1, a sellection on the node with shortest path from the root source will be used. If None, no transform gate will be used. Implemenetation of Highway Network may need this option.

                transformConfig     `Train.Variable.LinearTransform`     - The linear transform configuration of the transform gate.

                dimensionMappingConfig  `Train.Variable.LinearTransform` - The linear transform configuration of any mismatched dimensioned data with the coreNode. If None, the dimensions stated in `self.axis` should be the same across all incoming nodes.
            '''
            super().__init__(method, axis, coreNode, mergeDim)

            if transformGate is not None and not (isinstance(transformGate, int) and transformGate >= -1):
                raise ValueError("transformGate should be an integer greater than or equal to -1.")
            self.transformGate = transformGate

            if (transformGate is not None and (transformConfig is None or not(isinstance(transformConfig, Train.Variable.LinearTransform)))):
                raise ValueError("Where transform gate is needed, transformConfig should be passed with a Train.Variable.LinearTransform object.")
            self.transformConfig = transformConfig

            self.dimensionMappingConfig = dimensionMappingConfig

        def parseJSON(self, obj: Dict[str, Any]):
            """
                Parse a previously saved object into this class of @ModelNode.Layer.Incoming.Config.   --- UPDATED (Dexter) 20191001

                Returns
                ------------------------------
                
                `dict<str,*>`  - JSON object from Project file
            """
            # Iterate the object keys and take actions on mapping back to the ModelNode.Layer.Incoming.Config Class.
            for (k, v) in obj.items():
                if (k in ["transformConfig", "dimensionMappingConfig"]):
                    setattr(self, k, Train.Variable.LinearTransform())
                    getattr(self,k).parseJSON(v)
                elif (k not in ["_instanceClass", "mergeDim", "method"]):
                    setattr(self, k, v)
            
            # NOTE: Core node, transformGate will be recovered in Train.parseJSON() because it needs to wait all LayerProfiles to be loaded.

    class Sum(ElementWiseConfig):
        '''
			Class representing an option of summing incoming nodes element-wise.   --- UPDATED (Dexter) 20180920
        '''
        def __init__(self, coreNode: 'ModelNode.Layer.Config' = None, axis: int = -1, mergeDim: int = None, transformGate: int = None, 
                    transformConfig: 'Train.Variable.LinearTransform' = Train.Variable.LinearTransform(weightConfig = Train.Variable.Config(initializer = Train.Variable.Initializer.TruncatedNormal(mean = 0.0, stddev = 0.05))),
                    dimensionMappingConfig: 'Train.Variable.LinearTransform' = Train.Variable.LinearTransform(weightConfig = Train.Variable.Config(initializer = Train.Variable.Initializer.TruncatedNormal(mean = 0.0, stddev = 0.05)))):
            '''
			Create an incoming summation option object.   --- UPDATED (Dexter) 20180920

                Parameters
                ------------------------------

                coreNode            `ModelNode.Layer.Config`  - The reference index of the ModelNode.Layer.Config.fromNode list. If None,  a selection on the node with smallest dimensioned and longest path from the root source will be used.

                axis                `int`           - The operation axis.
                
                mergeDim            `int`           - The type of dimension-merging methods if needed.

                transformGate       `int`           - The reference index of the ModelNode.Layer.Config.fromNode list of the node on which a transform gate will apply with distributed total proportion of nodes as 1. If -1, a sellection on the node with shortest path from the root source will be used. If None, no transform gate will be used. Implemenetation of Highway Network may need this option.

                transformConfig     `Train.Variable.LinearTransform`     - The linear transform configuration of the transform gate.

                dimensionMappingConfig  `Train.Variable.LinearTransform` - The linear transform configuration of any mismatched dimensioned data with the coreNode. If None, the dimensions stated in `self.axis` should be the same across all incoming nodes.
            '''
            super().__init__(_ModelNodeLayerIncoming.Types.Sum, coreNode, axis, mergeDim, transformGate, transformConfig, dimensionMappingConfig)

    class Multiply(ElementWiseConfig):
        '''
			Class representing an option of multiplying incoming nodes element-wise.   --- UPDATED (Dexter) 20180920
        '''
        def __init__(self, coreNode: 'ModelNode.Layer.Config' = None, axis: int = -1, mergeDim: int = None, transformGate: int = None, 
                    transformConfig: 'Train.Variable.LinearTransform' = Train.Variable.LinearTransform(weightConfig = Train.Variable.Config(initializer = Train.Variable.Initializer.TruncatedNormal(mean = 0.0, stddev = 0.05))),
                    dimensionMappingConfig: 'Train.Variable.LinearTransform' = Train.Variable.LinearTransform(weightConfig = Train.Variable.Config(initializer = Train.Variable.Initializer.TruncatedNormal(mean = 0.0, stddev = 0.05)))):
            '''
			Create an incoming multiplication option object.   --- UPDATED (Dexter) 20180920

                Parameters
                ------------------------------

                coreNode            `ModelNode.Layer.Config`  - The reference index of the ModelNode.Layer.Config.fromNode list. If None, a selection on the node with smallest dimensioned and longest path from the root source will be used.

                axis                `int`           - The operation axis.
                
                mergeDim            `int`           - The type of dimension-merging methods if needed.

                transformGate       `int`           - The reference index of the ModelNode.Layer.Config.fromNode list of the node on which a transform gate will apply with distributed total proportion of nodes as 1. If -1, a sellection on the node with shortest path from the root source will be used. If None, no transform gate will be used. Implemenetation of Highway Network may need this option.

                transformConfig     `Train.Variable.LinearTransform`     - The linear transform configuration of the transform gate.

                dimensionMappingConfig  `Train.Variable.LinearTransform` - The linear transform configuration of any mismatched dimensioned data with the coreNode. If None, the dimensions stated in `self.axis` should be the same across all incoming nodes.
            '''
            super().__init__(_ModelNodeLayerIncoming.Types.Multiply, coreNode, axis, mergeDim, transformGate, transformConfig, dimensionMappingConfig)

    class Blend(ElementWiseConfig):
        '''
			Class representing an option of blending incoming nodes element-wise.   --- UPDATED (Dexter) 20180920
        '''
        def __init__(self, coreNode: 'ModelNode.Layer.Config' = None, axis: int = -1, mergeDim: int = None, transformGate: int = None, 
                    transformConfig: 'Train.Variable.LinearTransform' = Train.Variable.LinearTransform(weightConfig = Train.Variable.Config(initializer = Train.Variable.Initializer.TruncatedNormal(mean = 0.0, stddev = 0.05))),
                    dimensionMappingConfig: 'Train.Variable.LinearTransform' = Train.Variable.LinearTransform(weightConfig = Train.Variable.Config(initializer = Train.Variable.Initializer.TruncatedNormal(mean = 0.0, stddev = 0.05))),
                    learnableBlend: bool = False):
            '''
			Create an incoming blending option object with x1 + x2 + ... + xN - N * (x1 * x2 * x3 * ... * xN).   --- UPDATED (Dexter) 20180920

                Parameters
                ------------------------------

                coreNode            `ModelNode.Layer.Config`  - The reference index of the ModelNode.Layer.Config.fromNode list. If None, a selection on the node with smallest dimensioned and longest path from the root source will be used.

                axis                `int`           - The operation axis.
                
                mergeDim            `int`           - The type of dimension-merging methods if needed.

                transformGate       `int`           - The reference index of the ModelNode.Layer.Config.fromNode list of the node on which a transform gate will apply with distributed total proportion of nodes as 1. If -1, a sellection on the node with shortest path from the root source will be used. If None, no transform gate will be used. Implemenetation of Highway Network may need this option.

                transformConfig     `Train.Variable.LinearTransform`     - The linear transform configuration of the transform gate.

                dimensionMappingConfig  `Train.Variable.LinearTransform` - The linear transform configuration of any mismatched dimensioned data with the coreNode. If None, the dimensions stated in `self.axis` should be the same across all incoming nodes.

                learnableBlend      `bool`          - If True, instead of originally blending, it will follow a1 * x1 + a2 * x2 + ... + aN * xN + b * (x1 * x2 * x3 * ... * xN) where a1 = a2 = ... = aN = 1 and b = N are learnable initialized constant values.
            '''
            super().__init__(_ModelNodeLayerIncoming.Types.Blend, coreNode, axis, mergeDim, transformGate, transformConfig, dimensionMappingConfig)
            self.learnableBlend = learnableBlend

class _ModelNodeLayerOutput():
    '''
			Class representing an output configuration of a node.   --- UPDATED (Dexter) 20180921
    '''
    class Types(Enumeration):
        '''
			Enumeration defining the available methods for output configuration of a NOM high level model node.   --- UPDATED (Dexter) 20190724
        '''
        # `int` - Abstract class representing representing a configuration for the output node. (Ref: @ModelNode.Layer.Output.Config )
        Config = 0

        # `int` - Output as node matrix operations.  (Ref: @ModelNode.Layer.Output.Default )
        Default = 1
        
        # `int` - Flatten the output on the requested axis. (Ref: @ModelNode.Layer.Output.Flatten )
        Flatten = 2
            
        # `int` - Reshape the output as specific shape. (Ref: @ModelNode.Layer.Output.Reshape )
        Reshape = 3
        
        # `int` - Select a specific dimension channel. (Ref: @ModelNode.Layer.Output.SelectChannel ) --- RESERVED
        SelectChannel = 4
    
    @staticmethod    
    def createFromJSON(obj: Dict[str, Any]) -> 'ModelNode.Layer.Output.Config':
        """
            Parse a previously saved object into a new @ModelNode.Layer.Output.Config object. This will auto-determine the sub-class of the object, and pass the JSON object to the inner method to continue to parse.   --- UPDATED (Dexter) 20190731

            Parameters
            ------------------------------

            obj `dict<str,*>` - JSON object from Project file.

            Returns
            ------------------------------

            `ModelNode.Layer.Output.Config` - A @ModelNode.Layer.Output.Config object.
        """
        # Parse this @ModelNode.Layer.Output.Config object.
        outputConfig = getattr(ModelNode.Layer.Output, ModelNode.Layer.Output.Types.getName(obj["_instanceClass"]))()
        outputConfig.parseJSON(obj)
        return outputConfig

    class Config():
        '''
			 A instance class representing an output configruation object.   --- UPDATED (Dexter) 20181128
        '''
        def __init__(self, method: 'ModelNode.Layer.Output.Types' = None):
            '''
			    Create an output configuration object.  --- UPDATED (Dexter) 20180921

                Parameters
                ------------------------------

                method      `ModelNode.Layer.Output.Types`   - A numerical representation for the incoming option method type.
            '''
            if method is None or not method in _ModelNodeLayerOutput.Types:
                raise ValueError("Required output method is not supported.")
            # `ModelNode.Layer.Output.Types` - The instnace class, as defined in @ModelNode.Layer.Output.Types .
            self._instanceClass = method
        
        def parseJSON(self, obj: Dict[str,Any]):
            """
                Parse a previously saved object into this class of @ModelNode.Layer.Output .   --- UPDATED (Dexter) 20181124

                Parameters
                ------------------------------

                obj `dict<str,*>` - JSON object from Project file
            """
            # Iterate the object keys and take actions on mapping back to the @ModelNode.Layer.Output Class.
            for (k, v) in obj.items():
                if (k not in ["_instanceClass"]):
                    setattr(self, k, v)

        @property
        def instanceClass(self) -> 'ModelNode.Layer.Incoming.Types':
            """
                An enumeration representation for the output option method type.   --- UPDATED (Dexter) 20190822

                Returns
                ------------------------------
                
                `ModelNode.Layer.Output.Types`  - An enumeration representation for the ioutputncoming option method type.
            """
            return self._instanceClass

        def __repr__(self) -> str:
            '''
			Get a string representation of this output configuration.   --- UPDATED (Dexter) 20180921

                Returns
                ------------------------------

                `str`   - The representation of this method.
            '''
            return "<OutuputConfig: method: \"" + str(self) + "\">"

        def __str__(self) -> str:
            '''
			Returns the method string of this output configuration.   --- UPDATED (Dexter) 20180921

                Returns 
                ------------------------------

                `str`   - The name of this method.
            '''
            return self.instanceClass.name

    class Default(Config):
        '''
			Class representing a default config for the output node.   --- UPDATED (Dexter) 20180921
        '''
        def __init__(self):
            '''
			Create a default output config.   --- UPDATED (Dexter) 20180921
            '''
            super().__init__(_ModelNodeLayerOutput.Types.Default)

    class Flatten(Config):
        '''
			Class representing a flattening process for the output node.   --- UPDATED (Dexter) 20180921
        '''
        def __init__(self, axis: int = 1):
            '''
			Create a flattening output config.   --- UPDATED (Dexter) 20180921

                Parameters
                ------------------------------

                axis        `int`   - The flattening axis of the output node.
            '''
            super().__init__(_ModelNodeLayerOutput.Types.Flatten)
            self.axis = axis

    class Reshape(Config):
        '''
			Class representing a reshape process for the output node.   --- UPDATED (Dexter) 20180921
        '''
        def __init__(self, shape: List[int] = (-1,)):
            '''
			Create a reshaping output config.   --- UPDATED (Dexter) 20180921

                Parameters
                ------------------------------

                shape       `list[int]`     - The shape of the available dimensions of each item.
            '''
            super().__init__(_ModelNodeLayerOutput.Types.Reshape)
            self.shape = shape

    class SelectChannel(Config):
        '''
			Class representing a channel selection for the output node.   --- RESERVED --- UPDATED (Dexter) 20180921
        '''
        def __init__(self, axis: int = 1, channel: int = 0):
            '''
			Create a chanel selection output config.   --- RESERVED --- UPDATED (Dexter) 20181125

                Parameters
                ------------------------------

                axis        `int`           - The selection axis of the output node.

                channel     `int`           - The selection channel of the output node.
            '''
            super().__init__(_ModelNodeLayerOutput.Types.SelectChannel)
            self.axis = axis
            self.channel = channel

class BuildItemList():
    '''
			Class representing different items according to different builds.   --- UPDATED (Dexter) 20181115
    '''
    def __init__(self):
        '''
			Create a build item list object.   --- UPDATED (Dexter) 20181115
        '''
        self.builds = {}

    def __getitem__(self, buildNo: int) -> Any:
        '''
			Get an item by a build number.   --- UPDATED (Dexter) 20181115
        
            Parameters
            ------------------------------

            buildNo     `int`           - The requested build.

            Returns
            ------------------------------

            `*`     - The requested item of a specific build.
        '''
        if (buildNo not in self.builds):
            self.builds[buildNo] = [] 
        return self.builds[buildNo]
    
    def __setitem__(self, buildNo: int, item: Any) -> Any:
        '''
			Set a list of sources on a specific build number.   --- UPDATED (Dexter) 20181115
        
            Parameters
            ------------------------------

            buildNo     `int`   - The target build.

            item        `*`     - The item of a specific build.

            Returns
            ------------------------------

            `*`     - The new item of the target build.
        '''
        self.builds[buildNo] = item
        return self.builds[buildNo]

class BuildSourceList(BuildItemList):
    '''
			Class representing different sources according to different builds.   --- UPDATED (Dexter) 20181115
    '''
    def __setitem__(self, buildNo: int, sources: List[Tuple[int, str]]) -> List[Tuple[int, str]]:
        '''
			Set a list of sources on a specific build number.   --- UPDATED (Dexter) 20181115
        
            Parameters
            ------------------------------

            buildNo     `int`                                   - The target build.

            sources     'list[Layertuple[int, str]Profile]'     - The list of sources of a specific build.

            Returns
            ------------------------------

            `list[tuple[int, str]]`     - The new list of sources.
        '''
        if (not all([(isinstance(st[0], int) and isinstance(st[1], str)) for st in sources])):
            raise ValueError("BuildSourceList should append a tuple of source information.")

        return super().__setitem__(buildNo, sources)

class BuildLayerList(BuildItemList):
    '''
			Class representing different layers according to different builds.   --- UPDATED (Dexter) 20181115
    '''
    def __setitem__(self, buildNo: int, layerProfiles: List['ModelNode.Layer.Config']) -> List['ModelNode.Layer.Config']:
        '''
			Add a layer on a specific build number.   --- UPDATED (Dexter) 20190730
        
            Parameters
            ------------------------------

            buildNo     `int`               - The target build.

            layerProfiles    'list[ModelNode.Layer.Config]'  - The list of layer profiles of a specific build.

            Returns
            ------------------------------

            `list[ModelNode.Layer.Config]`     - The new list of layers.
        '''
        if (not all([isinstance(lp, ModelNode.Layer.Config) for lp in layerProfiles])):
            raise ValueError("LayerNodeList should append ModelNode.Layer.Config objects.")

        return super().__setitem__(buildNo, layerProfiles)

class BuildOrderList(BuildItemList):
    '''
			Class representing different orders according to different builds.   --- UPDATED (Dexter) 20181115
    '''
    def __setitem__(self, buildNo: int, order: int) -> int:
        '''
			Set the order on a specific build number.   --- UPDATED (Dexter) 20181115
        
            Parameters
            ------------------------------

            buildNo     `int`           - The target build.

            order       `int`           - The order of a specific build.

            Returns
            ------------------------------

            `list[int]`                 - The new order.
        '''
        if (not isinstance(order, int)):
            raise ValueError("Order should be an integer.")

        return super().__setitem__(buildNo, order)

class _ModelNodeTypes(Enumeration):
    """
        Enumeration defining the types of model nodes.   --- UPDATED (Dexter) 20190508
    """
    # `ModelNode.Types` - Abstract class representing a model node configuration.
    Config = 0
    # `ModelNode.Types` - Class including different types of high-level Ladder model layer.
    Layer = 1
    # `ModelNode.Types` - Class including different types of low-level Ladder model layer.
    ComputationalUnit = 2
    # `ModelNode.Types` - Class for Tensorflow Hub layer.
    TFHub = 3

class _ModelNodeConfig():
    """
        Abstract class representing a model node configuration.   --- RESERVED --- UPDATED (Dexter) 20190508
    """
    def __init__(self, nodeType: 'ModelNode.Types' = _ModelNodeTypes.Config, name: str = ""):
        """
            Create a graph model node.  --- UPDATED (Dexter) 20200120

            Parameters
            ------------------------------

            name    `str`   - The name of this layer; the base name hierarchy in TensorFlow model.
        """
        # `ModelNode.Types` - The type of the model node, as defined in @ModelNode.Types .
        self._nodeType = ModelNode.Types.Config
        # `str` - The name of this model node; the base name hierarchy in TensorFlow model.
        self.name = name
        # `BuildSourceList` - A 2 dimension array of a list of incoming data preprocessing node in each build. Each node is represented by its key.
        self.fromSource = BuildSourceList()
        # `BuildLayerList` - A 2 dimension array of a list of incoming model nodes in each build. Each model node is represented by the node name. When implemented in prgoramming runtime, it should be a @ModelNode.Config object.
        self.fromNode = BuildLayerList()
        # `BuildLayerList` - A 2 dimension array of a list of connecting model nodes in each build. Each model node is represented by the node name. When implemented in prgoramming runtime, it should be a @ModelNode.Config object.
        self.toNode = BuildLayerList()
        # `BuildOrderList` - A list of topological orders in the graph of each build.
        self._order = BuildOrderList()
        # `Train` - The @NOM object where this @Source.Config belongs to. If this layer is just constructed, it is not belonged to any training instance. Always `null` or `undefined` in NOM JSON file to avoid circular referencing.
        self.train = None
        # `tf.DType` - The data type of this model node.
        self._dataType = tf.float32
        # `list<list<number>>` - A list of shapes in the order of each build. Each shape should be a list of number or `None`.
        self._shape = [[]]

        # Information that is used within a build.
        self._outputTensor = None
        self._built = False
        self._inputCollections = []
        self._weights = []
        self.weightLogging = True

    def parseJSON(self, obj: Dict[str, Any], train: 'Train'):
        """
            Abstract method to parse a previously saved object into this @DataPreprocessing.Node.Config object.   --- UPDATED (Dexter) 20190730
            
            Parameters
            ------------------------------

            obj `dict<str,*>` - JSON object from Project file.

            train `Train` - The train object this @DataPreprocessing.Node.Config object is attached in.
        """
        pass

    @staticmethod
    def nodeType(self) -> 'ModelNode.Types':
        """
            The node type of this object, as defined in @ModelNode.Types .   --- RESERVED --- UPDATED (Dexter) 20190508

            Returns
            ------------------------------

            `ModelNode.Types` - The node type of this object, as defined in @ModelNode.Types .
        """
        return self._nodeType
    
    @staticmethod
    def shape(self) -> 'list<list<int>>':
        """
            A list of shapes in the order of each build. Each shape should be a list of number or `None`.    --- UPDATED (Dexter) 20190508

            Returns
            ------------------------------

            `list<list<int>>` A list of shapes in the order of each build. Each shape should be a list of number or `None`.
        """
        return self._shape
    
    @staticmethod
    def order(self) -> 'list<int>':
        """
            A list of topological orders in the graph of each build.    --- UPDATED (Dexter) 20190508

            Returns
            ------------------------------

            `list<list<int>>` - A list of topological orders in the graph of each build.
        """
        return self._order
    
    @property
    def dtype(self):
        '''
			Get the data type of this layer.   --- UPDATED (Dexter) 20181214
        '''
        return self._dataType
    
    def isFinal(self) -> bool:
        """
            Abstract method to check if this layer is a final layer.   --- UPDATED (Dexter) 20190409

            Returns
            ------------------------------

            `bool` - Whether this layer is a final layer.
        """
        pass

    def switchOffWeightLogging(self):
        '''
			Optionally switch off weightLogging.   --- UPDATED (Dexter) 20190508
        '''
        self.weightLogging = False

    def _clearTempTensors(self):
        '''
			Abstract method to clear temp tensors that are on previous graphs, usually call for a new build.   --- UPDATED (Dexter) 20190508
        '''
        self._weights = []
    
    def _build(self, buildNo: int):
        '''
			Build the TensorFlow Graph of this layer.   --- UPDATED (Dexter) 20190724

            Parameters
            ------------------------------

            buildNo     `int`   - The build number to be built.
        '''
        self._clearTempTensors()
        raise ValueError("Model Node cannot be built directly. Please opt for a specific layer profile like ModelNode.Layer.Convolution or a CustomLayerProfile.")

class _ModelNodeLayerConfig(_ModelNodeConfig):
    '''
			Class representing a layer module.   --- UPDATED (Dexter) 20180921
    '''
    def __init__(self, layerType: 'ModelNode.Layer.Types',
                    name: str = None, layerUnits: int = 150, final: bool = False, 
                    incomingConfig: 'ModelNode.Layer.Incoming.Config' = _ModelNodeLayerIncoming.Concat(), 
                    linearTransform: 'Train.Variable.LinearTransform' = Train.Variable.LinearTransform.createBasicConfig(weightAvg=0, weightStdDev = 0.004, weightL1Loss = False, weightL2Loss = True, weightL2Decay = 0.004, biasInitial = 0.001),
                    activation: 'Train.Activation' = Train.Activation.Relu, activationParams: Dict[str, Any] = {}, 
                    batchNorm: bool = True, batchNormParams: Dict[str, Any] = {}, dropout: float = 1, 
                    outputConfig: 'ModelNode.Layer.Output.Config' = _ModelNodeLayerOutput.Default()):
        '''
			Create a new layer.   --- UPDATED (Dexter) 20200122

            Parameters
            ------------------------------

            layerType `ModelNode.Layer.Types` - The type of the NOM layer, as defined in @ModelNode.Layer.Types .

            name            `str`   - The name of this layer.
            
            layerUnits      `int`   - The number of hidden units in this layer.

            final           `bool`  - Whether this is a final layer (training task).

            incomingConfig  `ModelNode.Layer.Incoming.Config`    - Input configurations.

            linearTransform `Train.Variable.LinearTransform` - The linear transformation configuration.
            
            activation      `Train.Activation`   - The activation function of this layer.

            activationParams    `dict{str:*}`   - Activation parameters as defined in TensorFlow.

            batchNorm       `bool`  - Whether to use batch normalization before activation function.

            batchNormParams     `dict{str:*}`   - Batch normalization parameters as defined in TensorFlow.

            dropout         `float` - The keep probability of dropout during training.

            outputConfig    `ModelNode.Layer.Output.Config`      - Output configurations.
        '''
        # Ensure the layer name is a string.
        if name is not None and classof(name) not in ["str", "String"]:
            raise ValueError("Layer Profile Name must be a string")
        
        super().__init__(nodeType = ModelNode.Types.Layer, name = name)
        # `ModelNode.Layer.Types` - The type of the NOM layer, as defined in @ModelNode.Layer.Types .
        self._layerType = layerType
        
        ### Define layer setup configurations.
        # `int` - The number of hidden units in this layer.
        self.layerUnits = layerUnits
        # `Train.Variable.LinearTransform` - The linear transformation configuration of this layer.
        self.linearTransform = linearTransform
        # `bool` - Whether to use batch normalization before activation function.
        self.batchNorm = batchNorm
        # `dict{str:*}` - Batch normalization parameters as defined in TensorFlow.
        self.batchNormParams = batchNormParams
        # `str` - The activation function of this layer.
        self.activation = activation
        # `dict{str:*}` - Activation parameters as defined in TensorFlow.
        self.activationParams = activationParams
        # `ModelNode.Layer.Incoming.Config` - The input configurations of this layer.
        self.incomingConfig = incomingConfig
        # `ModelNode.Layer.Output.Config` - The output configurations of this layer.
        self.outputConfig = outputConfig
        
        ### Define model-related information.
        # `bool` - Whether this is a final layer (training task).   --- DEPRECATED
        self._final = final

        ### Define training-related information.
        # `float` - The keep probability of dropout during training, in terms of %.
        self.dropout = dropout
        # `tf.Tensor` - The dropout tensor to be handled within TensorFlow training.
        self._dropoutTensor = None

        self.act_dict = {Train.Activation.Relu.value: '\'relu\'',
                    Train.Activation.Relu6.value: 'tf.nn.relu6',
                    Train.Activation.Crelu.value: 'tf.nn.crelu',
                    Train.Activation.Elu.value: '\'elu\'',
                    Train.Activation.Selu.value: '\'selu\'',
                    Train.Activation.Softplus.value: '\'softplus\'',
                    Train.Activation.Softsign.value: '\'softsign\'',
                    Train.Activation.Tanh.value: '\'tanh\'',
                    Train.Activation.HardSigmoid.value: '\'hard_sigmoid\'',
                    Train.Activation.Sigmoid.value: '\'sigmoid\'',
                    Train.Activation.Linear.value: 'None', 
                    }

    def parseJSON(self, obj: Dict[str, Any], train: 'Train'):
        """
            Parse a previously saved object into this class of @ModelNode.Layer.Config.   --- UPDATED (Dexter) 20200501
            
            Parameters
            ------------------------------

            obj `dict<str,*>` - JSON object from Project file.

            train `Train` - The train object this @DataPreprocessing.Node.Config object is attached in.
        """
        # Iterate the object keys and take actions on mapping back to the @ModelNode.Layer.Config Class.
        for (k,v) in obj.items():
            if (k == "_langMap"):
                setattr(self, k, ({mk: mv for mk,mv in v}) if isinstance(v, list) else v)
            elif (k == "shape"):
                # Backward compatibility for one-dimension shape or order.   --- MAXVER 1908
                if (isinstance(v, list) and len(v)):
                    setattr(self, k, v)
                else:
                    setattr(self, k, [v])
            elif (k == "_order"):
                # Backward compatibility for one-dimension shape or order.   --- MAXVER 1908
                if (isinstance(v, list) and len(v)):
                    if (len(np.array(v)) == 2):
                        setattr(self, k, [s[0] for s in v])
                    else:
                        setattr(self, k, v)
                else:
                    setattr(self, k, [v])
            elif (k == "linearTransform"):
                if (v is not None):
                    setattr(self, k, Train.Variable.LinearTransform.createFromJSON(v))
                else:
                    setattr(self, k, None)
            elif (k == "incomingConfig"):
                setattr(self, k, ModelNode.Layer.Incoming.createFromJSON(v))
            elif (k == "outputConfig"):
                if (v is not None):
                    setattr(self, k, ModelNode.Layer.Output.createFromJSON(v))
                else:
                    setattr(self, k, None)
            elif (k == "compareTensorIdx"):
                setattr(self, "lossDppKey", v)
            elif (k == "convDilation"):
                # Old definition of convolutional dilation starts with 0.
                if ((not hasattr(train, "oldVersion")) or train.oldVersion < 190305):
                    setattr(self, k, v + 1)
                else:
                    setattr(self, k, v)
            elif (k == "name"):
                # Remove all spaces
                setattr(self, k, ModelNode.updateNodeName(v))
            elif (k == "refLayerName"):
                # Remove all spaces
                setattr(self, k, ModelNode.updateNodeName(v) if v is not None else v)
            elif (k == "activation" or "Activation" in k):
                setattr(self, k, Train.Activation(v))
            elif (k not in ["train", "_layerType", "_nodeType", "_taskType"]):
                setattr(self, k, self.enumParser(k, v))                   

        # Assign this layer to the train object.
        self.train = train

    def enumParser(self, k: str, v: Any) -> Any:
        """Abstract method to parse the given key and value, to get the expected enumeration value.   --- UPDATED (Dexter) 20200501
        """
        return v

    def isFinal(self) -> bool:
        """
            Check if this layer is a final layer.   --- UPDATED (Dexter) 20190508

            Returns
            ------------------------------

            `bool` - Whether this layer is a final layer.
        """
        return isinstance(self, ModelNode.Layer.Task.Config)

    def _build(self, buildNo: int):
        '''
			Build the TensorFlow Graph of this layer.   --- UPDATED (Dexter) 20181115

            Parameters
            ------------------------------

            buildNo     `int`   - The build number to be built.
        '''
        self._clearTempTensors()
        raise ValueError("ModelNode.Layer.Config cannot be built directly. Please opt for a specific layer profile like ModelNode.Layer.Convolution or a CustomLayerProfile.")

    def _clearTempTensors(self):
        '''
			Clear temp tensors that are on previous graphs, usually call for a new build.   --- UPDATED (Dexter) 20181110
        '''
        self._weights = []
        self._dropoutTensor = None

    def _getTensorsFromPreviousNodes(self, buildNo: int = 0) -> List['tf.Tensor']:
        '''
			Get the output tensors of the previous layer modules.   --- UPDATED (Dexter) 20181115

            Parameters
            ------------------------------

            buildNo     `int`   - The build number to be built.
            
            Returns
            ------------------------------

            `list[tf.Tensor]` - Output tensor of the previous nodes.
        '''
        return [n._outputTensor for n in self.fromNode[buildNo]]
    
    def _getTensorsFromPreviousSources(self, buildNo: int = 0) -> List['tf.Tensor']:
        '''
			Get the data source tensors of the previous sources.   --- UPDATED (Dexter) 20190506

            Parameters
            ------------------------------

            buildNo     `int`   - The build number to be built.
            
            Returns
            ------------------------------

            `list[tf.Tensor]` - Output tensor of the previous sources.
        '''
        return [self.train._sourceTensors[dppKey] for dppKey in self.fromSource[buildNo]]
    
    def _getTensorsFromAvailableInputs(self, buildNo: int = 0) -> List['tf.Tensor']:
        '''
			Get the feed-in tensors from the previous layer modules or sources.   --- DEPRECATED --- UPDATED (Dexter) 20181115

            Parameters
            ------------------------------

            buildNo     `int`   - The build number to be built.
            
            Returns
            ------------------------------

            `tf.Tensor` - Collection tensor of all incoming inputs.
        '''
        return [*self._getTensorsFromPreviousSources(buildNo = buildNo), *self._getTensorsFromPreviousNodes(buildNo = buildNo)]
    
    def _combineIncomingTensors(self, buildNo: int = 0) -> 'tf.Tensor':
        '''
			Get the feed-in tensors from the previous layer modules or sources.   --- UPDATED (Dexter) 20200115

            Parameters
            ------------------------------

            buildNo     `int`   - The build number to be built.
            
            Returns
            ------------------------------

            `tf.Tensor` - Collection tensor of all incoming inputs.
        '''
        self._inputCollections = incomingNodes = [*self._getTensorsFromPreviousSources(buildNo = buildNo), *self._getTensorsFromPreviousNodes(buildNo = buildNo)]

        # Standardize data type.
        incomingNodes = [tf.cast(node, self._dataType) if node.dtype != self._dataType else node for node in incomingNodes]
        
        # Ensure there is at least one incoming node.
        if (len(incomingNodes) == 0):
            raise ValueError("There is no input layers for this layer (" + self.name + ").")
        
        # If there is only one node, just pass the node to the layer.
        elif (len(incomingNodes) == 1):
            startTensor = incomingNodes[0]

        # If there is more than one node, collect the incoming node according to the incoming configurations.
        else:
            # Before any checking, reshape any incoming nodes of shape [None] into [None,1].
            incomingNodes = [node if len(node.shape) > 1 else tf.reshape(node, [node.shape[0], 1]) for node in incomingNodes]

            # 1)    Prepare necessary information.
            coreNodeTensor = None
            incomingShapes = [tuple(s for s in tensor.shape) for tensor in incomingNodes]
            lowestDim = min([len(s) for s in incomingShapes])
            isElementWiseOp = self.incomingConfig.instanceClass in [ModelNode.Layer.Incoming.Types.Sum, ModelNode.Layer.Incoming.Types.Multiply, ModelNode.Layer.Incoming.Types.Blend]
            
            # 1-1)  Determine the core node.
            if self.incomingConfig.coreNode is None:
                # 1-1-A1)   Find the nodes with lowest number of dimensions.
                lowDTensors = [(idx, tensor) for idx,tensor in enumerate(incomingNodes) if len(tensor.shape) == lowestDim and idx >= len(self.fromSource[buildNo])]

                # 1-1-A2)   Determine the core node according to the lowest total dimension.
                totalDims = [(idx, functools.reduce(lambda a,b:a*b.value, tensor.shape[1:-1], 1)) for idx,tensor in lowDTensors]
                lowestTotalDims = min([s for idx, s in totalDims])
                coreNodeIdx = [idx for idx,s in totalDims if s == lowestTotalDims][0]
            
            else:
                # 1-1-B1)   Ensure the coreNode is in the incoming node.
                if (self.incomingConfig.coreNode not in self.fromNode[buildNo]):
                    raise ValueError("Core node not in the from node list.")
                
                # 1-1-B2)   Assign the core node.
                coreNodeIdx = len(self.fromSource[buildNo]) + self.fromNode[buildNo].index(self.incomingConfig.coreNode)

            # 1-1-B2)    Assign the defined coreNode.
            coreNodeTensor = incomingNodes[coreNodeIdx]

            # 1-1-2)  Determine the shape of the core node.
            coreShape = tuple(s for s in coreNodeTensor.shape)
            
            # 1-2)    Determine the concatenation axis.
            axis = (len(coreShape) - 1) if self.incomingConfig.axis == -1 else self.incomingConfig.axis

            # 1-2-1)  Raise errors for illogical connections.
            if (axis >= len(coreShape)):
                # 1-2-1-A) The merge axis should be within the dimension of core node.
                raise ValueError("Axis is larger than or equal to the length of core node's shape.")

            elif any([(None in s[1:]) for s in incomingShapes]):
                # 1-2-1-B) None of the incoming shape should have None shape after the fist dimension.
                raise ValueError("There should be no varied dimensions on dimension 1 or above.")

            elif not all([all([(dim == coreShape[idx+1] or dim % coreShape[idx+1] == 0) for idx,dim in enumerate(s[1:axis])]) for s in incomingShapes]):
                # 1-2-1-C) Case: CoreNode: [None,4,4,6] IncomingNode: [None,5,5,7] => Error;   CoreNode: [None,4,4,6] IncomingNode: [None,8,8,8] => OK
                raise ValueError("Some incoming layers are not having the same dimension or multiplicative dimension as the core node along the dimensions before the concatenation axis.")
            
            # 1-2-2)  Align the dimensions of all tensors through reshaping.
            for idx,t in enumerate(incomingNodes):
                # 1-2-2-A)  If other nodes are having shape longer than the core node, reshape it as the same dimension length.
                if len(incomingShapes[idx]) > axis + 1:
                    axisShapes = incomingShapes[idx][axis:]
                    newAxisShape = functools.reduce(lambda x,y: x*y, axisShapes, 1)
                    incomingNodes[idx] = tf.reshape(t, [*Train.Variable.setAsReshape(incomingShapes[idx][:axis]), newAxisShape])
                    
                # 1-2-2-B)  If other nodes are having shape shorter than the core node, reshape it (create 1 dimension) as the same dimension length.
                elif len(incomingShapes[idx]) <= axis:
                    incomingNodes[idx] = tf.reshape(t, Train.Variable.setAsReshape([(incomingShapes[idx][dimIdx] if dimIdx < len(incomingShapes[idx]) else 1) for dimIdx in range(0,axis)]))
                    incomingShapes[idx] = tuple(s for s in incomingNodes[idx].shape)
                    
                # 1-3)  Handle dimension reduction on dimensions before operation axis. --- BETA
                if incomingShapes[idx][:axis] != coreShape[:axis]:
                    # 1-3-1) Understand how dimensions are to be split.
                    compareSplit = [(None if (dim is None or coreShape[dimIdx] is None) else int(dim/coreShape[dimIdx])) for dimIdx,dim in enumerate(incomingShapes[idx][:axis])]
                    
                    # 1-3-2A) Handle incoming nodes according to the defined methods.
                    if (self.incomingConfig.mergeDim in [ModelNode.Layer.Incoming.MergeDimMethods.SpaceToDepth, ModelNode.Layer.Incoming.MergeDimMethods.SubSample]):
                        if (len(compareSplit) == 3 and (compareSplit[1] == compareSplit[2]) and axis == len(coreShape) - 1):
                            # 1-3-2A-A) Use tensorflow space to dimension if needed.
                            #4 dim
                            incomingNodes[idx] = tf.nn.space_to_depth(t, compareSplit[1])
                        else:#3 dim
                            tempTensor = t
                            transposeIdx = []
                            newLastDimension = incomingShapes[idx][axis]

                            # Below algoritm of space to depth.
                            # 1-3-2A-B-1) Split the tensor on the needed dimensions.
                            splittedDim = 0
                            splitList = []
                            for sDim,split in enumerate(compareSplit):
                                if split is not None and split > 1:
                                    tempTensor = tf.convert_to_tensor(tf.split(tempTensor, split, axis=(sDim + splittedDim)))
                                    newLastDimension *= split
                                    splittedDim += 1
                                    splitList.append(True)
                                else:
                                    splitList.append(False)
                            
                            # 1-3-2A-B-2) Construct transpose index list
                            totalSplitDim = splittedDim
                            for tDim,s in enumerate(splitList):
                                if s:
                                    splittedDim -= 1
                                    transposeIdx.append(splittedDim)
                                else:
                                    transposeIdx.append(totalSplitDim + tDim)
                            spaceDims = set(range(0, len(tempTensor.shape))) - set(transposeIdx)
                            transposeIdx = [*transposeIdx, *spaceDims]
                            
                            # 1-3-2A-B-2) Transpose the tensor accordingly.
                            tempTensor = tf.transpose(tempTensor, transposeIdx)

                            # 1-3-2A-B-3) Reshape and update the tensor.
                            incomingNodes[idx] = tf.reshape(tempTensor, [*Train.Variable.setAsReshape([preShape for preShape in tempTensor.shape[:axis]]), newLastDimension])

                        # 1-3-2A-C)  If it's subsampling, select the first channel.
                        if self.incomingConfig.mergeDim == ModelNode.Layer.Incoming.MergeDimMethods.SubSample:
                            incomingNodes[idx] = incomingNodes[idx][...,0:incomingShapes[idx][axis]]

                    # 1-3-2B) Handle incoming nodes according to the defined methods.  --- BETA
                    elif (self.incomingConfig.mergeDim in [ModelNode.Layer.Incoming.MergeDimMethods.MaxPool, ModelNode.Layer.Incoming.MergeDimMethods.MeanPool]):
                        # Throw error if there is no multiplicable axis.
                        if (None in compareSplit[1:]):
                            raise ValueError("There should be no varied dimensions on dimension 1 or above.")
                        
                        # 1-3-2B-1) Use TensorFlow pool in case the first dimension is batch, last dimension is channel.
                        incomingNodes[idx] = tf.nn.pool(t, compareSplit[1:], "MAX" if self.incomingConfig == ModelNode.Layer.Incoming.MergeDimMethods.MaxPool else "AVG", "VALID")
                            
            # 1-4-A)  Concatenate all tensors.
            if self.incomingConfig.instanceClass == ModelNode.Layer.Incoming.Types.Concat:
                # Throw error if not all incoming nodes having dimensions equal except the axis dimension.
                firstShape = tuple(s for s in incomingNodes[0].shape)
                if (not all([all([(s == firstShape[idx] or idx == axis) for idx,s in enumerate(n.shape)]) for n in incomingNodes])):
                    raise ValueError("If it's concat, all dimensions of incoming tensors should be equal, except the axis dimension.")
                startTensor = tf.concat(incomingNodes, axis=axis)
            
            # 1-4-B) If they are having element-wise operations: --- BETA
            elif isElementWiseOp:
                # 1-4-B-1) Update incoming shapes, core node and shape.
                incomingShapes = [tuple(s for s in tensor.shape) for tensor in incomingNodes]
                coreNodeTensor = incomingNodes[coreNodeIdx]
                coreShape = tuple(s for s in coreNodeTensor.shape)
                
                # 1-4-B-2) Convert the dimensions if the core axis dimension are not the same with another node.
                with tf.compat.v1.variable_scope(self.name + "DimMap", reuse=tf.compat.v1.AUTO_REUSE) as scope:
                    for idx,t in enumerate(incomingNodes):
                        if (incomingShapes[idx] != coreShape):
                            dimMapResults = self.incomingConfig.dimensionMappingConfig.buildOn(incomingShapes[idx], toUnit = coreShape[-1], defaultDevice = self.train.device)
                            incomingShapes[idx] = dimMapResults.results
                            self._weights.extend(dimMapResults.weights)

                # 1-4-B-3) Handle transformation gate.
                if self.incomingConfig.transformGate is not None:
                    # 1-4-B-3-A) Implement original Highway Network settings if there is only two incoming nodes.
                    if (len(incomingNodes) == 2):
                        # Auto Determining the transform gate node.
                        if (self.incomingConfig.transformGate == -1):
                            incomingSources = self.fromSource[buildNo]
                            incomingLayerProfiles = self.fromNode[buildNo]
                            if (len(incomingSources) > 0):
                                longestPathNodeIdx = 0
                                longestPathNode = incomingNodes[0]
                            else:
                                lowestOrder = min([lp._order[buildNo] for lp in incomingLayerProfiles])
                                longestPathNodeIdx = [idx for idx,lp in enumerate(incomingLayerProfiles) if lp._order[buildNo] == lowestOrder][0]
                                longestPathNode = incomingNodes[longestPathNodeIdx]
                        else:
                            selGateIdx = [idx for idx,lp in enumerate(self.fromNode[buildNo]) if lp == self.incomingConfig.transformGate][0]
                            longestPathNode = incomingNodes[selGateIdx]

                        with tf.compat.v1.variable_scope(self.name + "TransGate", reuse=tf.compat.v1.AUTO_REUSE) as scope:
                            transformResults = self.incomingConfig.transformConfig.buildOn(longestPathNode, toUnit = coreShape[-1], defaultDevice = self.train.device)
                        transformGate = tf.nn.sigmoid(transformResults.results)
                        self._weights.extend(transformResults.weights)
                        transformGates = [((1-transformGate) if tIdx == longestPathNodeIdx else transformGate) for tIdx,tensor in enumerate(incomingNodes)]
                    
                    # 1-4-B-3-B) Use softmax as transformation gate if there are more than 2 incoming nodes.
                    else:
                        # Create transform gate from each incoming node.
                        allTransformResults = []
                        with tf.compat.v1.variable_scope(self.name + "TransGate", reuse=tf.compat.v1.AUTO_REUSE) as scope:
                            for t in incomingNodes:
                                transformResults = self.incomingConfig.transformConfig.buildOn(t, toUnit = coreShape[-1], defaultDevice = self.train.device)
                                allTransformResults.append(transformResults.results)
                                self._weights.extend(transformResults.weights)
                        transformGates = tf.nn.softmax(tf.transpose(tf.convert_to_tensor(allTransformResults), perm=[*[dimIdx for dimIdx in range(1, axis+2)], 0]))

                    # 1-4-B-3-2) Multiply the incoming nodes with transform gates.
                    transformedNodes = [tensor * transformGates[tIdx] for tIdx, tensor in enumerate(incomingNodes)]

                    # 1-4-B-3-3A) Sum all transformed nodes if the incoming config method is Sum.
                    if self.incomingConfig.instanceClass == ModelNode.Layer.Incoming.Types.Sum:
                        startTensor = functools.reduce(lambda a,b: a+b, transformedNodes)
                    
                    # 1-4-B-3-3B) Multiply all transformed nodes if the incoming config method is Multiply.
                    elif self.incomingConfig.instanceClass == ModelNode.Layer.Incoming.Types.Multiply:
                        startTensor = functools.reduce(lambda a,b: a*b, transformedNodes)
                    
                    # 1-4-B-3-3C) Blend all transformed nodes if the incoming config method is Blend.
                    elif self.incomingConfig.instanceClass == ModelNode.Layer.Incoming.Types.Blend:
                        prodAll = functools.reduce(lambda a,b: a*b, transformedNodes)

                        # 1-4-B-3-3C-A) Create learnable parameters as the coefficient if it is learnable.
                        if (self.incomingConfig.learnableBlend):
                            learnableVarConfig = Train.Variable.Config(initializer = Train.Variable.Initializer.Constant(1))
                            blendAlpha = learnableVarConfig.create(self.name + "BlendAlpha", [len(transformedNodes)], defaultDevice = self.train.device)
                            preSum = [tensor * blendAlpha[tIdx] for tIdx, tensor in enumerate(incomingNodes)]
                            sumAll = functools.reduce(lambda a,b: a+b, preSum)

                            learnableVarConfig = Train.Variable.Config(initializer = Train.Variable.Initializer.Constant(len(transformedNodes)))
                            blendBeta = learnableVarConfig.create(self.name + "BlendBeta", [1], defaultDevice = self.train.device)
                            startTensor = sumAll - prodAll * blendBeta
                        
                        # 1-4-B-3-3C-B) Just sum up and minus the product if it is not learnable.
                        else:
                            sumAll = functools.reduce(lambda a,b: a+b, transformedNodes)
                            startTensor = sumAll - prodAll * len(transformedNodes)

            else:
                raise ValueError("Incoming method --- ", self.incomingConfig.instanceClass.name , " is not supported yet.")

        return startTensor

    def _commonLayerOps(self, operatedLayer: 'tf.Tensor') -> 'tf.Tensor':
        '''
            Apply common layer operations: batch norm, activation, dropout to a layer-specific operated layer.   --- UPDATED (Dexter) 20191010

            Parameters
            ------------------------------

            operatedLayer     `tf.Tensor`     - The processed tensor after layer-specific operations like dense, convolution, etc.

            Returns

            ------------------------------

            `tf.Tensor`     - The output tensor of this node before final output configurations.
        '''
        # 1. Apply batch norm
        if (self.batchNorm):
            mid = tf.compat.v1.layers.batch_normalization(operatedLayer, training = self.train._bnTensor, **self.batchNormParams)
        else:
            mid = operatedLayer

        # 2. Apply activation
        if (self.activation is not None):
            mid = Train.activationFunctions(self.activation)(mid, **self.activationParams)

        # 3. Apply drop out
        if (self.dropout < 1):
            self._dropoutTensor = tf.compat.v1.placeholder(tf.float32)
            mid = tf.nn.dropout(mid, (1. - self._dropoutTensor))

        return mid

    def _processOutputTensor(self, finalTensor: 'tf.Tensor') -> 'tf.Tensor':
        '''
			Process the output tensor according to output configs.   --- UPDATED (Dexter) 20190806

            Parameters
            ------------------------------

            finalTensor     `tf.Tensor`     - The processed tensor after this node.

            Returns
            ------------------------------

            `tf.Tensor`     - The output tensor of this node after the processing defined in output configurations.
        '''
        # Prepare necessary information.
        outputConfig = self.outputConfig

        # A)  Output with no processing.
        if (outputConfig.instanceClass == ModelNode.Layer.Output.Types.Default):
            return finalTensor
        
        # B)  Flatten the output tensor.
        elif (outputConfig.instanceClass == ModelNode.Layer.Output.Types.Flatten):
            if (self.outputConfig.axis == 0 or self.outputConfig.axis == -len(finalTensor.shape)):
                raise ValueError("Axis cannot be flattened on the batch axis.")
            elif (self.outputConfig.axis >= len(finalTensor.shape) or self.outputConfig.axis < -len(finalTensor.shape)):
                raise ValueError("Flattening axis is out of range.")
            finalShape = finalTensor.shape
            flattenShape = finalShape[outputConfig.axis:]
            newAxisShape = functools.reduce(lambda x,y: x*y, flattenShape, 1)
            return tf.reshape(finalTensor, [*Train.Variable.setAsReshape(finalShape[:outputConfig.axis]), newAxisShape])
        
        # C)  Reshape the output tensor.
        elif (outputConfig.instanceClass == ModelNode.Layer.Output.Types.Reshape):
            # Get the new shape.
            newShape = [*outputConfig.shape]

            # Get the total dimension specified in the new shape.
            confirmShape = round(functools.reduce(lambda x,y: x*y, [s for s in newShape if s not in [None, -1]], 1))

            # Get the total dimension specified in the original shape.
            totalShape = round(functools.reduce(lambda x,y: x*y, [s for s in finalTensor.shape if s is not None], 1))

            # Confirm if the remaining axis (with value -1) is an integer.
            if (totalShape % confirmShape != 0):
                raise ValueError("The newly requested shape is not compatible with the layer original output shape.")
            
            # Replace the remaining axis (with value -1) with the reshaped dimension.
            idx = newShape[1:].index(-1) if -1 in newShape[1:] else newShape[1:].index(None) if None in newShape[1:] else -1
            if (idx == -1):
                if totalShape == confirmShape:
                    return tf.reshape(finalTensor, [*Train.Variable.setAsReshape(newShape)])
                else:
                    raise ValueError("The newly requested shape is not compatible with the layer original output shape.")
            else:
                newShape[idx] = round(totalShape / confirmShape)
                return tf.reshape(finalTensor, [*Train.Variable.setAsReshape(newShape)])

    def _updateOrder(self, buildNo: int = 0):
        '''
			Update the order of this layer in a chained way.   --- UPDATED (Dexter) 20181115

            Parameters
            ------------------------------

            buildNo     `int`   - The build number to be built.
        '''
        self._order[buildNo] = max([0, *[lp._order[buildNo] for lp in self.fromNode[buildNo]]]) + 1
        for lp in self.toNode[buildNo]:
            lp._updateOrder(buildNo=buildNo)

    def appendNode(self, layerProfile: 'ModelNode.Layer.Config', buildNo: int = 0):
        '''
			Append a layer next to this layer.   --- UPDATED (Dexter) 20181115

            Parameters
            ------------------------------

            layerProfile    `ModelNode.Layer.Config`  - The layer profile of the next layer after this layer.

            buildNo     `int`   - The build number to be built.
        '''
        # Ensure this is not appended on a final layer.
        if (self._final):
            raise ValueError("Layer cannot be appended on a Task Layer.")
        elif (layerProfile in self.toNode[buildNo]):
            raise ValueError("Layer already connected.")

        # Set the connection with the next layer.
        self.toNode[buildNo].append(layerProfile)
        layerProfile.fromNode[buildNo].append(self)
        self.train.updateLayerOrder(layerProfile, buildNo = buildNo)

    def addSources(self, *sources: Union[str, 'DataPreprocessing.Node.Config'], clear: bool = False, buildNo: int = 0):
        '''
			Add multiple training data sources to this layer.   --- UPDATED (Dexter) 20190508

            Parameters
            ------------------------------

            *sources    `str|DataPreprocessing.Node.Config+`     - A list of sources of the Train.sources this layer is linking to.

            clear       `bool`                              - Whether to clear previous sources.

            buildNo     `int`   - The build number to be built.
        '''
        # Clear previous sources if needed.
        if (clear):
            self.fromSource[buildNo].clear()
        
        # Append the sources to the .fromSource attribute.
        for s in sources:
            if isinstance(s, DataPreprocessing.Node.Config):
                self.fromSource[buildNo].append(s.key)
            elif isinstance(s, str):
                self.fromSource[buildNo].append(s)
            else:
                raise ValueError("Not supported type of data source is added into a layer.")

            self.train.updateLayerOrder(self, buildNo=buildNo)

    def setSymmetricLayer(self, builtLayerList: Dict[str, bool], useSymmetricWeights: bool = True, startFromThis: bool = False, untilLayer: Union[str, 'ModelNode.Layer.Config'] = None, buildNo: int = 0):
        '''
			Append the next symmetric layer on a particular layer.   --- BETA --- UPDATED (Dexter) 20190730

            Parameters
            ------------------------------

            builtLayerList      `dict{str:bool}`    - A map storing whether the corresponding symmetric layer has been built.

            useSymmetricWeights `bool`              - Whether to use symmetric weights.

            startFromThis       `bool`              - Whether this layer is the mirror layer.

            untilLayer          `str|ModelNode.Layer.Config`  - The ending symmetric layer. If None, it will be mirrored up to training sources, i.e. a ModelNode.Layer.Task.Regressor will be appended.

            buildNo             `int`   - The build number to be built.
        '''
        # Get the previous layers.
        previousLayers = [l for l in self.fromNode[buildNo]]
        previousSources = self.fromSource[buildNo]

        # Raise Error if there are multiple incoming data.
        if (len(previousLayers) + len(previousSources) > 0):
            raise ValueError("Layers not having one and only one layer node inputs are not able to be set with a symmetric network.")

        # Apply symmetric node.
        newLayer = self._copySymmetricLayerConfig(useSymmetricWeights, buildNo = buildNo)
        if startFromThis:
            newLayer.attachTo(*self.train.getEndingNodes(buildNo = buildNo), buildNo = buildNo)
        else:
            newLayer.attachTo(*[self.train.modelNodes[l.name+"_Sym"] for l in self.toNode[buildNo]], buildNo = buildNo)
        builtLayerList[self.name] = True

        # Loop each previous layers.
        for l in previousLayers:
            # Ensure the previous layer hasn't been built, and all of the next nodes of the previous layer have been built.
            if (builtLayerList[l.name] == False and all([builtLayerList[toL.name] for toL in l.toNode[buildNo]])):
                l.setSymmetricLayer(builtLayerList, buildNo = buildNo)
        
        # Loop each source.
        for s in previousSources:
            forLayers = self.train.getLayersUsingSource(*s, buildNo = buildNo)
            if (all([builtLayerList[toL.name] for toL in forLayers])):
                colConfig = self.train.getDataSource(*s)
                if (colConfig.dtype in [tf.int64, tf.int32]):
                    newLayer = ModelNode.Layer.Task.Classifier(classCount = -1, lossDppKey = s)
                else:
                    newLayer = ModelNode.Layer.Task.Regressor(lossDppKey = s)
                newLayer.attachTo(*[self.train.modelNodes[l.name+"_Sym"] for l in forLayers], buildNo = buildNo)

    def _copySymmetricLayerConfig(self, useSymmetricWeights: bool = True, buildNo: int = 0) -> 'ModelNode.Layer.Config':
        '''
			Base class method for creating a new layer profile but with a symmetric configuration. No actions taken, and a sub-class should be used.   --- BETA --- UPDATED (Dexter) 20181115

            Parameters
            ------------------------------

            useSymmetricWeights `bool`              - Whether to use symmetric weights.

            buildNo             `int`   - The build number to be built.
        '''
        if (len(self.fromNode[buildNo]) != 1):
            raise ValueError("Layers not having one and only one layer node inputs are not able to be copied in a symmetric way.")

        return self.copy(self.name+"_Sym")

    def copy(self, name: str) -> 'ModelNode.Layer.Config':
        '''
			Copy this layer profile.   --- UPDATED (Dexter) 20190730

            Parameters
            ------------------------------

            name    `str`   - The new name of the copied layer
        '''
        # Layer can't be copied if it is already built.
        if (self._built):
            raise ValueError("This layer (" + self.name + ") cannot be copied because it has been built.")
        # Ensure this is not a subclass.
        elif (not isinstance(self, ModelNode.Layer.Config)):
            raise ValueError("This layer class (" + self.__class__.__name__ + ") has not supported for copying.")
        
        # Create a new Layer Profile on this.
        return ModelNode.Layer.Config(name = name, layerUnits = self.layerUnits, final = self._final,
                            incomingConfig = self.incomingConfig, linearTransform = self.linearTransform,
                            activation = self.activation, activationParams = self.activationParams,
                            batchNorm = self.batchNorm, batchNormParams = self.batchNormParams, dropout = self.dropout, 
                            outputConfig = self.outputConfig)

    def appendOn(self, layerOrSource: Union['ModelNode.Layer.Config', 'DataPreprocessing.Node.Config'], buildNo: int = 0) -> 'ModelNode.Layer.Config':
        '''
			Append this layer on a previously another layer or data source.   --- UPDATED (Dexter) 20190730

            Parameters
            ------------------------------

            layerOrSource    `ModelNode.Layer.Config|DataPreprocessing.Node.Config`      - The layer profile or column config on which this layer append.

            buildNo             `int`   - The build number to be built.

            Returns
            ------------------------------

            `ModelNode.Layer.Config`   - This layer profile object.
        '''
        if isinstance(layerOrSource, ModelNode.Layer.Config):
            # Ensure the previous layer is attached to a training model.
            if layerOrSource.train is None:
                raise ValueError("The layer on which this layer append is not specified in any training models.")
            
            layerOrSource.train.appendNode(self, appendAt = layerOrSource, buildNo=buildNo)
            
        elif isinstance(layerOrSource, DataPreprocessing.Node.Config):
            # Ensure the column config is attached to a training model.
            if layerOrSource.source is None:
                raise ValueError("The DataPreprocessing.Node.Config object on which this layer append is not specified in any training sources.")
            elif layerOrSource.train is None:
                raise ValueError("The Source.Config object containing this DataPreprocessing.Node.Config object on which this layer append is not specified in any training models.")
            
            layerOrSource.train.appendNode(self, appendAt = layerOrSource, buildNo=buildNo)
        
        else:
            # Ensure the layerOrSource is an appropriate object.
            raise ValueError("`layerOrSource` must be a ModelNode.Layer.Config or DataPreprocessing.Node.Config object.")

        return self

    def attachTo(self, *layerOrSources: Union['ModelNode.Layer.Config', 'DataPreprocessing.Node.Config'], buildNo: int = 0) -> 'ModelNode.Layer.Config':
        '''
			Attach this layer to several previous another layer or data source.   --- UPDATED (Dexter) 20190814

            Parameters
            ------------------------------

            *layerOrSource    `ModelNode.Layer.Config|DataPreprocessing.Node.Config+`      - The layer profile or column config on which this layer append.

            Returns
            ------------------------------

            `ModelNode.Layer.Config`   - This layer profile object.
        '''
        # Ensure the layerOrSource is an appropriate object.
        if any([not (isinstance(ls, ModelNode.Layer.Config) or isinstance(ls, DataPreprocessing.Node.Config)) for ls in layerOrSources]):
            raise ValueError("`layerOrSource` must be a ModelNode.Layer.Config or DataPreprocessing.Node.Config object.")
        
        # Ensure all previous layers and sources are under the same training model.
        else:
            allPreviousTrains = [ls.train for ls in layerOrSources]
            if (allPreviousTrains[0] is None or any([t != allPreviousTrains[0] for t in allPreviousTrains])):
                raise ValueError("All of the previous layer or sources must be within the same training model.")

        # Attach this layer.
        allPreviousTrains[0].attachNode(self, *layerOrSources, buildNo = buildNo)
        
        return self

    def updateOnDemand(self, **kwargs):
        '''
			Update the model during training.   --- RESERVED for future use. --- UPDATED (Dexter) 20180630
        '''
        pass

    def printWhenTraining(self, *tensors):
        '''
			Add tensors that will be printed on console during training.   --- UPDATED (Dexter) 20180630

            Parameters
            ------------------------------

            *tensors    `tf.Tensor`     - One or multiple tensors.
        '''
        self.train._printTensors[len(self.train._printTensors):] = tensors

    def keras_call_combineIncomingTensors(self, input_sources: List[List[Optional[int]]], input_tensors: List[List[Optional[int]]]
        ,input_sources_name: List[str], input_tensors_name: List[str], output_reg: str, buildNo: int = 0):
        '''
			Get the feed-in tensors from the previous layer modules or sources.   --- UPDATED (kai Hsiang, Dexter) 20190120

            Parameters
            ------------------------------

            input_sources          `list<list<int>>`    - list of input sources shape.

            input_tensors          `list<list<int>>`    - list of input tensors shape.
            
            input_sources_name     `list<str>`          - list of input sources name.
            
            input_tensors_name     `list<str>`          - list of input tensors name.

            output_reg             `str`                - Ragister name for this layer

            buildNo                `int`               - The build number to be built.           
            
            Returns
            ------------------------------

            `tuple<list<int>, list<str>, list<str>, str>` - Output shape, list of code, list of code, register name
        '''
        incoming_shapes = []
        incoming_shapes.extend(input_sources)
        incoming_shapes.extend(input_tensors)
        incoming_names = []
        incoming_names.extend(input_sources_name)
        incoming_names.extend(input_tensors_name)
        tmp_reg = [output_reg + '_' + str(i) for i in range(len(incoming_names))]
        check_first = [0 for i in range(len(incoming_names))]
        layer_obj = []
        call_obj = []
        if len(incoming_shapes) == 0:
            raise ValueError("There is no input layers for this layer (" + self.name + ").")
        elif len(incoming_shapes) == 1:
            output_shape = incoming_shapes[0]
            output_name = incoming_names[0]
        else:
            # create counter for each incoming source
            counter = [0 for x in incoming_shapes]
            # Before any checking, reshape any incoming nodes of shape [None] into [None,1].
            for idx, shape in enumerate(incoming_shapes):
                if len(shape) == 1:
                    # self.name_combine_incomingID_counter
                    string = 'self.%s_combine_%d_%d = tf.keras.layers.Reshape((1,))'%(self.name, idx, counter[idx])
                    layer_obj.append(string)
                    if check_first[idx] == 0:
                        check_first[idx] = 1
                        A = tmp_reg[idx]
                        B = incoming_names[idx]
                    else:
                        A = tmp_reg[idx]
                        B = tmp_reg[idx]
                    string = '%s = self.%s_combine_%d_%d(%s)'%(A,self.name, idx, counter[idx],B)
                    counter[idx]+=1
            # Determine the core node
            lowestDim = min([len(s) for s in incoming_shapes])
            isElementWiseOp = self.incomingConfig.instanceClass in [ModelNode.Layer.Incoming.Types.Sum, ModelNode.Layer.Incoming.Types.Multiply, ModelNode.Layer.Incoming.Types.Blend]
            lowestidx = []
            if self.incomingConfig.coreNode is None:
                for t_idx, shape in enumerate(input_tensors):
                    if len(shape) == lowestDim:
                        totalDims = functools.reduce(lambda x,y: x*y, shape[1:-1], 1)
                        lowestidx.append((t_idx+len(input_sources), totalDims))
                lowestidx = sorted(lowestidx, key=lambda x:x[1])
                coreNodeIdx = lowestidx[0][0]
            else:
                # 1-1-B1)   Ensure the coreNode is in the incoming node.
                if (self.incomingConfig.coreNode not in self.fromNode[buildNo]):
                    raise ValueError("Core node not in the from node list.")       
                # 1-1-B2)   Assign the core node.
                coreNodeIdx = len(input_sources) + self.fromNode[buildNo].index(self.incomingConfig.coreNode)
            coreShape = incoming_shapes[coreNodeIdx]
            axis = (len(coreShape) - 1) if self.incomingConfig.axis == -1 else self.incomingConfig.axis
            '''print(type(incoming_shapes[0][1]))
            print(incoming_shapes)
            exit()'''
            for s in incoming_shapes:
                print(s)
            
            if (axis >= len(coreShape)):
                # 1-2-1-A) The merge axis should be within the dimension of core node.
                raise ValueError("Axis is larger than or equal to the length of core node's shape.")

            elif any([(None in s[1:]) for s in incoming_shapes]):
                # 1-2-1-B) None of the incoming shape should have None shape after the fist dimension.
                raise ValueError("There should be no varied dimensions on dimension 1 or above.")

            elif not all([all([(dim == coreShape[idx+1] or dim % coreShape[idx+1] == 0) for idx,dim in enumerate(s[1:axis])]) for s in incoming_shapes]):
                # 1-2-1-C) Case: CoreNode: [None,4,4,6] IncomingNode: [None,5,5,7] => Error;   CoreNode: [None,4,4,6] IncomingNode: [None,8,8,8] => OK
                raise ValueError("Some incoming layers are not having the same dimension or multiplicative dimension as the core node along the dimensions before the concatenation axis.")
            

            '''if (axis >= len(coreShape)):
                raise ValueError("Axis is larger than or equal to the length of core node's shape.")

            elif not all([(all([(dim == coreShape[idx+1] or (dim % coreShape[idx+1] == 0)) for idx,dim in enumerate(s[1:axis])])) for s in incoming_shapes]):
                # 1-2-1-B) Case: CoreNode: [None,4,4,6] IncomingNode: [None,5,5,7] => Error;   CoreNode: [None,4,4,6] IncomingNode: [None,8,8,8] => OK
                for s in incoming_shapes:
                    print(s[1:axis],coreShape[1:axis])
                    print(s,coreShape)
                raise ValueError("Some incoming layers are not having the same dimension or multiplicative dimension as the core node along the dimensions before the concatenation axis.")
            '''
            # 1-2-2)  Align the dimensions of all tensors through reshaping.
            for idx,shape in enumerate(incoming_shapes):
                if len(shape) > axis + 1: # Case: axis = 2: [None,16,16,12] => [None,16,16*12]
                    axisShapes = shape[axis:]
                    newAxisShape = functools.reduce(lambda x,y: x*y, axisShapes, 1)
                    new_shape = shape[:axis]
                    new_shape.append(newAxisShape)
                    string = 'self.%s_combine_%d_%d = tf.keras.layers.Reshape(('%(self.name, idx, counter[idx])
                    string += ','.join([str(int(x)) for x in new_shape[1:]])
                    string += ',))'
                    layer_obj.append(string)
                    if check_first[idx] == 0:
                        check_first[idx] = 1
                        A = tmp_reg[idx]
                        B = incoming_names[idx]
                    else:
                        A = tmp_reg[idx]
                        B = tmp_reg[idx]
                    string = '%s = self.%s_combine_%d_%d(%s)'%(A, self.name, idx, counter[idx], B)
                    call_obj.append(string)
                    counter[idx]+=1
                    incoming_shapes[idx] = new_shape
                elif len(shape) <= axis: # Case: axis = 2: [None,16] => [None,16,1]
                    padding_size = axis - len(shape) + 1
                    new_shape = shape.copy()
                    for i in range(padding_size):
                        new_shape.append(1)
                    string = 'self.%s_combine_%d_%d = tf.keras.layers.Reshape(('%(self.name, idx, counter[idx])
                    string += ','.join([str(int(x)) for x in new_shape[1:]])
                    string += ',))'
                    layer_obj.append(string)
                    if check_first[idx] == 0:
                        check_first[idx] = 1
                        A = tmp_reg[idx]
                        B = incoming_names[idx]
                    else:
                        A = tmp_reg[idx]
                        B = tmp_reg[idx]
                    string = '%s = self.%s_combine_%d_%d(%s)'%(A, self.name, idx, counter[idx], B)
                    call_obj.append(string)
                    counter[idx]+=1
                    incoming_shapes[idx] = new_shape
                if shape[:axis] != coreShape[:axis]:
                    # 1-3-1) Understand how dimensions are to be split.
                    compareSplit = [(None if (dim is None or coreShape[dimIdx] is None) else int(dim/coreShape[dimIdx])) for dimIdx,dim in enumerate(incoming_shapes[idx][:axis])]
                    print(compareSplit)
                    final_shape = (-1,) + tuple(coreShape[1:axis]) + (shape[-1]*functools.reduce(lambda x,y:x*y,compareSplit[1:]),)
                    mul_list = [final_shape[1]]
                    for i in range(len(final_shape[1:axis])-1):
                        mul_list.append(final_shape[i+2]*mul_list[i])
                    trans_parm = [1,0] + [i+2 for i in range(len(final_shape) - 1)]
                    block_size = compareSplit[1:]
                    # 1-3-2A) Handle incoming nodes according to the defined methods.
                    if (self.incomingConfig.mergeDim in [ModelNode.Layer.Incoming.MergeDimMethods.SpaceToDepth, ModelNode.Layer.Incoming.MergeDimMethods.SubSample]):
                        if check_first[idx] == 0:
                            check_first[idx] = 1
                            A = tmp_reg[idx]
                            B = incoming_names[idx]
                        else:
                            A = tmp_reg[idx]
                            B = tmp_reg[idx]
                        string = '%s = self.general_s2d(%s, %s, %s, %s, %s, %s)'\
                                %(A,B,'[-1,'+','.join([str(x) for x in shape[1:]])+']','['+','.join([str(x) for x in final_shape])+']'
                                ,'['+','.join([str(x) for x in mul_list])+']','['+','.join([str(x) for x in trans_parm])+']'
                                ,'['+','.join([str(x) for x in block_size])+']')
                        call_obj.append(string)
                        incoming_shapes[idx] = final_shape
                        # 1-3-2A-2-A)  If it's subsampling, select the first channel.
                        if self.incomingConfig.mergeDim == ModelNode.Layer.Incoming.MergeDimMethods.SubSample:
                            #tempTensor = incomingNodes[idx][...,0:oriLastDimension]
                            string = '%s = %s[...,0:%d]'%(A,A,coreShape[-1])
                            call_obj.append(string)
                            incoming_shapes[idx] = coreShape

                    # 1-3-2B) Handle incoming nodes according to the defined methods.  --- BETA
                    elif (self.incomingConfig.mergeDim in [ModelNode.Layer.Incoming.MergeDimMethods.MaxPool, ModelNode.Layer.Incoming.MergeDimMethods.MeanPool]):
                        # Throw error if there is no multiplicable axis.
                        if (None in compareSplit[1:]):
                            raise ValueError("There should be no varied dimensions on dimension 1 or above.")
                        
                        # 1-3-2B-1) Use TensorFlow pool in case the first dimension is batch, last dimension is channel.
                        #incomingNodes[idx] = tf.nn.pool(t, compareSplit[1:], "MAX" if self.incomingConfig == ModelNode.Layer.Incoming.MergeDimMethods.MaxPool else "AVG", "VALID")
                   
            # 1-4-A)  Concatenate all tensors.
            if self.incomingConfig.instanceClass == ModelNode.Layer.Incoming.Types.Concat:
                new_axis_dim = 0
                for idx, shape in enumerate(incoming_shapes):
                    new_axis_dim += shape[axis]
                output_shape = list(incoming_shapes[0]).copy()
                output_shape[axis] = new_axis_dim
                concate_list = []
                for idx, c in enumerate(check_first):
                    if c == 0:
                        concate_list.append(incoming_names[idx])
                    else:
                        concate_list.append(tmp_reg[idx])
                string = '%s = tf.keras.layers.concatenate(['%(output_reg) +','.join(concate_list) + '],axis =%d)'%(axis)
                call_obj.append(string)
                output_name = output_reg
            # 1-4-B) If they are having element-wise operations: --- BETA
            elif isElementWiseOp:
                raise ValueError("Not implement")
            else:
                raise ValueError("Not implement")
        return output_shape, layer_obj, call_obj, output_name
        
    def keras_call_processOutputTensor(self, input_shape: List[Optional[int]], layer_output_name: str, output_reg: str, buildNo: int = 0) :
        '''
			Get the feed-in tensors from the previous layer modules or sources.   --- UPDATED (kai Hsiang, Dexter) 20200120

            Parameters
            ------------------------------

            input_shape             `list<int>`     - The build number to be built.

            layer_output_name       `str`           - The build number to be built.
            
            output_reg              `str`           - The build number to be built.

            buildNo                 `int`           - The build number to be built.           
            
            Returns
            ------------------------------

            `tuple<list<int>, list<str>>` - Output shape, list of code
        '''
        layer_obj = []
        call_obj = []
        if layer_output_name != output_reg:
            A = output_reg
            B = layer_output_name
        else:
            A = output_reg
            B = output_reg
        # A)  Output with no processing.
        if ((self.outputConfig is None) or (self.outputConfig.instanceClass == ModelNode.Layer.Output.Types.Default)):
            return layer_obj, call_obj
        
        # B)  Flatten the output tensor.
        elif (self.outputConfig.instanceClass == ModelNode.Layer.Output.Types.Flatten):
            string = 'self.%s_output_reshape = tf.keras.layers.Flatten()'%(self.name)
            layer_obj.append(string)
            string = '%s = self.%s_output_reshape(%s)'%(A, self.name, B)
            call_obj.append(string)
            return layer_obj, call_obj
        # C)  Reshape the output tensor.
        elif (self.outputConfig.instanceClass == ModelNode.Layer.Output.Types.Reshape):

            shapeTotal = functools.reduce(lambda x,y: x*y, input_shape[1:], 1)
            newShapeTotal = functools.reduce(lambda x,y: x*y, self._shape[buildNo][1:], 1)
            if (shapeTotal != newShapeTotal):
                raise ValueError("Item shape does not equalt to the output shape.")
            
            string = 'self.%s_output_reshape = tf.keras.layers.Reshape(('%(self.name)
            string += ','.join([str(int(x)) for x in self._shape[buildNo][1:]])
            string += '))'
            layer_obj.append(string)
            string = '%s = self.%s_output_reshape(%s)'%(A, self.name, B)
            call_obj.append(string)
            return layer_obj, call_obj
    
    def get_initiallizer(self, key = 'weightConfig'):
        '''
			get initiallizer name   --- UPDATED (kai Hsiang, Dexter) 20200120

            Parameters
            ------------------------------

            key     `str`     - config type
                        
            Returns
            ------------------------------

            `str` - keras initializer code
        '''
        '''initializer_type = self.keras_initializer_info[key]['initializer']['_type']
        default = {'value':0., 'mean':0., 'stddev': 0.02, 'gain': 1.0, 'maxval': 1.0, 'minval':0.}
        for parm_name, value in self.keras_initializer_info[key]['initializer'].items():
            try:
                default[parm_name] = float(value)
            except:
                pass
        self.initializer_dictionary = \
        {'Constant':'initializers.Constant(value=%g)'%(default['value']),
         'Zeros':'initializers.Zeros()',
         'RandomNormal':'initializers.RandomNormal(mean=%g, stddev=%g, seed=1234)'%(default['mean'], default['stddev']),
         'Ones':'initializers.Ones()',
         'TruncatedNormal':'initializers.TruncatedNormal(mean=%g, stddev=%g, seed=1234)'%(default['mean'], default['stddev']),
         'RandomUniform':'initializers.RandomUniform(minval=%g, maxval=%g, seed=1234)'%(default['minval'], default['maxval']),
         'Orthogonal':'initializers.Orthogonal(gain=%g, seed=1234)'%(default['gain']),
         'Identity':'initializers.Identity(gain=%g)'%(default['gain']),
         'GlorotNormal':'initializers.glorot_normal(seed=1234)',
         'GloroUniform':'initializers.glorot_uniform(seed=1234)',
         'HeNormal':'initializers.he_normal(seed=1234)',
         'HeUniform':'initializers.he_uniform(seed=1234)',
         'LecunNormal':'initializers.lecun_normal(seed=1234)',
         'LecunUniform':'initializers.lecun_uniform(seed=1234)'}
        try:
            return self.initializer_dictionary[initializer_type]
        except:
            return None'''
        return "initializers." + getattr(self.linearTransform, key).initializer.getConstructorString()
    
    def get_regularizer(self, key = 'weightConfig'):
        '''
            get regularizer name   --- UPDATED (kai Hsiang, Dexter) 20200120

            Parameters
            ------------------------------

            key     `str`     - config type
                        
            Returns
            ------------------------------

            `str` - keras regularizer code
        '''
        L1 = getattr(self.linearTransform, key).l1Loss
        L2 = getattr(self.linearTransform, key).l2Loss
        L1Decay = getattr(self.linearTransform, key).l1Decay
        L2Decay = getattr(self.linearTransform, key).l2Decay
        if (L1 and L2) and (L1Decay != 0 and L2Decay != 0):
            return 'regularizers.l1_l2(l1=%g, l2=%g)'%(float(L1Decay),float(L2Decay))
        if L1 and L1Decay != 0:
            return 'regularizers.l1(%g)' % (float(L1Decay))
        if L2 and L2Decay != 0:
            return 'regularizers.l2(%g)' % (float(L2Decay))
        
        return 'None'
            
class _ModelNodeComputationalUnit():
    '''
			Class representing a Computational Unit module.   --- UPDATED (Kai Hsiang) 20190829
    '''
    @staticmethod
    def createFromJSON(obj: Dict[str, Any], train: 'Train') -> 'ModelNode.ComputationalUnit.Config':
        """
            Parse a previously saved object into a new @ModelNode.ModelNode.Config object. This will auto-determine the sub-class of the object, and pass the JSON object to the inner method to continue to parse.   --- UPDATED (Dexter) 20190821

            Parameters
            ------------------------------

            obj `dict<str, *>` - JSON object from Project file.
            
            train `Train` - The train of that the layers attach to.

            Returns
            ------------------------------

            `ModelNode.ComputationalUnit.Config` - A @ModelNode.ModelNode.Config object.
        """
        # Create the computational unit object.
        modelNode = ModelNode.ComputationalUnit.Config()

        # Parse the layer profile.
        modelNode.parseJSON(obj, train)
        return modelNode

    class Config(_ModelNodeConfig):
        def __init__(self, name:str = None, unitNamespace:str = None):
            '''
                Create a Cpomputational Unit.   --- UPDATED (Kai Hsiang, Dexter) 20200122

                Parameters
                ------------------------------

                name                `str`                           - The name of this layer.

                unitNamespace `str` - The computational unit namespace.
            '''
            super().__init__(nodeType = ModelNode.Types.ComputationalUnit, name = name)
            # `bool` - Whether this unit is needed to be initialized the layer.
            self._tfInit = True
            # `int` - The namespace of the Computational Unit.
            self._unitNamespace = unitNamespace
            # `str` - The TensorFlow computational unit function name.
            self._tfFunctionName = None
            # `dict<(int|str),*>` - The TensorFlow function parameters, definitions and corresponding values.
            self._tfParams = {}
            # `int` - Maximum count of incoming tensors.
            self._tfMaxIncomingCount = None
            # `bool` - Whether this unit is behaved differently on train and test stage.
            self._tfTrainingStage = None

        def parseJSON(self, obj: Dict[str, Any], train: 'Train'):
            """
                Parse a previously saved object into this class of @ComputationalUnitConfig .   --- UPDATED (Dexter) 20200122

                Parameters
                ------------------------------

                obj `dict<str,*>` - JSON object from Project file

                train `Train` - The train of that the layers attach to.
            """
            for (k, v) in obj.items():
                if (k == "name"):
                    setattr(self, k, ModelNode.updateNodeName(v).replace(' ',''))
                elif (k == "shape"):
                    # Backward compatibility for one-dimension shape or order.   --- MAXVER 1908
                    if (isinstance(v, list) and len(v) > 0):
                        setattr(self, k, v)
                    else:
                        setattr(self, k, [v])
                elif (k == "_order"):
                    # Backward compatibility for one-dimension shape or order.   --- MAXVER 1908
                    if (isinstance(v, list) and len(v)):
                        if (len(np.array(v)) == 2):
                            setattr(self, k, [s[0] for s in v])
                        else:
                            setattr(self, k, v)
                    else:
                        setattr(self, k, [v])
                elif (k in ["fromNode", "toNode"]):
                    setattr(self, k, [[s.replace(' ','') for s in v[0]]])
                elif (k not in ["_train", "_nodeType"]):
                    setattr(self, k, v)

            # Assign this layer to the train object.
            self.train = train
        
        @property
        def unitNamespace() -> 'int':
            """
                The namespace of the Computational Unit.   --- UPDATED (Dexter) 20190819

                Returns
                ------------------------------

                `int` - The namespace of the Computational Unit.
            """
            return self._unitNamespace

        def keras_call_layer_build(self, reshape_output_name, output_reg, inputshape = None):
            '''
                Create keras layer code   --- UPDATED (kai Hsiang) 20190909

                Parameters
                ------------------------------

                reshape_output_name     `str`                   - register name from auto reshape stage

                output_reg              `str`                   - register name
                
                inputshape              `list<tuple<int>>`      - list of input tensor shape.
                            
                Returns
                ------------------------------

                `list<str>, list<str>, <str>, list<tuple<int>>` - list of code, list of code, register name, Output shape
            '''
            if self._tfInit == False:
                layer_list = []
                call_list = []
                args = []
                kwargs = []
                for p in self._tfParams:
                    if isinstance(p[0], int):
                        try:
                            args.append((int(p[0]),p[1]['pyArgumentValue']))
                        except:
                            pass
                    else:
                        try:
                            kwargs.append(p[0] + '=' + p[1]['pyArgumentValue'])
                        except:
                            pass
                args = sorted(args, key = lambda x:x[0])
                args = [str(x[1]) for x in args]
                args.extend(kwargs)
                if self._tfMaxIncomingCount == 1:
                    if len(reshape_output_name) > 1:
                        string = '%s = %s([' % (output_reg,self._tfFunctionName)
                        string += ','.join(reshape_output_name)
                        string += '],'
                        string += ','.join(args)
                        string += ')'
                        call_list.append(string)
                    else:
                        string = '%s = %s(' % (output_reg,self._tfFunctionName)
                        string += reshape_output_name[0]
                        if len(args) >0:
                            string += ','
                            string += ','.join(args)
                        string += ')'
                        call_list.append(string)
                else:
                    if len(reshape_output_name) > 1:
                        string = '%s = %s(' % (output_reg,self._tfFunctionName)
                        string += ','.join(reshape_output_name)
                        if len(args) >0:
                            string += ','
                            string += ','.join(args)
                        string += ')'
                        call_list.append(string)
                    else:
                        string = '%s = %s(' % (output_reg,self._tfFunctionName)
                        string += reshape_output_name[0]
                        string += ',1'
                        if len(args) >0:
                            string += ','
                            string += ','.join(args)
                        string += ')'
                        call_list.append(string)

                '''
                if len(reshape_output_name) > 1:
                    string = '%s = %s([' % (output_reg,self._tfFunctionName)
                    string += ','.join(reshape_output_name)
                    string += '],'
                    string += ','.join(args)
                    string += ')'
                    call_list.append(string)
                else:
                    string = '%s = %s(' % (output_reg,self._tfFunctionName)
                    string += reshape_output_name[0]
                    string += ','
                    string += ','.join(args)
                    string += ')'
                    call_list.append(string)
                '''
                
            if self._tfInit == True:
                layer_list = []
                call_list = []
                # get tf params
                args = []
                kwargs = []
                for p in self._tfParams:
                    if isinstance(p[0], int):
                        try:
                            args.append((int(p[0]),p[1]['pyArgumentValue']))
                        except:
                            pass
                    else:
                        try:
                            kwargs.append(p[0] + '=' + p[1]['pyArgumentValue'])
                        except:
                            pass
                
                args = sorted(args, key = lambda x:x[0])
                args = [str(x[1]) for x in args]
                args.extend(kwargs)
                string = 'self.%s = %s(' %(self.name, self._tfFunctionName)
                string += ','.join(args)
                string += ')'
                layer_list.append(string)
                if self._tfMaxIncomingCount == 1:
                    if len(reshape_output_name) > 1:
                        string = '%s = self.%s([' % (output_reg,self.name)
                        string += ','.join(reshape_output_name)
                        string += ']'
                        if self._tfTrainingStage:
                            string += ', training=training'
                        string += ')'
                    else:
                        string = '%s = self.%s(' % (output_reg,self.name)
                        string += reshape_output_name[0]
                        if self._tfTrainingStage:
                            string += ', training=training'
                        string += ')'
                else:
                    if len(reshape_output_name) > 1:
                        string = '%s = self.%s(' % (output_reg,self.name)
                        string += ','.join(reshape_output_name)
                        #string += ''
                        if self._tfTrainingStage:
                            string += ', training=training'
                        string += ')'
                    else:
                        string = '%s = self.%s(' % (output_reg,self.name)
                        string += reshape_output_name[0]
                        if self._tfTrainingStage:
                            string += ',1, training=training'
                        string += ')'
                '''
                if len(reshape_output_name) > 1:
                    string = '%s = self.%s([' % (output_reg,self.name)
                    string += ','.join(reshape_output_name)
                    string += ']'
                    if self._tfTrainingStage:
                        string += ', training=training'
                    string += ')'
                else:
                    string = '%s = self.%s(' % (output_reg,self.name)
                    string += reshape_output_name[0]
                    if self._tfTrainingStage:
                        string += ', training=training'
                    string += ')'
                '''
                call_list.append(string)

            return layer_list, call_list, output_reg, inputshape[0]

        def keras_call_combineIncomingTensors(self, input_sources, input_tensors, input_sources_name, input_tensors_name, output_reg, buildNo = 0):
            incoming_names = []
            incoming_names.extend(input_sources_name)
            incoming_names.extend(input_tensors_name)
            incoming_shapes = []
            incoming_shapes.extend(input_sources)
            incoming_shapes.extend(input_tensors)
            return incoming_shapes, [], [], incoming_names
            
        def keras_call_processOutputTensor(self, input_shape, layer_output_name, output_reg, buildNo = 0) :
            return [], []

        def keras_call_auto_reshape_build(self, input_shape, combine_output_name, output_reg):
            '''
                Create keras reshape layer code   --- UPDATED (kai Hsiang) 20190909

                Parameters
                ------------------------------

                inputshape              `list<tuple<int>>`          - list of input tensor shape.
                
                combine_output_name     `str`                       - register name from auto reshape stage

                output_reg              `str`                       - register name
                            
                Returns
                ------------------------------

                `list<str>, list<str>, <str>, list<tuple<int>>` - list of code, list of code, register name, Output shape
            '''
            return [], [], combine_output_name, input_shape

class _ModelNodeLayerCollector(_ModelNodeLayerConfig):
    '''
			Class representing a collector layer, without any dimensional transformation from all the input layers.   --- UPDATED (Dexter) 20181105
    '''
    def __init__(self, name: str = None, 
                activation: 'Train.Activation' = Train.Activation.Linear, activationParams: Dict = {}, 
                incomingConfig: 'ModelNode.Layer.Incoming.Config' = _ModelNodeLayerIncoming.Concat(), 
                batchNorm: bool = True, batchNormParams: Dict = {}, dropout: float = 1,
                outputConfig: 'ModelNode.Layer.Output.Config' = _ModelNodeLayerOutput.Default()):
        '''
			Create a collector layer, without any dimensional transformation from all the input layers.   --- UPDATED (Dexter) 20200117

            Parameters
            ------------------------------

            name            `str`   - The name of this layer.

            activation      `Train.Activation`   - The activation function of this layer.
            
            activationParams    `dict{str:*}`   - Activation parameters as defined in TensorFlow.
            
            incomingConfig  `ModelNode.Layer.Incoming.Config`    - Input configurations.

            batchNorm       `bool`  - Whether to use batch normalization before activation function.
            
            batchNormParams     `dict{str:*}`   - Batch normalization parameters as defined in TensorFlow.
            
            dropout         `float` - The keep probability of dropout during training.

            outputConfig    `ModelNode.Layer.Output.Config`      - Output configurations.
        '''
        super().__init__(ModelNode.Layer.Types.Collector, name = name, layerUnits = -1, 
                        activation = activation, activationParams = activationParams, incomingConfig = incomingConfig,
                        batchNorm = batchNorm, batchNormParams = batchNormParams, dropout = dropout, outputConfig = outputConfig)
    
    def _copySymmetricLayerConfig(self, useSymmetricWeights: bool = True, buildNo: int = 0) -> 'ModelNode.Layer.Collector':
        '''
			Create a symmetric layer profile, but this class is not supported.   --- UPDATED (Dexter) 20181105

            Parameters
            ------------------------------

            useSymmetricWeights `bool`              - Whether to use symmetric weights.

            buildNo             `int`   - The build number to be built.
        
            Returns
            ------------------------------

            `ModelNode.Layer.Collector`    - A copied profile of this layer.
        '''
        if (len(self.fromNode[buildNo]) != 1):
            raise ValueError("Layers not having one and only one layer node inputs are not able to be copied in a symmetric way.")

        l = self.copy(self.name + "_Sym")
        if (useSymmetricWeights):
            l.refLayerName = self.name
        return l

    def copy(self, name: str) -> 'ModelNode.Layer.Collector':
        '''
			Copy this layer profile.   --- UPDATED (Dexter) 20190730

            Parameters
            ------------------------------

            name    `str`   - The new name of the copied layer

            Returns
            ------------------------------

            `ModelNode.Layer.Collector`    - A copied profile of this layer.
        '''
        # Ensure this is not a subclass.
        if (not isinstance(self, ModelNode.Layer.Collector)):
            raise ValueError("This layer class (" + self.__class__.__name__ + ") has not supported for copying.")
        
        # Create a new Layer Profile on this.
        return ModelNode.Layer.Collector(name = name, activation = self.activation, activationParams = self.activationParams, incomingConfig = self.incomingConfig,
                        batchNorm = self.batchNorm, batchNormParams = self.batchNormParams, dropout = self.dropout, outputConfig = self.outputConfig)
    
    def _build(self, buildNo: int):
        '''
			Build the TensorFlow Graph of this layer.   --- UPDATED (Dexter) 20181221

            Parameters
            ------------------------------

            buildNo     `int`   - The build number to be built.
        '''
        self._clearTempTensors()

        # 0. Ensure all incoming nodes have been built.
        if (all([n._built for n in self.fromNode[buildNo]])):
            # 1. Collect the incoming inputs.
            mid = self._combineIncomingTensors(buildNo = buildNo)

            # TensorFlow Graph building, using the layer name as the scope.
            with tf.compat.v1.variable_scope(self.name, reuse=tf.compat.v1.AUTO_REUSE) as scope:
                # 2. General layer operations.
                mid = self._commonLayerOps(mid)

                # 5. Final output
                self._outputTensor = self._processOutputTensor(mid)

            self._built = True

            # Create chain build actions
            for n in self.toNode[buildNo]:
                n._build(buildNo)
    
    def keras_call_layer_build(self, reshape_output_name, output_reg, inputshape = None):
        '''
			Create keras layer code   --- UPDATED (kai Hsiang, Dexter) 20200501

            Parameters
            ------------------------------

            reshape_output_name     `str`                   - register name from auto reshape stage

            output_reg              `str`                   - register name
            
            inputshape              `list<tuple<int>>`      - list of input tensor shape.
                        
            Returns
            ------------------------------

            `list<str>, list<str>, <str>, list<tuple<int>>` - list of code, list of code, register name, Output shape
        '''
        if reshape_output_name != output_reg:
            A = output_reg
            B = reshape_output_name
        else:
            A = output_reg
            B = output_reg
        output_name = reshape_output_name
        if self.activation == Train.Activation.HardSigmoid:
            act = 'tf.nn.hard_sigmoid'
        else:
            act = 'tf.nn.' + self.activation.name.lower()
       
        layer_list, call_list = [],[]
        if self.activation != Train.Activation.Linear:
            output_name = output_reg
            call = '%s = %s(%s)'% (A,act,B)
            call_list.append(call)
        
        if self.batchNorm:
            layer_list.append('self.%s_bn = BatchNormalization()'%(self.name))
            call_list.append('%s = self.%s_bn(%s, training=training)'%(A,self.name,A))
            return layer_list, call_list, output_name, inputshape
        else:
            return layer_list, call_list, output_name, inputshape
    
    def keras_call_auto_reshape_build(self, input_shape, combine_output_name, output_reg):
        '''
			Create keras reshape layer code   --- UPDATED (kai Hsiang) 20190909

            Parameters
            ------------------------------

            inputshape              `list<tuple<int>>`          - list of input tensor shape.
            
            combine_output_name     `str`                       - register name from auto reshape stage

            output_reg              `str`                       - register name
                        
            Returns
            ------------------------------

            `list<str>, list<str>, <str>, list<tuple<int>>` - list of code, list of code, register name, Output shape
        '''
        layer_obj = []
        call_obj = []
        return layer_obj, call_obj, combine_output_name, input_shape

class _ModelNodeLayerFullyConnected(_ModelNodeLayerConfig):
    '''
			Class representing a basic layer, aka fully connected layer, dense layer, etc.   --- UPDATED (Dexter) 20181105
    '''
    def __init__(self, name: str = None, layerUnits: int = 150, flattenToAxis: int = 1, refLayerName: str = None, refLayerTranspose: bool = False,
                    incomingConfig: 'ModelNode.Layer.Incoming.Config' = _ModelNodeLayerIncoming.Concat(), 
                    linearTransform: 'Train.Variable.LinearTransform' = Train.Variable.LinearTransform.createBasicConfig(weightAvg = 0, weightStdDev = 0.004, weightL1Loss = False, weightL2Loss = True, weightL2Decay = 0.004, biasInitial = 0.001),
                    activation: 'Train.Activation' = Train.Activation.Relu, activationParams: Dict = {}, 
                    batchNorm: bool = True,  batchNormParams: Dict = {}, dropout: float = 1, 
                    outputConfig: 'ModelNode.Layer.Output.Config' = _ModelNodeLayerOutput.Default(),
                    buildNo = 0, weightDecayRate = 0.004, biasInitial = 0.001, weightAvg=0, weightStdDev=0.04, weightL1Loss=False, weightL2Loss=True):
        '''
			Create a basic layer, aka fully connected layer, dense layer, etc.   --- UPDATED (Dexter) 20200117

            Parameters
            ------------------------------

            name            `str`   - The name of this layer.
            
            layerUnits      `int`   - The number of hidden units in this layer.
            
            flattenToAxis  `int`   - The flattening axis after consolidating all preceding layers.
            
            refLayerName    `str`   - Any referenced weight that this layer mirrors for.

            refLayerTranspose   `bool`  - Whether transpose is required for the referenced weight.
            
            incomingConfig  `ModelNode.Layer.Incoming.Config`    - Input configurations.

            linearTransform `Train.Variable.LinearTransform` - The linear transformation configuration.

            activation      `Train.Activation`   - The activation function of this layer.
            
            activationParams    `dict{str:*}`   - Activation parameters as defined in TensorFlow.
            
            batchNorm       `bool`  - Whether to use batch normalization before activation function.
            
            batchNormParams     `dict{str:*}`   - Batch normalization parameters as defined in TensorFlow.
            
            dropout         `float` - The keep probability of dropout during training.
            
            outputConfig    `ModelNode.Layer.Output.Config`      - Output configurations.

            buildNo         `int`   - The build number of staged training   [Deprecated]
            
            weightDecayRate `float` - The constant for weighting L2-loss of weights   [Deprecated]
            
            biasInitial     `float` - Constant initial value of biases   [Deprecated]
            
            weightAvg       `float` - The average value of the initialization of weights   [Deprecated]
            
            weightStdDev    `float` - The standard deviation of the initialization of weights   [Deprecated]
            
            weightL1Loss    `bool`  - Whether to take L1-loss on the weights   [Deprecated]
            
            weightL2Loss    `bool`  - Whether to take L2-loss on the weights   [Deprecated]
        '''
        if (linearTransform is None and any([param is not None for param in [weightAvg, weightStdDev, weightL1Loss, weightL2Loss, weightDecayRate, biasInitial]])):
            linearTransform = Train.Variable.LinearTransform.createBasicConfig(weightAvg = weightAvg, weightStdDev = weightStdDev, 
                              weightL1Loss = weightL1Loss, weightL2Loss = weightL2Loss, weightL2Decay = weightDecayRate, biasInitial = biasInitial)
        super().__init__(ModelNode.Layer.Types.FullyConnected, name = name, layerUnits = layerUnits, 
                    incomingConfig = incomingConfig, linearTransform = linearTransform, 
                    activation = activation, activationParams = activationParams, 
                    batchNorm = batchNorm, batchNormParams = batchNormParams, 
                    dropout = dropout, outputConfig = outputConfig)
        self.flattenToAxis = flattenToAxis
        self.refLayerName = refLayerName
        self.refLayerTranspose = refLayerTranspose

    def _copySymmetricLayerConfig(self, useSymmetricWeights: bool = True, buildNo: int = 0) -> 'ModelNode.Layer.FullyConnected':
        '''
			Create a symmetric layer profile.   --- BETA --- UPDATED (Dexter) 20181105

            Parameters
            ------------------------------

            useSymmetricWeights `bool`              - Whether to use symmetric weights.

            buildNo             `int`   - The build number to be built.
        
            Returns
            ------------------------------

            `ModelNode.Layer.FullyConnected`    - A copied profile of this layer.
        '''
        if (len(self.fromNode[buildNo]) != 1):
            raise ValueError("Layers not having one and only one layer node inputs are not able to be copied in a symmetric way.")

        l = self.copy(self.name + "_Sym")
        if (useSymmetricWeights):
            l.refLayerName = self.name
            l.refLayerTranspose = True
        return l

    def copy(self, name: str) -> 'ModelNode.Layer.FullyConnected':
        '''
			Copy this layer profile.   --- UPDATED (Dexter) 20180730

            Parameters
            ------------------------------

            name    `str`   - The new name of the copied layer

            Returns
            ------------------------------

            `ModelNode.Layer.FullyConnected`    - A copied profile of this layer.
        '''
        # Ensure this is not a subclass.
        if (not isinstance(self, ModelNode.Layer.FullyConnected)):
            raise ValueError("This layer class (" + self.__class__.__name__ + ") has not supported for copying.")
        
        # Create a new Layer Profile on this.
        return ModelNode.Layer.FullyConnected(name=name, layerUnits = self.layerUnits, flattenToAxis = self.flattenToAxis, refLayerName = self.refLayerName, refLayerTranspose=self.refLayerTranspose,
                    incomingConfig = self.incomingConfig, linearTransform=self.linearTransform, 
                    activation = self.activation, activationParams = self.activationParams, 
                    batchNorm = self.batchNorm, batchNormParams = self.batchNormParams, 
                    dropout = self.dropout, outputConfig = self.outputConfig)

    def _build(self, buildNo: int):
        '''
			Build the TensorFlow Graph of this layer.   --- UPDATED (Dexter) 20190210

            Parameters
            ------------------------------

            buildNo     `int`   - The build number to be built.
        '''
        self._clearTempTensors()

        # 0. Ensure all incoming nodes have been built.
        if (all([n._built for n in self.fromNode[buildNo]])):
            # 1. Get refrenced layer weights if required.
            w = None
            if (self.refLayerName is not None):
                refLayer = self.train.modelNodes[self.refLayerName]

                with tf.compat.v1.variable_scope(self.refLayerName, reuse=tf.compat.v1.AUTO_REUSE) as scope:
                    refW = Train.Variable.getFromGraph("weight")
                
                if self.refLayerTranspose:
                    with tf.compat.v1.variable_scope(self.name, reuse=tf.compat.v1.AUTO_REUSE) as scope:
                        w = tf.transpose(refW, [1,0])
                else:
                    w = refW
            
            # 2. Collect the incoming inputs.
            mid = self._combineIncomingTensors(buildNo = buildNo)

            # TensorFlow Graph building, using the layer name as the scope.
            with tf.compat.v1.variable_scope(self.name, reuse=tf.compat.v1.AUTO_REUSE) as scope:
                # 3. Perform linear Transform.
                linearTransformResults = self.linearTransform.buildOn(mid, toUnit = self.layerUnits, axis = self.flattenToAxis, weightSharing = w, defaultDevice = self.train.device)
                mid = linearTransformResults.results
                self._weights.extend(linearTransformResults.weights)
            
                # 4. General layer operations.
                mid = self._commonLayerOps(mid)

                # 5. Final output
                self._outputTensor = self._processOutputTensor(mid)

            self._built = True

            # Create chain build actions
            for n in self.toNode[buildNo]:
                n._build(buildNo)
    
    def keras_call_layer_build(self, reshape_output_name, output_reg, inputshape = None):
        '''
			Create keras layer code   --- UPDATED (kai Hsiang, Dexter) 20200120

            Parameters
            ------------------------------

            reshape_output_name     `str`                   - register name from auto reshape stage

            output_reg              `str`                   - register name
            
            inputshape              `list<tuple<int>>`      - list of input tensor shape.
                        
            Returns
            ------------------------------

            `list<str>, list<str>, <str>, list<tuple<int>>` - list of code, list of code, register name, Output shape
        '''
        new_shape = [None,self.layerUnits]
        if reshape_output_name != output_reg:
            A = output_reg
            B = reshape_output_name
        else:
            A = output_reg
            B = output_reg

        act = self.act_dict[self.activation.value]
        w_init = self.get_initiallizer()
        b_init = self.get_initiallizer(key = 'biasConfig')
        w_reg = self.get_regularizer()
        b_reg = self.get_regularizer(key = 'biasConfig')
        layer_list, call_list = [],[]
        layer = 'self.%s = tf.keras.layers.Dense(%d, activation = %s,  kernel_initializer=%s, bias_initializer=%s, kernel_regularizer=%s, bias_regularizer=%s)'% \
                (self.name, self.layerUnits, act, w_init, b_init, w_reg, b_reg)
        layer_list.append(layer)
        if self.refLayerName is not None:
            self.refLayerName = self.refLayerName.replace(' ','')
            inputshape = [str(x) for x in inputshape]
            layer_list.append('self.%s.build(('%(self.name) + ','.join(inputshape) + '))')
            layer_list.append('self.%s.kernel = None'%(self.name))
            if self.refLayerTranspose:
                call_list.append('self.%s.kernel = tf.transpose(self.%s.kernel, perm=[1, 0])'%(self.name, self.refLayerName))
            else:
                call_list.append('self.%s.kernel = self.%s.kernel'%(self.name, self.refLayerName))
        call = '%s = self.%s(%s)'% (A,self.name,B)
        call_list.append(call)
        if self.dropout < 100:
            d_rate = (100 - self.dropout)/100
            layer_list.append('self.%s_dp = tf.keras.layers.Dropout(%g)'%(self.name, d_rate))
            call_list.append('%s = self.%s_dp(%s)'%(A,self.name,A))
        if self.batchNorm:
            layer_list.append('self.%s_bn = BatchNormalization()'%(self.name))
            call_list.append('%s = self.%s_bn(%s, training=training)'%(A,self.name,A))
            return layer_list, call_list, output_reg, new_shape
        else:
            return layer_list, call_list, output_reg, new_shape
        
    def keras_call_auto_reshape_build(self, input_shape, combine_output_name, output_reg):
        '''
			Create keras reshape layer code   --- UPDATED (kai Hsiang) 20190909

            Parameters
            ------------------------------

            inputshape              `list<tuple<int>>`          - list of input tensor shape.
            
            combine_output_name     `str`                       - register name from auto reshape stage

            output_reg              `str`                       - register name
                        
            Returns
            ------------------------------

            `list<str>, list<str>, <str>, list<tuple<int>>` - list of code, list of code, register name, Output shape
        '''
        layer_obj = []
        call_obj = []
        if self.flattenToAxis < 0:
            axis = len(input_shape) + self.flattenToAxis
        else:
            axis = self.flattenToAxis
        if combine_output_name != output_reg:
            A = output_reg
            B = combine_output_name
        else:
            A = output_reg
            B = output_reg
        output_name = output_reg
        if axis == len(input_shape) - 1:
            new_shape = input_shape
            output_name = combine_output_name
        elif axis == 1:
            new_shape = input_shape[0:axis]
            new_shape.append(functools.reduce(lambda x,y: x*y, input_shape[axis:], 1))
            string = 'self.%s_auto_reshape = tf.keras.layers.Flatten()'%(self.name)
            layer_obj.append(string)
            string = '%s = self.%s_auto_reshape(%s)'%(A, self.name, B)
            call_obj.append(string)
        elif axis > 1 and axis < len(input_shape) - 1:
            new_shape = input_shape[0:axis]
            new_shape.append(functools.reduce(lambda x,y: x*y, input_shape[axis:], 1))
            string = 'self.%s_auto_reshape = tf.keras.layers.Reshape(('%(self.name)
            string += ','.join([str(int(x)) for x in new_shape[1:]])
            string += '))'
            layer_obj.append(string)
            string = '%s = self.%s_auto_reshape(%s)'%(A, self.name, B)
            call_obj.append(string)
        else:
            raise ValueError("Linear Transformation axis should within the dimension of the incoming tensor.")
        
        return layer_obj, call_obj, output_name, new_shape

class CustomLayerProfile(_ModelNodeLayerConfig):
    '''
			Class representing a customized layer profile.  --- BETA --- UPDATED (Dexter) 20181105
    '''
    def __init__(self, name: str = None, cuzBuild: Callable = None, layerUnits: int = 150, 
                incomingConfig: 'ModelNode.Layer.Incoming.Config' = _ModelNodeLayerIncoming.Concat(), 
                activation: 'Train.Activation' = Train.Activation.Relu, activationParams: Dict = {}, 
                batchNorm: bool = True,  batchNormParams: Dict = {}, dropout: float = 1, 
                outputConfig: 'ModelNode.Layer.Output.Config' = _ModelNodeLayerOutput.Default(), buildNo = 0, **cuzArgs):
        '''
			Create a new customized layer.   --- BETA --- UPDATED (Dexter) 20200117

            Parameters
            ------------------------------

            name            `str`   - The name of this layer.
            
            cuzBuild        `Callable`  - A callback function for processing tensors during model building.

            layerUnits      `int`   - The number of hidden units in this layer.

            incomingConfig  `ModelNode.Layer.Incoming.Config`    - Input configurations.

            activation      `Train.Activation`   - The activation function of this layer.

            activationParams    `dict{str:*}`   - Activation parameters as defined in TensorFlow.

            batchNorm       `bool`  - Whether to use batch normalization before activation function.

            batchNormParams     `dict{str:*}`   - Batch normalization parameters as defined in TensorFlow.

            dropout         `float` - The keep probability of dropout during training.

            outputConfig    `ModelNode.Layer.Output.Config`      - Output configurations.

            buildNo         `int`   - The build number of staged training.   [Deprecated]

            *cuzArgs        `dict{str:*}`       - Customized parameter key-value pairs.
        '''
        super().__init__(None, name=name, layerUnits = layerUnits, 
                    incomingConfig = incomingConfig,
                    activation = activation, activationParams = activationParams, 
                    batchNorm = batchNorm, batchNormParams = batchNormParams, dropout = dropout, 
                    outputConfig = outputConfig)
        self._customizeBuild = cuzBuild
        self.cuzArgs = cuzArgs

    def _build(self, buildNo: int):
        '''
			Build the TensorFlow Graph of this layer.   --- BETA --- UPDATED (Dexter) 20180630

            Parameters
            ------------------------------

            buildNo     `int`   - The build number to be built.
        '''
        self._clearTempTensors()

        # 0. Ensure all incoming nodes have been built.
        if (all([n._built for n in self.fromNode[buildNo]])):
            # 1. Collect all incoming tensors, and confirm there is incoming tensor
            fromTensor = self._combineIncomingTensors(buildNo = buildNo)

            # 2. If there is a custom build function, apply it instead of the default linear model below.
            if (self._customizeBuild is not None):
                self._outputTensor = self._processOutputTensor(self._customizeBuild(self, fromTensor))
            else:
                raise ValueError("No build action is specified.")

            self._built = True
            # Create chain build actions
            for n in self.toNode[buildNo]:
                n._build(buildNo)
    
    def appendWeightTensors(self, *weightTensors):
        '''
			Append a weight tensor that is loggable during trainning.   --- BETA --- UPDATED (Dexter) 20180630

            Parameters
            ------------------------------

            *weightTensors  `tf.Tensor` - One or multiple weight tensors.
        '''
        self._weights[len(self._weights):] = weightTensors
    
    def setOutputTensor(self, outputTensor):
        '''
			Set the output tensor of this layer.   --- BETA --- UPDATED (Dexter) 20180630

            Parameters
            ------------------------------

            outputTensor  `tf.Tensor` - The output tensor.
        '''
        self._outputTensor = outputTensor
    
    def applyBatchNorm(self, tensor):
        '''
			Connect a given tensor to optional batch normalization.   --- BETA --- UPDATED (Dexter) 20191001

            Parameters
            ------------------------------

            tensor  `tf.Tensor` - The tensor just before batch normalization.
        '''
        if (self.batchNorm):
            return tf.compat.v1.layers.batch_normalization(tensor, training = self.train._bnTensor, **self.batchNormParams)
        else:
            return tensor

    def applyActivationFunction(self, tensor):
        '''
			Apply activation function on a given tensor.   --- BETA --- UPDATED (Dexter) 20180630

            Parameters
            ------------------------------

            tensor  `tf.Tensor` - The tensor just before activation function.
        '''
        return Train.activationFunctions(self.activation)(tensor, **self.activationParams)

    def applyDropout(self, tensor):
        '''
			Apply optional dropout on a given tensor.   --- BETA --- UPDATED (Dexter) 20191010

            Parameters
            ------------------------------

            tensor  `tf.Tensor` - The tensor just before dropout.
        '''
        if (self.dropout < 1):
            self._dropoutTensor = tf.compat.v1.placeholder(tf.float32)
            return tf.nn.dropout(tensor, (1. - self._dropoutTensor))
        else:
            return tensor
    
    def setInputCollections(self, tensor):
        '''
			Define the collected input tensors before further processes.   --- BETA --- UPDATED (Dexter) 20180630

            Parameters
            ------------------------------

            tensor  `tf.Tensor` - The tensor just before dropout.
        '''
        self._inputCollections = tensor

class _ModelNodeLayerGroup(_ModelNodeLayerConfig):
    '''
			Class representing a convolutional layer.   --- UPDATED (Dexter) 20181105
    '''
    def __init__(self, name: str = None, layerUnits: int = 0, refLayerName: str = None, refLayerTranspose: bool = False, 
                incomingConfig: 'ModelNode.Layer.Incoming.Config' = _ModelNodeLayerIncoming.Concat(), 
                linearTransform: 'Train.Variable.LinearTransform' = Train.Variable.LinearTransform.createBasicConfig(weightAvg = 0, weightStdDev = 5e-2, weightL1Loss = False, weightL2Loss = True, weightL2Decay = 0, biasInitial = 0.001),
                activation: 'Train.Activation' = Train.Activation.Relu, activationParams: Dict = {},
                batchNorm: bool = False, batchNormParams: Dict = {}, 
                outputConfig: 'ModelNode.Layer.Output.Config' = _ModelNodeLayerOutput.Default(),
                buildNo = 0, weightDecayRate=0,weightStdDev=5e-2, biasInitial=0.001, weightL2Loss=True, weightAvg=0,weightL1Loss=False):
        '''
			Creates a convolutional layer.   --- UPDATED (Dexter) 20200117

            Parameters
            ------------------------------

            name            `str`   - The name of this layer.
            
            layerUnits      `int`   - The number of hidden units in this layer.
            
            convFilterWidth  `int`   - The CNN filter width.
            
            convPadding      `bool`  - Whether to use padding on this CNN layer, i.e. the output image size will be the same as the previous one.

            convStride       `tuple(int, int)`   - The stride of the CNN filter.   --- BETA
            
            convDilation     `int`   - The CNN dilation of the CNN filter.
            
            reshape         `tuple(int+)`   - The shape of reshaping previous layers. If not specified, it will automatically reshaped if the previous layer can't interpret as an image.
            
            refLayerName    `str`   - Any referenced kernal that this layer mirrors for.

            refLayerTranspose   `bool`  - Whether transpose is required for the referenced weight.
            
            incomingConfig  `ModelNode.Layer.Incoming.Config`    - Input configurations.

            linearTransform `Train.Variable.LinearTransform` - The linear transformation configuration.

            activation      `Train.Activation`   - The activation function of this layer.
            
            activationParams    `dict{str:*}`   - Activation parameters as defined in TensorFlow.
            
            batchNorm       `bool`  - Whether to use batch normalization before activation function.
            
            batchNormParams     `dict{str:*}`   - Batch normalization parameters as defined in TensorFlow.
            
            outputConfig    `ModelNode.Layer.Output.Config`      - Output configurations.

            buildNo         `int`   - The build number of staged training.   [Deprecated]
            
            weightDecayRate `float` - The constant for weighting L2-loss of weights   [Deprecated]
            
            weightStdDev    `float` - The standard deviation of the initialization of weights   [Deprecated]
            
            biasInitial     `float` - Constant initial value of biases   [Deprecated]
            
            weightL2Loss    `bool`  - Whether to take L2-loss on the weights   [Deprecated]
            
            weightAvg       `float` - The average value of the initialization of weights   [Deprecated]
            
            weightL1Loss    `bool`  - Whether to take L1-loss on the weights   [Deprecated]
        '''
        super().__init__(ModelNode.Layer.Types.Convolution, name = name, layerUnits = layerUnits, incomingConfig = incomingConfig, linearTransform = linearTransform,
                        activation = activation, activationParams = activationParams, 
                        batchNorm = batchNorm, batchNormParams = batchNormParams, outputConfig = outputConfig)
        
        self.subModelNodes = []
        self.sub_layer_dict = {}
        self.refLayerName = refLayerName
        self.refLayerTranspose = refLayerTranspose

    def keras_call_layer_build(self, reshape_output_name, output_reg, inputshape = None):
        '''
			Create keras layer code   --- UPDATED (kai Hsiang, Dexter) 20200120

            Parameters
            ------------------------------

            reshape_output_name     `str`                   - register name from auto reshape stage

            output_reg              `str`                   - register name
            
            inputshape              `list<tuple<int>>`      - list of input tensor shape.
                        
            Returns
            ------------------------------

            `list<str>, list<str>, <str>, list<tuple<int>>` - list of code, list of code, register name, Output shape
        '''
        if reshape_output_name != output_reg:
            A = output_reg
            B = reshape_output_name
        else:
            A = output_reg
            B = output_reg
        

        layer_list, call_list = [],[]
        if self.refLayerName is not None:
            layer = 'self.%s = self.%s'% (self.name,  self.refLayerName)
        else:
            layer = 'self.%s = %s()'% (self.name,  self.name)
        layer_list.append(layer)

        call = '%s = self.%s(%s)'% (A,self.name,B)
        call_list.append(call)
        new_shape = self.subModelNodes[-1].shape
        
        return layer_list, call_list, output_reg, new_shape
 
    def keras_call_auto_reshape_build(self, input_shape, combine_output_name, output_reg):
        '''
			Create keras reshape layer code   --- UPDATED (kai Hsiang) 20190909

            Parameters
            ------------------------------

            inputshape              `list<tuple<int>>`          - list of input tensor shape.
            
            combine_output_name     `str`                       - register name from auto reshape stage

            output_reg              `str`                       - register name
                        
            Returns
            ------------------------------

            `list<str>, list<str>, <str>, list<tuple<int>>` - list of code, list of code, register name, Output shape
        '''
        layer_obj = []
        call_obj = []
        if combine_output_name != output_reg:
            A = output_reg
            B = combine_output_name
        else:
            A = output_reg
            B = output_reg
        output_name = output_reg
        new_shape = self._shape[0]   # input_shape
        output_name = combine_output_name
        return layer_obj, call_obj, output_name, new_shape

    def keras_group(self, buildNo = 0, train = None):
        self.layer_obj = []
        self.call_obj = []
        if self.refLayerName is not None:
            return [],[]
        RM = RegisterManger()
        call_order = {}
        order_list = []
        use_time = {} # final layer=> -1
        for key, value in self.sub_layer_dict.items():
            toNode = value.toNode[buildNo]
            
            if len(toNode) == 0:
                use_time[key] = -1
            else:
                use_time[key] = len(toNode)
            try:
                call_order[value._order[buildNo]].append(key)
            except:
                call_order[value._order[buildNo]] = [key]
        
        for key, value in call_order.items():
            order_list.append(key)
        order_list.sort()
        print('group connect')
        # start connect layer
        for order in order_list:
            for layer_name in call_order[order]:
                value = self.sub_layer_dict[layer_name]
                #print(value.name)
                input_sources_shape, input_tensors_shape = [],[]
                input_sources_name, input_tensors_name = [],[]

                # get input name and shape
                for dpp_key in value.fromSource[buildNo]:
                    input_shape = [x if x != 'None' else None for x in train.dppNodes[dpp_key].getShape(False)]
                    input_sources_shape.append(input_shape)
                    if dpp_key in RM.name_to_reg:
                        reg = RM.get_register(dpp_key)
                    else:
                        '''reg = 'x'
                        RM.name_to_reg[dpp_key] = reg'''
                        reg = RM.get_register(dpp_key)
                        self.call_obj.append('%s = x'%(reg))
                    
                    
                    input_sources_name.append(reg)

                for t in value.fromNode[buildNo]:
                    # t = t.replace(' ','')
                    try:
                        t = self.sub_layer_dict[t]
                        input_shape = [x if x != 'None' else None for x in t._shape[buildNo]]
                        input_tensors_shape.append(input_shape)
                        reg = RM.get_register(t.name)
                        use_time[t.name] -= 1
                        if use_time[t.name] == 0:
                            RM.free_register(t.name)
                        input_tensors_name.append(reg)
                    except:
                        t = train.modelNodes[t]
                        input_shape = [x if x != 'None' else None for x in t._shape[buildNo]]
                        input_tensors_shape.append(input_shape)
                        reg = RM.get_register(t.name)
                        self.call_obj.append('%s = x'%(reg))
                        
                        input_tensors_name.append(reg)
                # get new register
                RM.get_register(layer_name)
                output_reg = RM.get_register(layer_name)
                # combine incoming tensors
                input_shape, combine_layer, combine_call, combine_output_name = value.keras_call_combineIncomingTensors \
                (input_sources_shape, input_tensors_shape, input_sources_name, input_tensors_name, output_reg)
                for string in combine_layer:
                    self.layer_obj.append(string)
                for string in combine_call:
                    self.call_obj.append(string)
                # auto reshape
                auto_layer, reshape_call, reshape_output_name, new_shape = value.keras_call_auto_reshape_build(input_shape, combine_output_name, output_reg)
                for string in auto_layer:
                    self.layer_obj.append(string)
                for string in reshape_call:
                    self.call_obj.append(string)
                # layer object
                layer, call_layer, layer_output_name, new_shape = value.keras_call_layer_build(reshape_output_name, output_reg, new_shape)
                for string in call_layer:
                    if string != '':
                        self.call_obj.append(string)
                for string in layer:
                    self.layer_obj.append(string)
                # output config
                layer, call_layer = value.keras_call_processOutputTensor(new_shape, layer_output_name, output_reg)
                for string in call_layer:
                    if string != '':
                        self.call_obj.append(string)
                for string in layer:
                    self.layer_obj.append(string)
                if len(value.toNode[buildNo]) == 1 and value.toNode[buildNo][0] not in self.sub_layer_dict.keys():
                    self.call_obj.append('return %s'%(output_reg))


        return self.layer_obj, self.call_obj 

class _ModelNodeLayerConvolution(_ModelNodeLayerConfig):
    '''
			Class representing a convolutional layer.   --- UPDATED (Dexter) 20181105
    '''
    def __init__(self, name: str = None, layerUnits: int = 30, convFilterWidth: int = 3, convPadding: bool = True, convStride: Tuple[int, int] = [1,1], convDilation: int = 1, reshape: List[int] =None, refLayerName: str = None, refLayerTranspose: bool = False, 
                incomingConfig: 'ModelNode.Layer.Incoming.Config' = _ModelNodeLayerIncoming.Concat(), 
                linearTransform: 'Train.Variable.LinearTransform' = Train.Variable.LinearTransform.createBasicConfig(weightAvg = 0, weightStdDev = 5e-2, weightL1Loss = False, weightL2Loss = True, weightL2Decay = 0, biasInitial = 0.001),
                activation: 'Train.Activation' = Train.Activation.Relu, activationParams: Dict = {},
                batchNorm: bool = True, batchNormParams: Dict = {}, 
                outputConfig: 'ModelNode.Layer.Output.Config' = _ModelNodeLayerOutput.Default(),
                buildNo = 0, weightDecayRate=0,weightStdDev=5e-2, biasInitial=0.001, weightL2Loss=True, weightAvg=0,weightL1Loss=False):
        '''
			Creates a convolutional layer.   --- UPDATED (Dexter) 20200117

            Parameters
            ------------------------------

            name            `str`   - The name of this layer.
            
            layerUnits      `int`   - The number of hidden units in this layer.
            
            convFilterWidth  `int`   - The CNN filter width.
            
            convPadding      `bool`  - Whether to use padding on this CNN layer, i.e. the output image size will be the same as the previous one.

            convStride       `tuple(int, int)`   - The stride of the CNN filter.   --- BETA
            
            convDilation     `int`   - The CNN dilation of the CNN filter.
            
            reshape         `tuple(int+)`   - The shape of reshaping previous layers. If not specified, it will automatically reshaped if the previous layer can't interpret as an image.
            
            refLayerName    `str`   - Any referenced kernal that this layer mirrors for.

            refLayerTranspose   `bool`  - Whether transpose is required for the referenced weight.
            
            incomingConfig  `ModelNode.Layer.Incoming.Config`    - Input configurations.

            linearTransform `Train.Variable.LinearTransform` - The linear transformation configuration.

            activation      `Train.Activation`   - The activation function of this layer.
            
            activationParams    `dict{str:*}`   - Activation parameters as defined in TensorFlow.
            
            batchNorm       `bool`  - Whether to use batch normalization before activation function.
            
            batchNormParams     `dict{str:*}`   - Batch normalization parameters as defined in TensorFlow.
            
            outputConfig    `ModelNode.Layer.Output.Config`      - Output configurations.

            buildNo         `int`   - The build number of staged training.   [Deprecated]
            
            weightDecayRate `float` - The constant for weighting L2-loss of weights   [Deprecated]
            
            weightStdDev    `float` - The standard deviation of the initialization of weights   [Deprecated]
            
            biasInitial     `float` - Constant initial value of biases   [Deprecated]
            
            weightL2Loss    `bool`  - Whether to take L2-loss on the weights   [Deprecated]
            
            weightAvg       `float` - The average value of the initialization of weights   [Deprecated]
            
            weightL1Loss    `bool`  - Whether to take L1-loss on the weights   [Deprecated]
        '''
        if (linearTransform is None and any([param is not None for param in [weightAvg, weightStdDev, weightL1Loss, weightL2Loss, weightDecayRate, biasInitial]])):
            linearTransform = Train.Variable.LinearTransform.createBasicConfig(weightAvg = weightAvg, weightStdDev = weightStdDev, 
                              weightL1Loss = weightL1Loss, weightL2Loss = weightL2Loss, weightL2Decay = weightDecayRate, biasInitial = biasInitial)
        super().__init__(ModelNode.Layer.Types.Convolution, name = name, layerUnits = layerUnits, incomingConfig = incomingConfig, linearTransform = linearTransform,
                        activation = activation, activationParams = activationParams, 
                        batchNorm = batchNorm, batchNormParams = batchNormParams, outputConfig = outputConfig)
        self.convFilterWidth = convFilterWidth
        self.convPadding = convPadding
        self.convStride = convStride
        self.convDilation = convDilation
        self.reshape = reshape
        self.refLayerName = refLayerName
        self.refLayerTranspose = refLayerTranspose

    def _copySymmetricLayerConfig(self, useSymmetricWeights = True, buildNo: int = 0) -> 'ModelNode.Layer.Deconvolution':
        '''
			Create a symmetric layer profile.   --- BETA --- UPDATED (Dexter) 20181127

            Parameters
            ------------------------------

            useSymmetricWeights `bool`              - Whether to use symmetric weights.

            buildNo             `int`   - The build number to be built.
        
            Returns
            ------------------------------

            `ModelNode.Layer.Deconvolution`    - A copied profile of this layer.
        '''
        if (len(self.fromNode[buildNo]) != 1):
            raise ValueError("Layers not having one and only one layer node inputs are not able to be copied in a symmetric way.")

        return ModelNode.Layer.Deconvolution(name=(self.name + "_Sym"), layerUnits = self.layerUnits, convFilterWidth=self.convFilterWidth, convPadding=self.convPadding, convStride=self.convStride,
                convDilation=self.convDilation, refLayerName = self.name if useSymmetricWeights else None, refLayerTranspose = True, 
                incomingConfig=self.incomingConfig, linearTransform=self.linearTransform, 
                activation=self.activation, activationParams=self.activationParams, 
                batchNorm=self.batchNorm, batchNormParams=self.batchNormParams, 
                outputConfig=self.outputConfig)

    def copy(self, name: str) -> 'ModelNode.Layer.Convolution':
        '''
			Copy this layer profile.   --- UPDATED (Dexter) 20190730

            Parameters
            ------------------------------

            name    `str`   - The new name of the copied layer
        
            Returns
            ------------------------------

            `ModelNode.Layer.Convolution`    - A copied profile of this layer.
        '''
        # Ensure this is not a subclass.
        if (not isinstance(self, ModelNode.Layer.Convolution)):
            raise ValueError("This layer class (" + self.__class__.__name__ + ") has not supported for copying.")
        
        # Create a new Layer Profile on this.
        return ModelNode.Layer.Convolution(name=name, layerUnits = self.layerUnits, convFilterWidth=self.convFilterWidth, convPadding=self.convPadding, convStride=self.convStride,
                convDilation=self.convDilation, reshape=self.reshape, refLayerName=self.refLayerName, refLayerTranspose=self.refLayerTranspose, 
                incomingConfig=self.incomingConfig, linearTransform=self.linearTransform, 
                activation=self.activation, activationParams=self.activationParams, 
                batchNorm=self.batchNorm, batchNormParams=self.batchNormParams, 
                outputConfig = self.outputConfig)

    def _build(self, buildNo: int):
        '''
			Build the TensorFlow Graph of this layer.   --- UPDATED (Dexter) 20200501

            Parameters
            ------------------------------

            buildNo     `int`   - The build number to be built.
        '''
        self._clearTempTensors()

        # 0. Ensure all incoming nodes have been built.
        if (all([n._built for n in self.fromNode[buildNo]])):
            # 1. Get refrenced layer kernel if required
            if (self.refLayerName is not None):
                refLayer = self.train.modelNodes[self.refLayerName]
                with tf.compat.v1.variable_scope(self.refLayerName, reuse=tf.compat.v1.AUTO_REUSE) as scope:
                    kernal = Train.Variable.getFromGraph("weight")

                if (self.refLayerTranspose):
                    with tf.compat.v1.variable_scope(self.name, reuse=tf.compat.v1.AUTO_REUSE) as scope:
                        kernal = tf.transpose(kernal, [0,1,3,2])

            # TensorFlow Graph building, using the layer name as the scope
            with tf.compat.v1.variable_scope(self.name, reuse=tf.compat.v1.AUTO_REUSE) as scope:
                # 2. Get previous tensors and make CNN on it
                fromTensor = self._combineIncomingTensors(buildNo = buildNo)
                fromShape = [*fromTensor.shape]

                # 3. Support convolutional layer for other shapes of data
                if (len(fromShape) != 4):
                    reshape = self.reshape
                    if (reshape is None):
                        # If more than 4 dimension, flatten to the 4th dim.
                        if (len(fromShape) > 4):
                            flattenSize = functools.reduce(lambda x,y: x*y, fromShape[3:], 1)
                            fromTensor = tf.reshape(fromTensor, [*Train.Variable.setAsReshape(fromTensor.shape[:3]), flattenSize])
                            fromShape = [*fromTensor.shape]

                        # If only 3 dimension, supplement a dimension.
                        elif (len(fromShape) == 3):
                            fromTensor = tf.reshape(fromTensor, [*Train.Variable.setAsReshape(fromTensor.shape), 1])
                            fromShape = [*fromTensor.shape]

                        # If only 2 dimension, auto reshape to a 2D picture with 1 channel.
                        elif (len(fromShape) == 2):
                            lastShape = fromTensor.shape[-1]
                            largestDim = 1
                            anotherDim = 1
                            for d in range(2, math.floor(lastShape**.5)+1):
                                if lastShape%d == 0:
                                    largestDim = d
                                    anotherDim = lastShape//d
                            if largestDim >1:
                                fromTensor = tf.reshape(fromTensor, [-1, largestDim, anotherDim, 1])
                                fromShape = [*fromTensor.shape]
                            else:
                                raise ValueError("Unable to reshape before Convolutional Layer can be applied.")

                        elif (len(fromShape) == 1):
                            raise ValueError("One dimension data is not supported for convolutional layers.")
                    else:
                        # Check whether the reshape is matched with fromShape.
                        originalFromShapeTotal = functools.reduce(lambda a,b: a*b, [s for s in fromShape[1:]], 1)
                        newReshape = functools.reduce(lambda a,b: a*b, reshape, 1)
                        if (originalFromShapeTotal != newReshape):
                            raise ValueError("Unable to reshape to specified shape.")
                        elif (len(reshape) != 3):
                            raise ValueError("The convolutional shape should be in 3 dimensions.")
                        
                        # Update the reshape.
                        fromTensor = tf.reshape(fromTensor, [-1, *reshape])
                        fromShape = [*fromTensor.shape]
                
                # 4. Apply kernal on the incoming tensors.
                transformConfig = self.linearTransform
                if (self.refLayerName is None):
                    kernal = transformConfig.weightConfig.create("weight", [self.convFilterWidth,self.convFilterWidth,fromShape[-1],self.layerUnits], dtype = fromTensor.dtype, defaultDevice = self.train.device)
                biases = transformConfig.biasConfig.create("biases", [self.layerUnits], dtype=fromTensor.dtype, defaultDevice = self.train.device)
                self._weights.extend([kernal, biases])

                with tf.device(self.train.device):
                    mid = tf.nn.conv2d(fromTensor, kernal, [1, *self.convStride, 1], padding=('SAME' if self.convPadding else 'VALID'), dilations = [1, self.convDilation, self.convDilation, 1])
                    mid = tf.nn.bias_add(mid, biases)

                # 5. General layer operations.
                mid = self._commonLayerOps(mid)

                # 6. Final output
                self._outputTensor = self._processOutputTensor(mid)

            self._built = True

            # Create chain build actions
            for n in self.toNode[buildNo]:
                n._build(buildNo)
    
    def keras_call_layer_build(self, reshape_output_name, output_reg, inputshape = None):
        '''
			Create keras layer code   --- UPDATED (kai Hsiang, Dexter) 20200501

            Parameters
            ------------------------------

            reshape_output_name     `str`                   - register name from auto reshape stage

            output_reg              `str`                   - register name
            
            inputshape              `list<tuple<int>>`      - list of input tensor shape.
                        
            Returns
            ------------------------------

            `list<str>, list<str>, <str>, list<tuple<int>>` - list of code, list of code, register name, Output shape
        '''
        def conv_shape(L,S,K,P):
	        return int(((L-K+2*P)/S + 1))
        if self.convPadding == 'SAME':
            p = (self.convFilterWidth-1)/2
        else:
            p = 0
        k = self.convFilterWidth + (self.convFilterWidth - 1)*(self.convDilation - 1)
        H = conv_shape(inputshape[1],self.convStride[0],k,p)
        W = conv_shape(inputshape[2],self.convStride[1],k,p)
        new_shape = [None, H, W, self.layerUnits]
        if self.convDilation < 1:
            self.convDilation = 1
        else:
            self.convDilation = int(self.convDilation)

        if reshape_output_name != output_reg:
            A = output_reg
            B = reshape_output_name
        else:
            A = output_reg
            B = output_reg
        padding = ('SAME' if self.convPadding else 'VALID')
        # activation
        act = self.act_dict[self.activation.value]
        w_init = self.get_initiallizer()
        b_init = self.get_initiallizer(key = 'biasConfig')
        w_reg = self.get_regularizer()
        b_reg = self.get_regularizer(key = 'biasConfig')
        layer_list, call_list = [],[]
        layer = 'self.%s = tf.keras.layers.Conv2D(%d,[%d,%d], strides=(%d,%d), dilation_rate=(%d,%d), padding=\'%s\', activation=%s, kernel_initializer=%s, bias_initializer=%s, kernel_regularizer=%s, bias_regularizer=%s)'% \
                (self.name, self.layerUnits, self.convFilterWidth, self.convFilterWidth, 
                self.convStride[0],self.convStride[1], self.convDilation,self.convDilation,padding, act, w_init, b_init, w_reg, b_reg)
        layer_list.append(layer)
        if self.refLayerName is not None:
            inputshape = [str(x) for x in inputshape]
            self.refLayerName = self.refLayerName.replace(' ','')
            layer_list.append('self.%s.build(('%(self.name) + ','.join(inputshape) + '))')
            layer_list.append('self.%s.kernel = None'%(self.name))

            if self.refLayerTranspose:
                call_list.append('self.%s.kernel = tf.transpose(self.%s.kernel, perm=[0,1,3,2])'%(self.name, self.refLayerName))
            else:
                call_list.append('self.%s.kernel = self.%s.kernel'%(self.name, self.refLayerName))
        call = '%s = self.%s(%s)'% (A,self.name,B)
        call_list.append(call)
        # dropout
        if self.dropout < 100:
            d_rate = (100 - self.dropout)/100
            layer_list.append('self.%s_dp = tf.keras.layers.Dropout(%g)'%(self.name, d_rate))
            call_list.append('%s = self.%s_dp(%s)'%(A,self.name,A))

        if self.batchNorm:
            layer_list.append('self.%s_bn = BatchNormalization()'%(self.name))
            call_list.append('%s = self.%s_bn(%s, training=training)'%(A,self.name,A))
            return layer_list, call_list, output_reg, new_shape
        else:
            return layer_list, call_list, output_reg, new_shape
 
    def keras_call_auto_reshape_build(self, input_shape, combine_output_name, output_reg):
        '''
			Create keras reshape layer code   --- UPDATED (kai Hsiang) 20190909

            Parameters
            ------------------------------

            inputshape              `list<tuple<int>>`          - list of input tensor shape.
            
            combine_output_name     `str`                       - register name from auto reshape stage

            output_reg              `str`                       - register name
                        
            Returns
            ------------------------------

            `list<str>, list<str>, <str>, list<tuple<int>>` - list of code, list of code, register name, Output shape
        '''
        layer_obj = []
        call_obj = []
        if combine_output_name != output_reg:
            A = output_reg
            B = combine_output_name
        else:
            A = output_reg
            B = output_reg
        output_name = output_reg
        if len(input_shape) != 4:
            if self.reshape == None:
                if len(input_shape) > 4:
                    axis = 3
                    new_shape = input_shape[0:axis]
                    new_shape.append(functools.reduce(lambda x,y: x*y, input_shape[axis:], 1))
                    string = 'self.%s_auto_reshape = tf.keras.layers.Reshape(('%(self.name)
                    string += ','.join([str(int(x)) for x in new_shape[1:]])
                    string += '))'
                    layer_obj.append(string)
                    string = '%s = self.%s_auto_reshape(%s)'%(A, self.name, B)
                    call_obj.append(string)
                elif len(input_shape) == 3:
                    new_shape = input_shape.copy()
                    new_shape.append(1)
                    string = 'self.%s_auto_reshape = tf.keras.layers.Reshape(('%(self.name)
                    string += ','.join([str(int(x)) for x in new_shape[1:]])
                    string += '))'
                    layer_obj.append(string)
                    string = '%s = self.%s_auto_reshape(%s)'%(A, self.name, B)
                    call_obj.append(string)
                elif len(input_shape) == 2:
                    lastShape = input_shape[-1]
                    largestDim = 1
                    anotherDim = 1
                    for d in range(2, math.floor(lastShape**.5)+1):
                        if lastShape%d == 0:
                            largestDim = d
                            anotherDim = lastShape//d
                    new_shape = [None, largestDim, anotherDim, 1]
                    string = 'self.%s_auto_reshape = tf.keras.layers.Reshape(('%(self.name)
                    string += ','.join([str(int(x)) for x in new_shape[1:]])
                    string += '))'
                    layer_obj.append(string)
                    string = '%s = self.%s_auto_reshape(%s)'%(A, self.name, B)
                    call_obj.append(string)
                else:
                    raise ValueError("One dimension data is not supported.")
            else:
                originalFromShapeTotal = functools.reduce(lambda x,y: x*y, input_shape[1:], 1)
                newReshape = functools.reduce(lambda a,b: a*b, self.reshape, 1)
                if (originalFromShapeTotal != newReshape):
                    raise ValueError("Unable to reshape to specified shape.")
                string = 'self.%s_auto_reshape = tf.keras.layers.Reshape(('%(self.name)
                string += ','.join([str(int(x)) for x in list(self.reshape)])
                string += '))'
                layer_obj.append(string)
                string = '%s = self.%s_auto_reshape(%s)'%(A, self.name, B)
                call_obj.append(string)
        else:
            new_shape = input_shape
            output_name = combine_output_name
        return layer_obj, call_obj, output_name, new_shape

class _ModelNodeLayerDeconvolution(_ModelNodeLayerConfig):
    '''
		Class representing a deconvolutional Layer.   --- UPDATED (Dexter) 20180701
    '''
    class PaddingTypes(Enumeration):
        '''
			Enumeration definining the deconvolutional padding methods.   --- UPDATED (Dexter) 20190722
        '''
        # `Number` - Pad with zeros.
        Zeros = 0
        # `Number` - Stretch and upscale the original image.
        Stretch = 1
    
    class UpscaleTypes(Enumeration):
        '''
			Enumeration definining the deconvolutional upscale methods due to striding or stretched padding.   --- UPDATED (Dexter) 20190724
        '''
        # `Number` - Enlarge the image using bicubic resize.
        Bicubic = 0
        # `Number` - Enlarge the image using bilinear resize.
        Bilinear = 1
        # `Number` - Enlarge the image using nearest neighbor resize.
        NearestNeighbor = 2
        # `Number` - Pad the image with zeros.
        PadZeros = 3
        # `Number` - Add zeros between original pixels.
        InsertZeros = 4
    
    class AlgorithmTypes(Enumeration):
        """
            Enumeration definining the deconvolutional layer algorithms.   --- UPDATED (Dexter) 20190722
        """
        # `Number` - Use TensorFlow native Convolution 2D Transpose.
        Conv2DTranspose = 0
        # `Number` - Upscale the image and follow by a convolutional 2D layer.   --- BETA
        ScaleThenConv2D = 1

    def __init__(self, name: str = None, layerUnits: int = 30, convFilterWidth: int = 3, convPadding: bool = True, convStride: Tuple[int,int] = [1,1], convDilation: int = 1, 
                dconvPadding: 'ModelNode.Layer.Deconvolution.PaddingTypes' = PaddingTypes.Stretch, dconvUpscale: 'ModelNode.Layer.Deconvolution.UpscaleTypes' = UpscaleTypes.Bicubic, 
                dconvAlgorithm: 'ModelNode.Layer.Deconvolution.AlgorithmTypes' = AlgorithmTypes.Conv2DTranspose, refLayerName: str = None, refLayerTranspose: bool = False, 
                incomingConfig: 'ModelNode.Layer.Incoming.Config' = _ModelNodeLayerIncoming.Concat(), 
                linearTransform: 'Train.Variable.LinearTransform' = Train.Variable.LinearTransform.createBasicConfig(weightAvg = 0, weightStdDev = 5e-2, weightL1Loss = False, weightL2Loss = True, weightL2Decay = 0, biasInitial = 0.001),
                activation: 'Train.Activation' = Train.Activation.Relu, activationParams: Dict = {},
                batchNorm: bool = True, batchNormParams: Dict = {}, 
                outputConfig: 'ModelNode.Layer.Output.Config' = _ModelNodeLayerOutput.Default(),
                reshape = None, buildNo = 0, weightDecayRate=0,weightStdDev=5e-2, biasInitial=0.001, weightL2Loss=True, weightAvg=0,weightL1Loss=False):
        '''
			Creates a deconvolutional layer.   --- UPDATED (Dexter) 20200117

            Parameters
            ------------------------------

            name            `str`   - The name of this layer.
            
            layerUnits      `int`   - The number of hidden units in this layer.
            
            convFilterWidth  `int`   - The CNN filter width.
            
            convPadding      `bool`  - Whether to use padding on this CNN layer, i.e. the output image size will be the same as the previous one.

            convStride       `tuple(int, int)`   - The stride of the CNN filter.   --- BETA
            
            convDilation     `int`   - The CNN dilation of the CNN filter.

            dconvPadding `ModelNode.Layer.Deconvolution.PaddingTypes` - A value defined in @ModelNode.Layer.Deconvolution.PaddingTypes .   --- BETA

            dconvUpscale `ModelNode.Layer.Deconvolution.UpscaleTypes` - The deconvolution upscale methods due to striding or stretched padding, as defined in @ModelNode.Layer.Deconvolution.UpscaleTypes .   --- BETA

            dconvAlgorithm `ModelNode.Layer.Deconvolution.AlgorithmTypes` - A value defined in @ModelNode.Layer.Deconvolution.AlgorithmTypes .

            refLayerName    `str`   - Any referenced kernal that this layer mirrors for.

            refLayerTranspose   `bool`  - Whether transpose is required for the referenced weight.
            
            incomingConfig  `ModelNode.Layer.Incoming.Config`    - Input configurations.

            linearTransform `Train.Variable.LinearTransform` - The linear transformation configuration.

            activation      `Train.Activation`   - The activation function of this layer.
            
            activationParams    `dict{str:*}`   - Activation parameters as defined in TensorFlow.
            
            batchNorm       `bool`  - Whether to use batch normalization before activation function.
            
            batchNormParams     `dict{str:*}`   - Batch normalization parameters as defined in TensorFlow.
            
            outputConfig    `ModelNode.Layer.Output.Config`      - Output configurations.

            reshape         `list[int+]`    - The shape of reshaping previous layers. If not specified, it will automatically reshaped if the previous layer can't interpret as an image. [Deprecated]

            buildNo         `int`   - The build number of staged training.   [Deprecated]
            
            weightDecayRate `float` - The constant for weighting L2-loss of weights.   [Deprecated]
            
            weightStdDev    `float` - The standard deviation of the initialization of weights.   [Deprecated]
            
            biasInitial     `float` - Constant initial value of biases.   [Deprecated]
            
            weightL2Loss    `bool`  - Whether to take L2-loss on the weights.   [Deprecated]
            
            weightAvg       `float` - The average value of the initialization of weights.   [Deprecated]
            
            weightL1Loss    `bool`  - Whether to take L1-loss on the weights.   [Deprecated]
        '''
        if (linearTransform is None and any([param is not None for param in [weightAvg, weightStdDev, weightL1Loss, weightL2Loss, weightDecayRate, biasInitial]])):
            linearTransform = Train.Variable.LinearTransform.createBasicConfig(weightAvg = weightAvg, weightStdDev = weightStdDev, 
                              weightL1Loss = weightL1Loss, weightL2Loss = weightL2Loss, weightL2Decay = weightDecayRate, biasInitial = biasInitial)
        super().__init__(ModelNode.Layer.Types.Deconvolution, name = name, layerUnits = layerUnits, incomingConfig = incomingConfig,
                        activation = activation, activationParams = activationParams, linearTransform = linearTransform,
                        batchNorm = batchNorm, batchNormParams = batchNormParams, outputConfig = outputConfig)
        # `int` - The deconvolution layer filter width.
        self.convFilterWidth = convFilterWidth
        # `bool` - Whether to use padding on this deconvolution layer, i.e. the output image size will be the same as the previous one.
        self.convPadding = convPadding
        # `tuple<int, int>` - The stride of the deconvolution layer filter.
        self.convStride = convStride
        # `int` - The convolution dilation of this deconvolution filter.
        self.convDilation = convDilation
        # `ModelNode.Layer.Deconvolution.PaddingTypes` - The deconvolution padding method, as defined in @ModelNode.Layer.Deconvolution.PaddingTypes .   --- BETA
        self.dconvPadding = dconvPadding
        # `ModelNode.Layer.Deconvolution.StrideScaleTypes` - The deconvolution upscale methods due to striding or stretched padding, as defined in @ModelNode.Layer.Deconvolution.StrideScaleTypes .   --- BETA
        self.dconvUpscale = dconvUpscale
        # `ModelNode.Layer.Deconvolution.AlgorithmTypes` - A value defined in @ModelNode.Layer.Deconvolution.AlgorithmTypes .
        self.dconvAlgorithm = dconvAlgorithm
        # `str` - Any referenced weight that this layer mirrors for.
        self.refLayerName = refLayerName
        # `bool` - Whether transpose is required for the referenced weight.
        self.refLayerTranspose = refLayerTranspose
        self.reshape = reshape

    def enumParser(self, k: str, v: Any) -> Any:
        """Parse the given key and value, to get the expected enumeration value.   --- UPDATED (Dexter) 20200501

        Parameters
        ------------------------------
        
        k `str` - Object property key.

        v `*` - Stored value in JSON object.

        Returns
        ------------------------------

        `*` - Enumeration value.
        """
        if k == "dconvPadding":
            if isinstance(v, str):
                return getattr(ModelNode.Layer.Deconvolution.PaddingTypes, v)
            else:
                return getattr(ModelNode.Layer.Deconvolution.PaddingTypes, ModelNode.Layer.Deconvolution.PaddingTypes.getName(v))
        elif k == "dconvUpscale":
            if isinstance(v, str):
                return getattr(ModelNode.Layer.Deconvolution.UpscaleTypes, v)
            else:
                return getattr(ModelNode.Layer.Deconvolution.UpscaleTypes, ModelNode.Layer.Deconvolution.UpscaleTypes.getName(v))
        elif k == "dconvAlgorithm":
            if isinstance(v, str):
                return getattr(ModelNode.Layer.Deconvolution.AlgorithmTypes, v)
            else:
                return getattr(ModelNode.Layer.Deconvolution.AlgorithmTypes, ModelNode.Layer.Deconvolution.AlgorithmTypes.getName(v))
        else:
            return v

    def _copySymmetricLayerConfig(self, useSymmetricWeights = True, buildNo: int = 0) -> 'ModelNode.Layer.Convolution':
        '''
			Create a symmetric layer profile.   --- BETA --- UPDATED (Dexter) 20190725

            Parameters
            ------------------------------

            useSymmetricWeights `bool` - Whether to use symmetric weights.

            buildNo             `int`   - The build number to be built.
        
            Returns
            ------------------------------

            `ModelNode.Layer.Convolution`    - A copied profile of this layer.
        '''
        if (len(self.fromNode[buildNo]) != 1):
            raise ValueError("Layers not having one and only one layer node inputs are not able to be copied in a symmetric way.")

        return ModelNode.Layer.Convolution(name=(self.name + "_Sym"), layerUnits = self.layerUnits, convFilterWidth=self.convFilterWidth, convPadding=self.convPadding, convStride=self.convStride, convDilation=self.convDilation, 
                refLayerName = self.name if useSymmetricWeights else None, refLayerTranspose=True, 
                incomingConfig=self.incomingConfig, linearTransform=self.linearTransform, 
                activation=self.activation, activationParams=self.activationParams, 
                batchNorm=self.batchNorm, batchNormParams=self.batchNormParams, 
                outputConfig=self.outputConfig)

    def copy(self, name: str) -> 'ModelNode.Layer.Deconvolution':
        '''
			Copy this layer profile.   --- UPDATED (Dexter) 20190725

            Parameters
            ------------------------------

            name    `str`   - The new name of the copied layer

            Returns
            ------------------------------

            `ModelNode.Layer.Deconvolution`    - A copied profile of this layer.
        '''
        # Ensure this is not a subclass.
        if (not isinstance(self, ModelNode.Layer.Deconvolution)):
            raise ValueError("This layer class (" + self.__class__.__name__ + ") has not supported for copying.")
        
        # Create a new Layer Profile on this.
        return ModelNode.Layer.Deconvolution(name=name, layerUnits = self.layerUnits, convFilterWidth=self.convFilterWidth, convPadding=self.convPadding, convStride=self.convStride,
                convDilation=self.convDilation, dconvPadding=self.dconvPadding, dconvUpscale = self.dconvUpscale, dconvAlgorithm = self.dconvAlgorithm, refLayerName = self.refLayerName, refLayerTranspose=self.refLayerTranspose, 
                incomingConfig=self.incomingConfig, linearTransform=self.linearTransform, 
                activation=self.activation, activationParams=self.activationParams, 
                batchNorm=self.batchNorm, batchNormParams=self.batchNormParams, 
                outputConfig = self.outputConfig)

    def _build(self, buildNo: int):
        '''
			Build the TensorFlow Graph of this layer.   --- UPDATED (Dexter) 20200501

            Parameters
            ------------------------------

            buildNo     `int`   - The build number to be built.
        '''
        self._clearTempTensors()

        # 0. Ensure all incoming nodes have been built.
        if (all([n._built for n in self.fromNode[buildNo]])):
            # 1. Get refrenced layer kernel if required
            if (self.refLayerName is not None):
                refLayer = self.train.modelNodes[self.refLayerName]
                with tf.compat.v1.variable_scope(self.refLayerName, reuse=tf.compat.v1.AUTO_REUSE) as scope:
                    kernal = Train.Variable.getFromGraph("weight")
                
                if (self.refLayerTranspose):
                    with tf.compat.v1.variable_scope(self.name, reuse=tf.compat.v1.AUTO_REUSE) as scope:
                        kernal = tf.transpose(kernal, [0,1,3,2])

            # TensorFlow Graph building, using the layer name as the scope
            with tf.compat.v1.variable_scope(self.name, reuse=tf.compat.v1.AUTO_REUSE) as scope:
                # 2. Get previous tensors
                fromTensor = self._combineIncomingTensors(buildNo = buildNo)
                fromShape = [*fromTensor.shape]

                # 3. Support CNN for other shapes of data
                if (len(fromShape) != 4):
                    # If more than 4 dimension, flatten to the 4th dim.
                    if (len(fromShape) > 4):
                        flattenSize = functools.reduce(lambda x,y: x*y, fromShape[3:], 1)
                        fromTensor = tf.reshape(fromTensor, [*Train.Variable.setAsReshape(fromTensor.shape[:3]), flattenSize])
                        fromShape = [*fromTensor.shape]

                    # If only 3 dimension, supplement a dimension.
                    elif (len(fromShape) == 3):
                        fromTensor = tf.reshape(fromTensor, [*Train.Variable.setAsReshape(fromTensor.shape), 1])
                        fromShape = [*fromTensor.shape]
                    
                    # 2 or lower dimension is not supported.
                    else:
                        raise ValueError("2 or lower dimension is not supported for DCNN Layer.")

                # 4. If no linked kernal, build a new kernal
                transformConfig = self.linearTransform
                if (self.refLayerName is None):
                    kernal = transformConfig.weightConfig.create("weight", [self.convFilterWidth,self.convFilterWidth,fromShape[-1],self.layerUnits], dtype=fromTensor.dtype, defaultDevice = self.train.device)

                # 5. Set biases
                biases = transformConfig.biasConfig.create("biases", [kernal.shape[-1]], dtype=fromTensor.dtype, defaultDevice = self.train.device)
                self._weights.extend([kernal, biases])

                # 6. Calculate the output shape.
                outputH = 0
                outputW = 0
                if self.convPadding:
                    outputH = fromShape[1] * self.convStride[0]
                    outputW = fromShape[2] * self.convStride[1]
                else:
                    # Ref: https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/python/keras/utils/conv_utils.py (def deconv_output_length)
                    outputMorePxH = max([self.convFilterWidth+(self.convDilation-1)*(self.convFilterWidth-1) - self.convStride[0], 0])
                    outputMorePxW = max([self.convFilterWidth+(self.convDilation-1)*(self.convFilterWidth-1) - self.convStride[1], 0])
                    outputH = fromShape[1] * self.convStride[0] + outputMorePxH
                    outputW = fromShape[2] * self.convStride[1] + outputMorePxW

                # 7. Apply deconvolution.
                if self.dconvAlgorithm == ModelNode.Layer.Deconvolution.AlgorithmTypes.Conv2DTranspose:
                    # 7-1. Utilize Convolutional 2-D Transpose deconvolution.
                    if (self.train.device.startswith("/cpu") and self.convDilation > 1):
                        raise ValueError("Deconvolutional dilation is not supported for CPU processing. --- at Layer: " + self.name)
                    elif (self.convDilation > 1 and any([s>1 for s in self.convStride])):
                        raise ValueError("Deconvolutional layer using convolutional 2D transpose currently does not support dilation > 1 when stride > 1.   --- at Layer: " + self.name)

                    with tf.device(self.train.device): 
                        mid = tf.nn.conv2d_transpose(fromTensor, filter=tf.transpose(kernal, [0,1,3,2]), output_shape=[tf.shape(fromTensor)[0], outputH, outputW, kernal.shape[-1]], strides=[1,*self.convStride,1], padding=('SAME' if self.convPadding else 'VALID'), dilations = [1, self.convDilation, self.convDilation, 1])
                        mid = tf.nn.bias_add(mid, biases)
                else:
                    # 7-2. Implement Ladder original deconvolution (scale followed by convolutional layer).
                    # Calculate the actual filter length and pad length.
                    actualFilterWidth = self.convFilterWidth + (self.convDilation - 1) * (self.convFilterWidth - 1)
                    padLength = (actualFilterWidth - 1) * 2
                    
                    # Stretch the image so that the image is enlarged to every pixels that will contribute for the convolutional layer.
                    if (self.dconvPadding == ModelNode.Layer.Deconvolution.PaddingTypes.Stretch):
                        # Rasie errors for incompatible combinations.
                        if (self.dconvUpscale in [ModelNode.Layer.Deconvolution.UpscaleTypes.PadZeros, ModelNode.Layer.Deconvolution.UpscaleTypes.InsertZeros]):
                            raise ValueError("Deconvolutional upscale cannot be PadZeros or InsertZeros when it is stretching but not padding.")
                        
                        # Rescale the image.
                        resizeMethod = ModelNode.Layer.Deconvolution.getTFResizeMethod(self.dconvUpscale)
                        tfResizeMethod = tf.image.ResizeMethod.BICUBIC if resizeMethod == "BICUBIC" else tf.image.ResizeMethod.BILINEAR if resizeMethod == "BILINEAR" else tf.image.ResizeMethod.NEAREST_NEIGHBOR
                        fromTensor = tf.image.resize(fromTensor, (outputH + padLength, outputW + padLength), tfResizeMethod)
                    
                    # The image is enlarged to the output size and followed with a zero padding at margin for convolutional layer.
                    else:
                        # Pad zeros directly.
                        if (self.dconvUpscale == ModelNode.Layer.Deconvolution.UpscaleTypes.PadZeros):
                            fromTensor = tf.image.pad_to_bounding_box(fromTensor, int((outputH + padLength - fromShape[1]) / 2), int((outputW + padLength - fromShape[2]) / 2), outputH + padLength, outputW + padLength)
                        
                        # Pad zeros after some upscaling.
                        else:
                            if (self.dconvUpscale == ModelNode.Layer.Deconvolution.UpscaleTypes.InsertZeros):
                                # Insert Zero: (Stride == 2), resulting image is of shape stride * length .
                                #               ┌ 1 0 2 0 3 0 ┐
                                # ┌ 1 2 3 ┐     │ 0 0 0 0 0 0 │
                                # │ 2 3 4 │  >  │ 2 0 3 0 4 0 │
                                # └ 4 5 6 ┘     │ 0 0 0 0 0 0 │
                                #               │ 5 0 6 0 7 0 │
                                #               └ 0 0 0 0 0 0 ┘
                                if (self.convStride[1] > 1):
                                    moreH = [np.zeros(fromTensor.shape for x in range(0, self.convStride[1] - 1))]
                                    fromTensor = tf.reshape(tf.concat([fromTensor, *moreH], axis = -1), [fromTensor.shape[0], fromTensor.shape[1], fromTensor.shape[2] * self.convStride[1], fromTensor.shape[3]])
                                if (self.convStride[0] > 1):
                                    moreW = [np.zeros(fromTensor.shape for x in range(0, self.convStride[0] - 1))]
                                    fromTensor = tf.reshape(tf.concat([fromTensor, *moreH], axis = -2), [fromTensor.shape[0], fromTensor.shape[1] * self.convStride[0], fromTensor.shape[2], fromTensor.shape[3]])
                            
                            elif (self.convStride[1] > 1) or (self.convStride[0] > 1):
                                # Rescale the image to stride * length .
                                resizeMethod = ModelNode.Layer.Deconvolution.getTFResizeMethod(self.dconvUpscale)
                                tfResizeMethod = tf.image.ResizeMethod.BICUBIC if resizeMethod == "BICUBIC" else tf.image.ResizeMethod.BILINEAR if resizeMethod == "BILINEAR" else tf.image.ResizeMethod.NEAREST_NEIGHBOR
                                fromTensor = tf.image.resize(fromTensor, (fromTensor.shape[1] * self.convStride[0], fromTensor.shape[2] * self.convStride[1]), tfResizeMethod)
                            
                            # Pad the remaining pixels with zeros.
                            fromTensor = tf.image.pad_to_bounding_box(fromTensor, int((outputH + padLength - fromShape[1]) / 2), int((outputW + padLength - fromShape[2]) / 2), outputH + padLength, outputW + padLength)

                    with tf.device(self.train.device): 
                        # 7. Apply deconvolutional layer
                        mid = tf.nn.conv2d(fromTensor, kernal, [1,1,1,1], padding='SAME', dilations = [1, self.convDilation, self.convDilation, 1])
                        mid = tf.nn.bias_add(mid, biases)

                # 8. General layer operations.
                mid = self._commonLayerOps(mid)

                # 9. Final output
                self._outputTensor = self._processOutputTensor(mid)

            self._built = True
            # Create chain build actions
            for n in self.toNode[buildNo]:
                n._build(buildNo)
    
    @staticmethod
    def getTFResizeMethod(upscaleType: 'ModelNode.Layer.Deconvolution.UpscaleTypes') -> str:
        '''
			Get the TensorFlow resize method name.   --- UPDATED (Dexter) 20190724

            Parameters
            ------------------------------

            upscaleType  `ModelNode.Layer.Deconvolution.UpscaleTypes`   - A value defined in @ModelNode.Layer.Deconvolution.UpscaleTypes .

            Returns
            ------------------------------

            `str` - The TensorFlow resize method name.
        '''
        methodDict = {0: "BICUBIC", 1: "BINEAR", 2: "NEAREST_NEIGHBOR", 3: "AREA"}
        if upscaleType.value not in methodDict:
            raise ValueError("Resize method not supported.")
        return methodDict[upscaleType.value]

    def keras_call_layer_build(self, reshape_output_name, output_reg, inputshape = None):
        '''
			Create keras layer code   --- UPDATED (kai Hsiang, Dexter) 20200120

            Parameters
            ------------------------------

            reshape_output_name     `str`                   - register name from auto reshape stage

            output_reg              `str`                   - register name
            
            inputshape              `list<tuple<int>>`      - list of input tensor shape.
                        
            Returns
            ------------------------------

            `list<str>, list<str>, <str>, list<tuple<int>>` - list of code, list of code, register name, Output shape
        '''
        if self.convPadding:
            H = inputshape[1] * self.convStride[0]
            W = inputshape[2] * self.convStride[1]
        else:
            # Ref: https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/python/keras/utils/conv_utils.py (def deconv_output_length)
            outputMorePxH = max([self.convFilterWidth+(self.convDilation-1)*(self.convFilterWidth-1) - self.convStride[0], 0])
            outputMorePxW = max([self.convFilterWidth+(self.convDilation-1)*(self.convFilterWidth-1) - self.convStride[1], 0])
            H = inputshape[1] * self.convStride[0] + outputMorePxH
            W = inputshape[2] * self.convStride[1] + outputMorePxW
        new_shape = [None, H, W, self.layerUnits]

        if reshape_output_name != output_reg:
            A = output_reg
            B = reshape_output_name
        else:
            A = output_reg
            B = output_reg
        padding = ('SAME' if self.convPadding else 'VALID')
        # activation
        '''if self.activation == Train.Activation.Relu6 or self.activation == Train.Activation.Crelu:
            act = 'relu'
        elif self.activation == Train.Activation.HardSigmoid:
            act = 'hard_sigmoid'
        else:
            act = (Train.Activation.getName(self.activation)).split('.')[-1].lower()
        '''
        act = self.act_dict[self.activation.value]
        w_init = self.get_initiallizer()
        b_init = self.get_initiallizer(key = 'biasConfig')
        w_reg = self.get_regularizer()
        b_reg = self.get_regularizer(key = 'biasConfig')
        layer_list, call_list = [],[]
        layer = 'self.%s = tf.keras.layers.Conv2DTranspose(%d,[%d,%d], strides=(%d,%d), dilation_rate=(%d,%d), padding=\'%s\', activation=%s, kernel_initializer=%s, bias_initializer=%s, kernel_regularizer=%s, bias_regularizer=%s)'% \
                (self.name, self.layerUnits, self.convFilterWidth, self.convFilterWidth, 
                self.convStride[0],self.convStride[1], self.convDilation,self.convDilation, padding, act, w_init, b_init, w_reg, b_reg)
        layer_list.append(layer)
        if self.refLayerName is not None:
            self.refLayerName = self.refLayerName.replace(' ','')
            inputshape = [str(x) for x in inputshape]
            layer_list.append('self.%s.build(('%(self.name) + ','.join(inputshape) + '))')
            layer_list.append('self.%s.kernel = None'%(self.name))

            if self.refLayerTranspose:
                call_list.append('self.%s.kernel = self.%s.kernel'%(self.name, self.refLayerName))
            else:
                call_list.append('self.%s.kernel = self.%s.kernel'%(self.name, self.refLayerName))
        call = '%s = self.%s(%s)'% (A,self.name,B)
        call_list.append(call)
        # dropout
        if self.dropout < 100:
            d_rate = (100 - self.dropout)/100
            layer_list.append('self.%s_dp = tf.keras.layers.Dropout(%g)'%(self.name, d_rate))
            call_list.append('%s = self.%s_dp(%s)'%(A,self.name,A))
        if self.batchNorm:
            layer_list.append('self.%s_bn = BatchNormalization()'%(self.name))
            call_list.append('%s = self.%s_bn(%s, training=training)'%(A,self.name,A))
            #return [layer, 'self.%s_bn = BatchNormalization()'%(self.name)], [call, '%s = self.%s_bn(%s, training=training)'%(A,self.name,A)]
            return layer_list, call_list, output_reg, new_shape
        else:
            return layer_list, call_list, output_reg, new_shape

    def keras_call_auto_reshape_build(self, input_shape, combine_output_name, output_reg):
        '''
			Create keras reshape layer code   --- UPDATED (kai Hsiang) 20190909

            Parameters
            ------------------------------

            inputshape              `list<tuple<int>>`          - list of input tensor shape.
            
            combine_output_name     `str`                       - register name from auto reshape stage

            output_reg              `str`                       - register name
                        
            Returns
            ------------------------------

            `list<str>, list<str>, <str>, list<tuple<int>>` - list of code, list of code, register name, Output shape
        '''
        layer_obj = []
        call_obj = []
        if combine_output_name != output_reg:
            A = output_reg
            B = combine_output_name
        else:
            A = output_reg
            B = output_reg
        output_name = output_reg
        if len(input_shape) != 4:
            if self.reshape == None:
                if len(input_shape) > 4:
                    axis = 3
                    new_shape = input_shape[0:axis]
                    new_shape.append(functools.reduce(lambda x,y: x*y, input_shape[axis:], 1))
                    string = 'self.%s_auto_reshape = tf.keras.layers.Reshape(('%(self.name)
                    string += ','.join([str(int(x)) for x in new_shape[1:]])
                    string += '))'
                    layer_obj.append(string)
                    string = '%s = self.%s_auto_reshape(%s)'%(A, self.name, B)
                    call_obj.append(string)
                elif len(input_shape) == 3:
                    new_shape = input_shape.copy()
                    new_shape.append(1)
                    string = 'self.%s_auto_reshape = tf.keras.layers.Reshape(('%(self.name)
                    string += ','.join([str(int(x)) for x in new_shape[1:]])
                    string += '))'
                    layer_obj.append(string)
                    string = '%s = self.%s_auto_reshape(%s)'%(A, self.name, B)
                    call_obj.append(string)
                elif len(input_shape) == 2:
                    lastShape = input_shape[-1]
                    largestDim = 1
                    anotherDim = 1
                    for d in range(2, math.floor(lastShape**.5)+1):
                        if lastShape%d == 0:
                            largestDim = d
                            anotherDim = lastShape//d
                    new_shape = [None, largestDim, anotherDim, 1]
                    string = 'self.%s_auto_reshape = tf.keras.layers.Reshape(('%(self.name)
                    string += ','.join([str(int(x)) for x in new_shape[1:]])
                    string += '))'
                    layer_obj.append(string)
                    string = '%s = self.%s_auto_reshape(%s)'%(A, self.name, B)
                    call_obj.append(string)
                else:
                    raise ValueError("One dimension data is not supported.")
            else:
                originalFromShapeTotal = functools.reduce(lambda x,y: x*y, input_shape[1:], 1)
                newReshape = functools.reduce(lambda a,b: a*b, self.reshape, 1)
                new_shape = [input_shape[0]]
                new_shape.extend(self.reshape)
                if (originalFromShapeTotal != newReshape):
                    raise ValueError("Unable to reshape to specified shape.")
                string = 'self.%s_auto_reshape = tf.keras.layers.Reshape(('%(self.name)
                string += ','.join([str(int(x)) for x in list(self.reshape)])
                string += '))'
                layer_obj.append(string)
                string = '%s = self.%s_auto_reshape(%s)'%(A, self.name, B)
                call_obj.append(string)
        else:
            new_shape = input_shape
            output_name = combine_output_name
        return layer_obj, call_obj, output_name, new_shape

class _ModelNodeLayerConvolution1D(_ModelNodeLayerConfig):
    '''
			Class representing a qD convolutional layer.   --- UPDATED (Dexter) 20190723
    '''
    def __init__(self, name: str = None, layerUnits: int = 30, convFilterWidth: int = 3, convPadding: bool = True, convStride: int = 1, convDilation: int = 1, reshape: List[int] =None, refLayerName: str = None, refLayerTranspose: bool = False, 
                incomingConfig: 'ModelNode.Layer.Incoming.Config' = _ModelNodeLayerIncoming.Concat(), 
                linearTransform: 'Train.Variable.LinearTransform' = Train.Variable.LinearTransform.createBasicConfig(weightAvg = 0, weightStdDev = 5e-2, weightL1Loss = False, weightL2Loss = True, weightL2Decay = 0, biasInitial = 0.001),
                activation: str = "relu", activationParams: Dict = {},
                batchNorm: bool = True, batchNormParams: Dict = {}, 
                outputConfig: 'ModelNode.Layer.Output.Config' = _ModelNodeLayerOutput.Default(),
                buildNo = 0, weightDecayRate=0,weightStdDev=5e-2, biasInitial=0.001, weightL2Loss=True, weightAvg=0,weightL1Loss=False):
        '''
			Creates a convolutional layer.   --- UPDATED (Dexter) 20200117

            Parameters
            ------------------------------

            name            `str`   - The name of this layer.
            
            layerUnits      `int`   - The number of hidden units in this layer.
            
            convFilterWidth  `int`   - The CNN filter width.
            
            convPadding      `bool`  - Whether to use padding on this CNN layer, i.e. the output image size will be the same as the previous one.

            convStride       `tuple(int, int)`   - The stride of the CNN filter.   --- BETA
            
            convDilation     `int`   - The CNN dilation of the CNN filter.
            
            reshape         `tuple(int+)`   - The shape of reshaping previous layers. If not specified, it will automatically reshaped if the previous layer can't interpret as an image.
            
            refLayerName    `str`   - Any referenced kernal that this layer mirrors for.

            refLayerTranspose   `bool`  - Whether transpose is required for the referenced weight.
            
            incomingConfig  `ModelNode.Layer.Incoming.Config`    - Input configurations.

            linearTransform `Train.Variable.LinearTransform` - The linear transformation configuration.

            activation      `str`   - The activation function of this layer.
            
            activationParams    `dict{str:*}`   - Activation parameters as defined in TensorFlow.
            
            batchNorm       `bool`  - Whether to use batch normalization before activation function.
            
            batchNormParams     `dict{str:*}`   - Batch normalization parameters as defined in TensorFlow.
            
            outputConfig    `ModelNode.Layer.Output.Config`      - Output configurations.
        '''
        super().__init__(ModelNode.Layer.Types.Convolution1D, name = name, layerUnits = layerUnits, incomingConfig = incomingConfig, linearTransform = linearTransform,
                        activation = activation, activationParams = activationParams, 
                        batchNorm = batchNorm, batchNormParams = batchNormParams, outputConfig = outputConfig)
        # `int` - The convolutional layer filter width.
        self.convFilterWidth = convFilterWidth
        # `bool` - Whether to use padding on this convolutional layer, i.e. the output image size will be the same as the previous one.
        self.convPadding = convPadding
        # `int` - The stride of the convolutional layer filter.
        self.convStride = convStride
        # `int` - The convolution dilation of the convolutional layer filter.
        self.convDilation = convDilation
        # `list<int>` - The shape of reshaping previous layers. If `null`, it will automatically reshaped if the previous layer can't interpret as an image.
        self.reshape = reshape
        # `str` - Any referenced weight that this layer mirrors for.
        self.refLayerName = refLayerName
        # `bool` - Whether transpose is required for the referenced weight.
        self.refLayerTranspose = refLayerTranspose

    def _copySymmetricLayerConfig(self, useSymmetricWeights = True, buildNo: int = 0):
        '''
			Create a symmetric layer profile.   --- BETA --- UPDATED (Dexter) 20190822

            Parameters
            ------------------------------

            useSymmetricWeights `bool`              - Whether to use symmetric weights.

            buildNo             `int`   - The build number to be built.
        
            Returns
            ------------------------------

            `ModelNode.Layer.Deconvolution1D`    - A copied profile of this layer.
        '''
        if (len(self.fromNode[buildNo]) != 1):
            raise ValueError("Layers not having one and only one layer node inputs are not able to be copied in a symmetric way.")

        return ModelNode.Layer.Deconvolution1D(name=(self.name + "_Sym"), layerUnits = self.layerUnits, convFilterWidth=self.convFilterWidth, convPadding=self.convPadding, convStride=self.convStride,
                convDilation=self.convDilation, refLayerName = self.name if useSymmetricWeights else None, refLayerTranspose = True, 
                incomingConfig=self.incomingConfig, linearTransform=self.linearTransform, 
                activation=self.activation, activationParams=self.activationParams, 
                batchNorm=self.batchNorm, batchNormParams=self.batchNormParams, 
                outputConfig=self.outputConfig)

    def copy(self, name: str):
        '''
			Copy this layer profile.   --- UPDATED (Dexter) 20190822

            Parameters
            ------------------------------

            name    `str`   - The new name of the copied layer
        '''
        # Ensure this is not a subclass.
        if (not isinstance(self, ModelNode.Layer.Convolution1D)):
            raise ValueError("This layer class (" + self.__class__.__name__ + ") has not supported for copying.")
        
        # Create a new Layer Profile on this.
        return ModelNode.Layer.Convolution1D(name=name, layerUnits = self.layerUnits, convFilterWidth=self.convFilterWidth, convPadding=self.convPadding, convStride=self.convStride,
                convDilation=self.convDilation, reshape=self.reshape, refLayerName=self.refLayerName, refLayerTranspose=self.refLayerTranspose, 
                incomingConfig=self.incomingConfig, linearTransform=self.linearTransform, 
                activation=self.activation, activationParams=self.activationParams, 
                batchNorm=self.batchNorm, batchNormParams=self.batchNormParams, 
                outputConfig = self.outputConfig)

    def _build(self, buildNo: int):
        '''
			Build the TensorFlow Graph of this layer.   --- UPDATED (Dexter) 20200501

            Parameters
            ------------------------------

            buildNo     `int`   - The build number to be built.
        '''
        self._clearTempTensors()

        # 0. Ensure all incoming nodes have been built.
        if (all([n._built for n in self.fromNode[buildNo]])):
            # 1. Get refrenced layer kernel if required
            if (self.refLayerName is not None):
                refLayer = self.train.modelNodes[self.refLayerName]
                with tf.compat.v1.variable_scope(self.refLayerName, reuse=tf.compat.v1.AUTO_REUSE) as scope:
                    kernal = Train.Variable.getFromGraph("weight")

                if (self.refLayerTranspose):
                    with tf.compat.v1.variable_scope(self.name, reuse=tf.compat.v1.AUTO_REUSE) as scope:
                        kernal = tf.transpose(kernal, [0,2,1])

            # TensorFlow Graph building, using the layer name as the scope
            with tf.compat.v1.variable_scope(self.name, reuse=tf.compat.v1.AUTO_REUSE) as scope:
                # 2. Get previous tensors and make CNN on it
                fromTensor = self._combineIncomingTensors(buildNo = buildNo)
                fromShape = [*fromTensor.shape]

                # 3. Support convolutional layer for other shapes of data
                if (len(fromShape) != 3):
                    reshape = self.reshape
                    if (reshape is None):
                        # If more than 3dimension, flatten to the 3rd dim.
                        if (len(fromShape) > 3):
                            flattenSize = functools.reduce(lambda x,y: x*y, fromShape[2:], 1)
                            fromTensor = tf.reshape(fromTensor, [*Train.Variable.setAsReshape(fromTensor.shape[:2]), flattenSize])
                            fromShape = [*fromTensor.shape]

                        # If only 2 dimension, supplement a dimension.
                        elif (len(fromShape) == 2):
                            fromTensor = tf.reshape(fromTensor, [*Train.Variable.setAsReshape(fromTensor.shape), 1])
                            fromShape = [*fromTensor.shape]

                        elif (len(fromShape) == 1):
                            raise ValueError("One dimension data is not supported for convolutional layers.")
                    else:
                        # Check whether the reshape is matched with fromShape.
                        originalFromShapeTotal = functools.reduce(lambda a,b: a*b, [s for s in fromShape[1:]], 1)
                        newReshape = functools.reduce(lambda a,b: a*b, reshape, 1)
                        if (originalFromShapeTotal != newReshape):
                            raise ValueError("Unable to reshape to specified shape.")
                        elif (len(reshape) != 2):
                            raise ValueError("The convolutional shape should be in 2 dimensions.")
                        
                        # Update the reshape.
                        fromTensor = tf.reshape(fromTensor, [-1, *reshape])
                        fromShape = [*fromTensor.shape]
                
                # 4. Apply kernal on the incoming tensors.
                transformConfig = self.linearTransform
                if (self.refLayerName is None):
                    kernal = transformConfig.weightConfig.create("weight", [self.convFilterWidth,fromShape[-1],self.layerUnits], dtype = fromTensor.dtype, defaultDevice = self.train.device)
                biases = transformConfig.biasConfig.create("biases", [self.layerUnits], dtype=fromTensor.dtype, defaultDevice = self.train.device)
                self._weights.extend([kernal, biases])

                with tf.device(self.train.device):
                    mid = tf.nn.conv1d(fromTensor, kernal, [1, self.convStride, 1], padding=('SAME' if self.convPadding else 'VALID'), dilations = [1, self.convDilation, 1])
                    mid = tf.nn.bias_add(mid, biases)

                # 5. General layer operations.
                mid = self._commonLayerOps(mid)

                # 6. Final output
                self._outputTensor = self._processOutputTensor(mid)

            self._built = True

            # Create chain build actions
            for n in self.toNode[buildNo]:
                n._build(buildNo)

    def keras_call_layer_build(self, reshape_output_name, output_reg, inputshape = None):
        '''
			Create keras layer code   --- UPDATED (Dexter) 20200311

            Parameters
            ------------------------------

            reshape_output_name     `str`                   - register name from auto reshape stage

            output_reg              `str`                   - register name
            
            inputshape              `list<tuple<int>>`      - list of input tensor shape.
                        
            Returns
            ------------------------------

            `list<str>, list<str>, <str>, list<tuple<int>>` - list of code, list of code, register name, Output shape
        '''
        def conv_shape(L,S,K,P):
	        return int(((L-K+2*P)/S + 1))
        if self.convPadding == 'SAME':
            p = (self.convFilterWidth-1)/2
        else:
            p = 0
        k = self.convFilterWidth + (self.convFilterWidth - 1)*(self.convDilation - 1)
        L = conv_shape(inputshape[1],self.convStride,k,p)
        new_shape = [None, L, self.layerUnits]
        if self.convDilation < 1:
            self.convDilation = 1
        else:
            self.convDilation = int(self.convDilation)

        if reshape_output_name != output_reg:
            A = output_reg
            B = reshape_output_name
        else:
            A = output_reg
            B = output_reg
        padding = ('SAME' if self.convPadding else 'VALID')
        # activation
        act = self.act_dict[self.activation.value]
        w_init = self.get_initiallizer()
        b_init = self.get_initiallizer(key = 'biasConfig')
        w_reg = self.get_regularizer()
        b_reg = self.get_regularizer(key = 'biasConfig')
        layer_list, call_list = [],[]
        layer = 'self.%s = tf.keras.layers.Conv1D(%d, %d, strides=%d, dilation_rate=(%d), padding=\'%s\', activation=%s, kernel_initializer=%s, bias_initializer=%s, kernel_regularizer=%s, bias_regularizer=%s)'% \
                (self.name, self.layerUnits, self.convFilterWidth, 
                self.convStride, self.convDilation, padding, act, w_init, b_init, w_reg, b_reg)
        layer_list.append(layer)
        if self.refLayerName is not None:
            inputshape = [str(x) for x in inputshape]
            self.refLayerName = self.refLayerName.replace(' ','')
            layer_list.append('self.%s.build(('%(self.name) + ','.join(inputshape) + '))')
            layer_list.append('self.%s.kernel = None'%(self.name))

            if self.refLayerTranspose:
                call_list.append('self.%s.kernel = tf.transpose(self.%s.kernel, perm=[0,2,1])'%(self.name, self.refLayerName))
            else:
                call_list.append('self.%s.kernel = self.%s.kernel'%(self.name, self.refLayerName))
        call = '%s = self.%s(%s)'% (A,self.name,B)
        call_list.append(call)
        # dropout
        if self.dropout < 100:
            d_rate = (100 - self.dropout)/100
            layer_list.append('self.%s_dp = tf.keras.layers.Dropout(%g)'%(self.name, d_rate))
            call_list.append('%s = self.%s_dp(%s)'%(A,self.name,A))

        if self.batchNorm:
            layer_list.append('self.%s_bn = BatchNormalization()'%(self.name))
            call_list.append('%s = self.%s_bn(%s, training=training)'%(A,self.name,A))
            return layer_list, call_list, output_reg, new_shape
        else:
            return layer_list, call_list, output_reg, new_shape
 
    def keras_call_auto_reshape_build(self, input_shape, combine_output_name, output_reg):
        '''
			Create keras reshape layer code   --- UPDATED (Dexter) 20200311

            Parameters
            ------------------------------

            inputshape              `list<tuple<int>>`          - list of input tensor shape.
            
            combine_output_name     `str`                       - register name from auto reshape stage

            output_reg              `str`                       - register name
                        
            Returns
            ------------------------------

            `list<str>, list<str>, <str>, list<tuple<int>>` - list of code, list of code, register name, Output shape
        '''
        layer_obj = []
        call_obj = []
        if combine_output_name != output_reg:
            A = output_reg
            B = combine_output_name
        else:
            A = output_reg
            B = output_reg
        output_name = output_reg
        if len(input_shape) != 3:
            if self.reshape == None:
                if len(input_shape) > 3:
                    axis = 2
                    new_shape = input_shape[0:axis]
                    new_shape.append(functools.reduce(lambda x,y: x*y, input_shape[axis:], 1))
                    string = 'self.%s_auto_reshape = tf.keras.layers.Reshape(('%(self.name)
                    string += ','.join([str(int(x)) for x in new_shape[1:]])
                    string += '))'
                    layer_obj.append(string)
                    string = '%s = self.%s_auto_reshape(%s)'%(A, self.name, B)
                    call_obj.append(string)
                elif len(input_shape) == 2:
                    new_shape = input_shape.copy()
                    new_shape.append(1)
                    string = 'self.%s_auto_reshape = tf.keras.layers.Reshape(('%(self.name)
                    string += ','.join([str(int(x)) for x in new_shape[1:]])
                    string += '))'
                    layer_obj.append(string)
                    string = '%s = self.%s_auto_reshape(%s)'%(A, self.name, B)
                    call_obj.append(string)
                else:
                    raise ValueError("One dimension data is not supported.")
            else:
                originalFromShapeTotal = functools.reduce(lambda x,y: x*y, input_shape[1:], 1)
                newReshape = functools.reduce(lambda a,b: a*b, self.reshape, 1)
                if (originalFromShapeTotal != newReshape):
                    raise ValueError("Unable to reshape to specified shape.")
                string = 'self.%s_auto_reshape = tf.keras.layers.Reshape(('%(self.name)
                string += ','.join([str(int(x)) for x in list(self.reshape)])
                string += '))'
                layer_obj.append(string)
                string = '%s = self.%s_auto_reshape(%s)'%(A, self.name, B)
                call_obj.append(string)
        else:
            new_shape = input_shape
            output_name = combine_output_name
        return layer_obj, call_obj, output_name, new_shape

class _ModelNodeLayerDeconvolution1D(_ModelNodeLayerConfig):
    '''
		Class representing a 1D deconvolutional Layer.   --- UPDATED (Dexter) 20180701
    '''
    def __init__(self, name: str = None, layerUnits: int = 30, convFilterWidth: int = 3, convPadding: bool = True, convStride: int = 1, convDilation: int = 1, refLayerName: str = None, refLayerTranspose: bool = False, 
                incomingConfig: 'ModelNode.Layer.Incoming.Config' = _ModelNodeLayerIncoming.Concat(), 
                linearTransform: 'Train.Variable.LinearTransform' = Train.Variable.LinearTransform.createBasicConfig(weightAvg = 0, weightStdDev = 5e-2, weightL1Loss = False, weightL2Loss = True, weightL2Decay = 0, biasInitial = 0.001),
                activation: str = "relu", activationParams: Dict = {},
                batchNorm: bool = True, batchNormParams: Dict = {}, 
                outputConfig: 'ModelNode.Layer.Output.Config' = _ModelNodeLayerOutput.Default(),
                reshape = None, buildNo = 0, weightDecayRate=0,weightStdDev=5e-2, biasInitial=0.001, weightL2Loss=True, weightAvg=0,weightL1Loss=False):
        '''
			Creates a deconvolutional layer.   --- UPDATED (Dexter) 20200117

            Parameters
            ------------------------------

            name            `str`   - The name of this layer.
            
            layerUnits      `int`   - The number of hidden units in this layer.
            
            convFilterWidth  `int`   - The CNN filter width.
            
            convPadding      `bool`  - Whether to use padding on this CNN layer, i.e. the output image size will be the same as the previous one.

            convStride       `tuple(int, int)`   - The stride of the CNN filter.   --- BETA
            
            convDilation     `int`   - The CNN dilation of the CNN filter.

            dconvPadding `ModelNode.Layer.Deconvolution.PaddingTypes` - A value defined in @ModelNode.Layer.Deconvolution.PaddingTypes .   --- BETA

            dconvUpscale `ModelNode.Layer.Deconvolution.UpscaleTypes` - The deconvolution upscale methods due to striding or stretched padding, as defined in @ModelNode.Layer.Deconvolution.UpscaleTypes .   --- BETA

            dconvAlgorithm `ModelNode.Layer.Deconvolution.AlgorithmTypes` - A value defined in @ModelNode.Layer.Deconvolution.AlgorithmTypes .

            refLayerName    `str`   - Any referenced kernal that this layer mirrors for.

            refLayerTranspose   `bool`  - Whether transpose is required for the referenced weight.
            
            incomingConfig  `ModelNode.Layer.Incoming.Config`    - Input configurations.

            linearTransform `Train.Variable.LinearTransform` - The linear transformation configuration.

            activation      `str`   - The activation function of this layer.
            
            activationParams    `dict{str:*}`   - Activation parameters as defined in TensorFlow.
            
            batchNorm       `bool`  - Whether to use batch normalization before activation function.
            
            batchNormParams     `dict{str:*}`   - Batch normalization parameters as defined in TensorFlow.
            
            outputConfig    `ModelNode.Layer.Output.Config`      - Output configurations.

            reshape         `list[int+]`    - The shape of reshaping previous layers. If not specified, it will automatically reshaped if the previous layer can't interpret as an image. [Deprecated]

            buildNo         `int`   - The build number of staged training.   [Deprecated]
            
            weightDecayRate `float` - The constant for weighting L2-loss of weights.   [Deprecated]
            
            weightStdDev    `float` - The standard deviation of the initialization of weights.   [Deprecated]
            
            biasInitial     `float` - Constant initial value of biases.   [Deprecated]
            
            weightL2Loss    `bool`  - Whether to take L2-loss on the weights.   [Deprecated]
            
            weightAvg       `float` - The average value of the initialization of weights.   [Deprecated]
            
            weightL1Loss    `bool`  - Whether to take L1-loss on the weights.   [Deprecated]
        '''
        if (linearTransform is None and any([param is not None for param in [weightAvg, weightStdDev, weightL1Loss, weightL2Loss, weightDecayRate, biasInitial]])):
            linearTransform = Train.Variable.LinearTransform.createBasicConfig(weightAvg = weightAvg, weightStdDev = weightStdDev, 
                              weightL1Loss = weightL1Loss, weightL2Loss = weightL2Loss, weightL2Decay = weightDecayRate, biasInitial = biasInitial)
        super().__init__(ModelNode.Layer.Types.Deconvolution1D, name = name, layerUnits = layerUnits, incomingConfig = incomingConfig,
                        activation = activation, activationParams = activationParams, linearTransform = linearTransform,
                        batchNorm = batchNorm, batchNormParams = batchNormParams, outputConfig = outputConfig)
        # `int` - The deconvolution layer filter width.
        self.convFilterWidth = convFilterWidth
        # `bool` - Whether to use padding on this deconvolution layer, i.e. the output image size will be the same as the previous one.
        self.convPadding = convPadding
        # `int` - The stride of the deconvolution layer filter.
        self.convStride = convStride
        # `int` - The convolution dilation of this deconvolution filter.
        self.convDilation = convDilation
        # `str` - Any referenced weight that this layer mirrors for.
        self.refLayerName = refLayerName
        # `bool` - Whether transpose is required for the referenced weight.
        self.refLayerTranspose = refLayerTranspose

    def _copySymmetricLayerConfig(self, useSymmetricWeights = True, buildNo: int = 0) -> 'ModelNode.Layer.Convolution':
        '''
			Create a symmetric layer profile.   --- BETA --- UPDATED (Dexter) 20190822

            Parameters
            ------------------------------

            useSymmetricWeights `bool` - Whether to use symmetric weights.

            buildNo             `int`   - The build number to be built.
        
            Returns
            ------------------------------

            `ModelNode.Layer.Convolution`    - A copied profile of this layer.
        '''
        if (len(self.fromNode[buildNo]) != 1):
            raise ValueError("Layers not having one and only one layer node inputs are not able to be copied in a symmetric way.")

        return ModelNode.Layer.Convolution1D(name=(self.name + "_Sym"), layerUnits = self.layerUnits, convFilterWidth=self.convFilterWidth, convPadding=self.convPadding, convStride=self.convStride, convDilation=self.convDilation, 
                refLayerName = self.name if useSymmetricWeights else None, refLayerTranspose=True, 
                incomingConfig=self.incomingConfig, linearTransform=self.linearTransform, 
                activation=self.activation, activationParams=self.activationParams, 
                batchNorm=self.batchNorm, batchNormParams=self.batchNormParams, 
                outputConfig=self.outputConfig)

    def copy(self, name: str) -> 'ModelNode.Layer.Deconvolution':
        '''
			Copy this layer profile.   --- UPDATED (Dexter) 20190822

            Parameters
            ------------------------------

            name    `str`   - The new name of the copied layer

            Returns
            ------------------------------

            `ModelNode.Layer.Deconvolution`    - A copied profile of this layer.
        '''
        # Ensure this is not a subclass.
        if (not isinstance(self, ModelNode.Layer.Deconvolution1D)):
            raise ValueError("This layer class (" + self.__class__.__name__ + ") has not supported for copying.")
        
        # Create a new Layer Profile on this.
        return ModelNode.Layer.Deconvolution1D(name=name, layerUnits = self.layerUnits, convFilterWidth=self.convFilterWidth, convPadding=self.convPadding, convStride=self.convStride,
                convDilation=self.convDilation, refLayerName = self.refLayerName, refLayerTranspose=self.refLayerTranspose, 
                incomingConfig=self.incomingConfig, linearTransform=self.linearTransform, 
                activation=self.activation, activationParams=self.activationParams, 
                batchNorm=self.batchNorm, batchNormParams=self.batchNormParams, 
                outputConfig = self.outputConfig)

    def _build(self, buildNo: int):
        '''
			Build the TensorFlow Graph of this layer.   --- UPDATED (Dexter) 20200501

            Parameters
            ------------------------------

            buildNo     `int`   - The build number to be built.
        '''
        self._clearTempTensors()

        # 0. Ensure all incoming nodes have been built.
        if (all([n._built for n in self.fromNode[buildNo]])):
            # 1. Get refrenced layer kernel if required
            if (self.refLayerName is not None):
                refLayer = self.train.modelNodes[self.refLayerName]
                with tf.compat.v1.variable_scope(self.refLayerName, reuse=tf.compat.v1.AUTO_REUSE) as scope:
                    kernal = Train.Variable.getFromGraph("weight")
                
                if (self.refLayerTranspose):
                    with tf.compat.v1.variable_scope(self.name, reuse=tf.compat.v1.AUTO_REUSE) as scope:
                        kernal = tf.transpose(kernal, [0,2,1])

            # TensorFlow Graph building, using the layer name as the scope
            with tf.compat.v1.variable_scope(self.name, reuse=tf.compat.v1.AUTO_REUSE) as scope:
                # 2. Get previous tensors
                fromTensor = self._combineIncomingTensors(buildNo = buildNo)
                fromShape = [*fromTensor.shape]

                # 3.Apply preprocessing reshape to support deconvolutional layer for other shapes of data.
                if (len(fromShape) != 3):
                    # If more than 3 dimension, flatten to the 3rd dim.
                    if (len(fromShape) > 3):
                        flattenSize = functools.reduce(lambda x,y: x*y, fromShape[2:], 1)
                        fromTensor = tf.reshape(fromTensor, [*Train.Variable.setAsReshape(fromTensor.shape[:2]), flattenSize])
                        fromShape = [*fromTensor.shape]

                    # If only 2 dimension, supplement a dimension.
                    elif (len(fromShape) == 2):
                        fromTensor = tf.reshape(fromTensor, [*Train.Variable.setAsReshape(fromTensor.shape), 1])
                        fromShape = [*fromTensor.shape]
                    
                    # 2 or lower dimension is not supported.
                    else:
                        raise ValueError("2 or lower dimension is not supported for DCNN Layer.")

                # 4. If no linked kernal, build a new kernal
                transformConfig = self.linearTransform
                if (self.refLayerName is None):
                    kernal = transformConfig.weightConfig.create("weight", [self.convFilterWidth,fromShape[-1],self.layerUnits], dtype=fromTensor.dtype, defaultDevice = self.train.device)

                # 5. Set biases
                biases = transformConfig.biasConfig.create("biases", [kernal.shape[-1]], dtype=fromTensor.dtype, defaultDevice = self.train.device)
                self._weights.extend([kernal, biases])

                # 6. Calculate the output shape.
                outputW = 0
                if self.convPadding:
                    outputW = fromShape[1] * self.convStride
                else:
                    # Ref: https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/python/keras/utils/conv_utils.py (def deconv_output_length)
                    outputMorePxW = max([self.convFilterWidth+(self.convDilation-1)*(self.convFilterWidth-1) - self.convStride, 0])
                    outputW = fromShape[1] * self.convStride + outputMorePxW

                # 7. Apply deconvolution.
                # 7-1. Utilize Convolutional 2-D Transpose deconvolution.
                if (self.train.device.startswith("/cpu") and self.convDilation > 1):
                    raise ValueError("Deconvolutional dilation is not supported for CPU processing. --- at Layer: " + self.name)
                elif (self.convDilation > 1 and any([s>1 for s in self.convStride])):
                    raise ValueError("Deconvolutional layer using convolutional 2D transpose currently does not support dilation > 1 when stride > 1.   --- at Layer: " + self.name)

                with tf.device(self.train.device): 
                    mid = tf.nn.conv1d_transpose(fromTensor, filter=tf.transpose(kernal, [0,2,1]), output_shape=[tf.shape(fromTensor)[0], outputW, kernal.shape[-1]], strides=[1,self.convStride,1], padding=('SAME' if self.convPadding else 'VALID'), dilations = [1, self.convDilation, 1])
                    mid = tf.nn.bias_add(mid, biases)

                # 8. General layer operations.
                mid = self._commonLayerOps(mid)

                # 9. Final output
                self._outputTensor = self._processOutputTensor(mid)

            self._built = True
            # Create chain build actions
            for n in self.toNode[buildNo]:
                n._build(buildNo)

    def keras_call_layer_build(self, reshape_output_name, output_reg, inputshape = None):
        '''
			Create keras layer code   --- UPDATED (Dexter) 20200312

            Parameters
            ------------------------------

            reshape_output_name     `str`                   - register name from auto reshape stage

            output_reg              `str`                   - register name
            
            inputshape              `list<tuple<int>>`      - list of input tensor shape.
                        
            Returns
            ------------------------------

            `list<str>, list<str>, <str>, list<tuple<int>>` - list of code, list of code, register name, Output shape
        '''
        if self.convPadding:
            L = inputshape[1] * self.convStride
        else:
            # Ref: https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/python/keras/utils/conv_utils.py (def deconv_output_length)
            outputMorePx = max([self.convFilterWidth+(self.convDilation-1)*(self.convFilterWidth-1) - self.convStride, 0])
            L = inputshape[1] * self.convStride + outputMorePx
        new_shape = [None, L, self.layerUnits]

        if reshape_output_name != output_reg:
            A = output_reg
            B = reshape_output_name
        else:
            A = output_reg
            B = output_reg
        padding = ('SAME' if self.convPadding else 'VALID')
        # activation
        '''if self.activation == Train.Activation.Relu6 or self.activation == Train.Activation.Crelu:
            act = 'relu'
        elif self.activation == Train.Activation.HardSigmoid:
            act = 'hard_sigmoid'
        else:
            act = (Train.Activation.getName(self.activation)).split('.')[-1].lower()
        '''
        act = self.act_dict[self.activation.value]
        w_init = self.get_initiallizer()
        b_init = self.get_initiallizer(key = 'biasConfig')
        w_reg = self.get_regularizer()
        b_reg = self.get_regularizer(key = 'biasConfig')
        layer_list, call_list = [],[]
        layer_before = 'self.%s_before = tf.keras.layers.Lambda(lambda x: tf.keras.backend.expand_dims(x, axis=2))'%(self.name,)
        layer_list.append(layer_before)
        call = '%s = self.%s_before(%s)'% (A,self.name,B)
        call_list.append(call)
        
        layer = 'self.%s = tf.keras.layers.Conv2DTranspose(%d,(%d, 1), strides=(%d, 1), dilation_rate=(%d, 1), padding=\'%s\', activation=%s, kernel_initializer=%s, bias_initializer=%s, kernel_regularizer=%s, bias_regularizer=%s)'% \
                (self.name, self.layerUnits, self.convFilterWidth, 
                self.convStride, self.convDilation, padding, act, w_init, b_init, w_reg, b_reg)
        layer_list.append(layer)
        
        if self.refLayerName is not None:
            self.refLayerName = self.refLayerName.replace(' ','')
            inputshape = [str(x) for x in inputshape]
            layer_list.append('self.%s.build(('%(self.name) + ','.join(inputshape) + '))')
            layer_list.append('self.%s.kernel = None'%(self.name))

            if self.refLayerTranspose:
                call_list.append('self.%s.kernel = self.%s.kernel'%(self.name, self.refLayerName))
            else:
                call_list.append('self.%s.kernel = self.%s.kernel'%(self.name, self.refLayerName))
        call = '%s = self.%s(%s)'% (A,self.name,A)
        call_list.append(call)

        layer_after = 'self.%s_after = tf.keras.layers.Lambda(lambda x: tf.keras.backend.squeeze(x, axis=2))'%(self.name,)
        layer_list.append(layer_after)
        call = '%s = self.%s_after(%s)'% (A,self.name,A)
        call_list.append(call)

        # dropout
        if self.dropout < 100:
            d_rate = (100 - self.dropout)/100
            layer_list.append('self.%s_dp = tf.keras.layers.Dropout(%g)'%(self.name, d_rate))
            call_list.append('%s = self.%s_dp(%s)'%(A,self.name,A))
        if self.batchNorm:
            layer_list.append('self.%s_bn = BatchNormalization()'%(self.name))
            call_list.append('%s = self.%s_bn(%s, training=training)'%(A,self.name,A))
            #return [layer, 'self.%s_bn = BatchNormalization()'%(self.name)], [call, '%s = self.%s_bn(%s, training=training)'%(A,self.name,A)]
            return layer_list, call_list, output_reg, new_shape
        else:
            return layer_list, call_list, output_reg, new_shape

    def keras_call_auto_reshape_build(self, input_shape, combine_output_name, output_reg):
        '''
			Create keras reshape layer code   --- UPDATED (Dexter) 20200312

            Parameters
            ------------------------------

            inputshape              `list<tuple<int>>`          - list of input tensor shape.
            
            combine_output_name     `str`                       - register name from auto reshape stage

            output_reg              `str`                       - register name
                        
            Returns
            ------------------------------

            `list<str>, list<str>, <str>, list<tuple<int>>` - list of code, list of code, register name, Output shape
        '''
        layer_obj = []
        call_obj = []
        if combine_output_name != output_reg:
            A = output_reg
            B = combine_output_name
        else:
            A = output_reg
            B = output_reg
        output_name = output_reg
        if len(input_shape) != 3:
            if len(input_shape) > 3:
                axis = 2
                new_shape = input_shape[0:axis]
                new_shape.append(functools.reduce(lambda x,y: x*y, input_shape[axis:], 1))
                string = 'self.%s_auto_reshape = tf.keras.layers.Reshape(('%(self.name)
                string += ','.join([str(int(x)) for x in new_shape[1:]])
                string += '))'
                layer_obj.append(string)
                string = '%s = self.%s_auto_reshape(%s)'%(A, self.name, B)
                call_obj.append(string)
            elif len(input_shape) == 2:
                new_shape = input_shape.copy()
                new_shape.append(1)
                string = 'self.%s_auto_reshape = tf.keras.layers.Reshape(('%(self.name)
                string += ','.join([str(int(x)) for x in new_shape[1:]])
                string += '))'
                layer_obj.append(string)
                string = '%s = self.%s_auto_reshape(%s)'%(A, self.name, B)
                call_obj.append(string)
            else:
                raise ValueError("One dimension data is not supported.")
        else:
            new_shape = input_shape
            output_name = combine_output_name
        return layer_obj, call_obj, output_name, new_shape

class RNNLayer(_ModelNodeLayerConfig):
    '''
			Class representing a RNN layer.   --- UPDATED (CYK) 20190916
    '''
        
    def __init__(self,name: str = None, layerUnits: int = 150, incomingConfig: 'IncomingConfig' = _ModelNodeLayerIncoming.Concat(),
                    isKerasLayer: bool = False, 
                    linearTransform: 'RNNConfig' = RNNConfig.createBasicConfig(),
                    activation: int = Train.Activation.Tanh, activationParams: Dict = {},
                    batchNorm: bool = True, useBias: bool = True, dropout: float = 1, recurrentDropout: float = 1,
                    stateClip: float = None, statefull: bool = False, goBackwards: bool = False, returnSequenceLen: int = None,
                    returnSequences: bool = False, returnState: bool = False, outputConfig: 'ModelNode.Layer.Output.Config' = _ModelNodeLayerOutput.Default()):
        '''
			Create a RNN layer.   --- UPDATED (CYK) 20200117

            Parameters
            ------------------------------

            name            `str`   - The name of this layer.
            
            layerUnits      `int`   - The number of hidden units in this layer.

            incomingConfig  `IncomingConfig`    - Input configurations.

            isKerasLayer    `bool`  - Whether to use tf.Layer.keras or not.

            linearTransform `RNNConfig` - The RNN configuration.

            activation      `int`   - The activation function of this layer.
            
            activationParams    `dict{str:*}`   - Activation parameters as defined in TensorFlow.
            
            batchNorm       `bool`  - Whether to use batch normalization before activation function.

            useBias         `bool`  - whether the layer uses bias.
            
            dropout         `float` - The keep probability of dropout during training.

            recurrentDropout    `float` - The keep probability of recurrentDropout during training.

            stateClip       `float` - If provided the new state is clipped by this value .

            statefull       `bool`  -  whether the last state for each sample at index i in a batch will be used as initial state for the sample of index i in the next batch.

            goBackwards     `bool`  - whether input sequence enter backwards.

            returnSequenceLen    `int` - The sequence length of output.(only for rl LSTM Layer)

            returnSequences  `bool`  - Whether to only return the last time step output , or the whole output sequence.(only for keras LSTM Layer)

            returnState     `bool`  - Whether to return the last states and output, The returned elements of the last states list are the memory cell and the shadow State.

            outputConfig    `OutputConfig`      - Output configurations.

        '''
        if linearTransform is None:
            linearTransform = RNNConfig.createBasicConfig()
        super().__init__(ModelNode.Layer.Types.RNN, name = name, layerUnits = layerUnits, incomingConfig = incomingConfig, linearTransform = linearTransform,
                    batchNorm = batchNorm, outputConfig = outputConfig)
        self.isKerasLayer = isKerasLayer
        self.activation =  activation
        self.activationParams = activationParams
        self.useBias = useBias
        self.dropout = dropout
        self.recurrentDropout = recurrentDropout
        self.stateClip = stateClip
        self.statefull = statefull
        self.goBackwards = goBackwards
        self.returnSequenceLen = returnSequenceLen
        self.returnSequences = returnSequences
        self.returnState = returnState

    def copy(self,name):
        '''
			Copy this layer profile.   --- UPDATED (CYK) 20190916

            Parameters
            ------------------------------

            name    `str`   - The new name of the copied layer
        '''
        # Ensure this is not a subclass.
        if (self.__class__.__name__ != "RNNLayer"):
            raise ValueError("This layer class (" + self.__class__.__name__ + ") has not supported for copying.")
        
        # Create a new Layer Profile on this.
        return RNNLayer(ModelNode.Layer.Types.RNN, name=name, layerUnits =  self.layerUnits, incomingConfig = self.incomingConfig,
                    isKerasLayer = self.isKerasLayer, linearTransform = self.linearTransform, activation = self.activation, activationParams = self.activationParams,
                    batchNorm = self.batchNorm, useBias = self.useBias, dropout = self.dropout, recurrentDropout = self.recurrentDropout,
                    stateClip = self.stateClip, statefull = self.statefull, goBackwards = self.goBackwards,
                    returnSequenceLen = self.returnSequenceLen, returnSequences = self.returnSequences, returnState = self.returnState, outputConfig = self.outputConfig)
    
    def keras_layer_build(self):
        if self.isKerasLayer:
            return 'self.%s = tf.keras.layers.SimpleRNN(%d)'%(self.name, self.layerUnits)
        else:
            return 'self.%s = RNN(%d)'%(self.name, self.layerUnits)

    def _activationToKerasName(self,activation):
        if activation == Train.Activation.Relu6 or activation == Train.Activation.Crelu:
            act = 'relu'
        elif activation == Train.Activation.HardSigmoid:
            act = 'hard_sigmoid'
        else:
            act = (Train.Activation.getName(activation)).split('.')[-1].lower()
        return act

    def keras_call_layer_build(self, reshape_output_name, output_reg, inputshape = None):
        new_shape = [None,self.returnSequenceLen,self.layerUnits]
        # 
        # dropout
        self.dropout = 1 - self.dropout / 100.
        self.recurrentDropout = 1 - self.recurrentDropout / 100.

        if reshape_output_name != output_reg:
            A = output_reg
            B = reshape_output_name
        else:
            A = output_reg
            B = output_reg

        layer_list, call_list = [],[]
        

        #act = self._activationToKerasName(self.activation)
        act = self.act_dict[self.activation.value]
        u_b = 'True' if self.useBias else 'False'
        w_init = self.get_initiallizer(key = 'weightConfig')
        re_w_init = self.get_initiallizer(key = 'recurrentWeightConfig')
        b_init = self.get_initiallizer(key = 'biasConfig')
        w_reg = self.get_regularizer(key = 'weightConfig')
        re_w_reg = self.get_regularizer(key = 'recurrentWeightConfig')
        b_reg = self.get_regularizer(key = 'biasConfig')
        d = self.dropout
        re_d = self.recurrentDropout
        r_st = 'True' if self.returnState else 'False'
        g_b = 'True' if self.goBackwards else 'False'
        sf = 'True' if self.statefull else 'False'

        if self.isKerasLayer:
            r_se = 'True' if self.returnSequences else 'False'
            
            layer = 'self.%s = tf.keras.layers.SimpleRNN(%d, activation = %s, use_bias=%s,  kernel_initializer=%s, recurrent_initializer=%s, bias_initializer=%s,'\
                 'kernel_regularizer=%s, recurrent_regularizer=%s, bias_regularizer=%s, dropout=%f, recurrent_dropout=%f, return_sequences=%s, return_state=%s,'\
                 'go_backwards=%s, stateful=%s )'% (self.name, self.layerUnits, act, u_b, w_init, re_w_init, b_init, w_reg, re_w_reg, b_reg, d, re_d, r_se, r_st, g_b, sf)
        else:
            s_c = 'None' if self.stateClip is None else str(self.stateClip)
            r_se_l = 'None' if self.returnSequenceLen is None else str(self.returnSequenceLen)   

            layer = 'self.%s = RNN(%d, activation = %s,kernelInitializer = %s,recurrentInitializer = %s,biasInitializer = %s,'\
                'kernelRegularizer = %s,recurrentRegularizer = %s,biasRegularizer = %s,useBias = %s,dropout =%f,recurrentDropout =%f,'\
                'stateClip = %s, statefull = %s, goBackwards = %s, returnSequenceLen = %s, returnState = %s)'% \
                (self.name, self.layerUnits, act, w_init, re_w_init, b_init, w_reg ,re_w_reg,b_reg, u_b, d, re_d, s_c, sf, g_b, r_se_l, r_st)
        
        layer_list.append(layer)
        
        #if self.refLayerName is not None:
        #    raise ValueError('RNNLayer can\'t mirrors for refernced referenced kernal from ohter layer' )

        call = '%s = self.%s(%s)'% (A,self.name,B)
        call_list.append(call)
        
        if self.batchNorm:
            layer_list.append('self.%s_bn = BatchNormalization()'%(self.name))
            call_list.append('%s = self.%s_bn(%s, training=training)'%(A,self.name,A))
            return layer_list, call_list, output_reg, new_shape
        else:
            return layer_list, call_list, output_reg, new_shape
    
    def keras_call_auto_reshape_build(self, input_shape, combine_output_name, output_reg):
        layer_obj = []
        call_obj = []

        if combine_output_name != output_reg:
            A = output_reg
            B = combine_output_name
        else:
            A = output_reg
            B = output_reg
        output_name = output_reg

        if len(input_shape) != 3:
            if len(input_shape) > 3 :
                if len(input_shape) < 5:
                    new_shape = input_shape[0:2]
                    new_shape.append(-1)
                    string = 'self.%s_auto_reshape = tf.keras.layers.Reshape(('%(self.name)
                    string += ','.join([str(int(x)) for x in new_shape[1:]])
                    string += '))'
                    layer_obj.append(string)
                    string = '%s = self.%s_auto_reshape(%s)'%(A, self.name, B)
                    call_obj.append(string)
                else:
                    raise ValueError("When dimension of data is more than 4, it is not supported.")
            elif len(input_shape) == 2 :
                dim = input_shape[-1]
                new_shape = input_shape[0:1]
                new_shape.append(1)
                new_shape.append(dim)
                string = 'self.%s_auto_reshape = tf.keras.layers.Reshape(('%(self.name)
                string += ','.join(['1',str(int(dim))])
                string += '))'
                layer_obj.append(string)
                string = '%s = self.%s_auto_reshape(%s)'%(A, self.name, B)
                call_obj.append(string)
            else:
                raise ValueError("One dimension data is not supported.")
        else:
            new_shape = input_shape
            output_name = combine_output_name
        return layer_obj, call_obj, output_name, new_shape

class GRULayer(_ModelNodeLayerConfig):
    '''
			Class representing a GRU layer.   --- UPDATED (CYK) 20191101
    '''       
    def __init__(self,name: str = None, layerUnits: int = 150, incomingConfig: 'IncomingConfig' = _ModelNodeLayerIncoming.Concat(),
                    isKerasLayer: bool = False, 
                    linearTransform: 'GRUConfig' = GRUConfig.createBasicConfig(),
                    resetGateActivation: int = Train.Activation.Sigmoid,updateGateActivation: int = Train.Activation.Sigmoid,stateCandidateActivation: int = Train.Activation.Tanh,
                    batchNorm: bool = True, useBias: bool = True, dropout: float = 1, recurrentDropout: float = 1,
                    stateClip: float = None, statefull: bool = False, goBackwards: bool = False, returnSequenceLen: int = None,
                    returnSequences: bool = False, returnState: bool = False, outputConfig: 'ModelNode.Layer.Output.Config' = _ModelNodeLayerOutput.Default()):
        '''
			Create a LSTM layer.   --- UPDATED (CYK) 20200117

            Parameters
            ------------------------------

            name            `str`   - The name of this layer.
            
            layerUnits      `int`   - The number of hidden units in this layer.

            incomingConfig  `IncomingConfig`    - Input configurations.

            isKerasLayer    `bool`  - Whether to use tf.Layer.keras or not.

            linearTransform `GRUConfig` - The GRU configuration.

            resetGateActivation       `int`   - The activation function of resetGate of this layer.

            updateGateActivation      `int`   - The activation function of updateGate of this layer.

            stateCandidateActivation      `int`   - The activation function of stateCandidate of this layer.
            
            batchNorm       `bool`  - Whether to use batch normalization before activation function.

            useBias         `bool`  - whether the layer uses bias.

            usePeephole     `bool`  -  whether the layer uses Peephole.
            
            dropout         `float` - The keep probability of dropout during training.

            recurrentDropout    `float` - The keep probability of recurrentDropout during training.

            stateClip `float` - If provided the new state is clipped by this value .

            statefull       `bool`  -  whether the last state for each sample at index i in a batch will be used as initial state for the sample of index i in the next batch.

            goBackwards     `bool`  - whether input sequence enter backwards.

            returnSequenceLen    `int` - The sequence length of output.(only for rl GRU Layer)

            returnSequences  `bool`  - Whether to only return the last time step output , or the whole output sequence.(only for keras GRU Layer)

            returnState     `bool`  - Whether to return the last states and output.

            outputConfig    `OutputConfig`      - Output configurations.

        '''
        if linearTransform is None:
            linearTransform = GRUConfig.createBasicConfig()
        super().__init__(ModelNode.Layer.Types.GRU, name = name, layerUnits = layerUnits, incomingConfig = incomingConfig, linearTransform = linearTransform,
                    batchNorm = batchNorm, outputConfig = outputConfig)
        self.isKerasLayer = isKerasLayer
        if self.isKerasLayer:
            if (resetGateActivation != updateGateActivation):
                raise ValueError('The recurrent_activation of keras GRU layer is wrong')

        self.resetGateActivation = resetGateActivation
        self.updateGateActivation = updateGateActivation
        self.stateCandidateActivation = stateCandidateActivation
        self.useBias = useBias
        self.dropout = dropout
        self.recurrentDropout = recurrentDropout
        self.stateClip = stateClip
        self.statefull = statefull
        self.goBackwards = goBackwards
        self.returnSequenceLen = returnSequenceLen
        self.returnSequences = returnSequences
        self.returnState = returnState

    def copy(self,name):
        '''
			Copy this layer profile.   --- UPDATED (CYK) 20191101

            Parameters
            ------------------------------

            name    `str`   - The new name of the copied layer
        '''
        # Ensure this is not a subclass.
        if (self.__class__.__name__ != "GRULayer"):
            raise ValueError("This layer class (" + self.__class__.__name__ + ") has not supported for copying.")
        
        # Create a new Layer Profile on this.
        return GRULayer(name=name, layerUnits =  self.layerUnits, incomingConfig = self.incomingConfig,
                    isKerasLayer = self.isKerasLayer, linearTransform = self.linearTransform, resetGateActivation = self.resetGateActivation,
                    updateGateActivation = self.updateGateActivation, stateCandidateActivation = self.stateCandidateActivation,
                    batchNorm = self.batchNorm, useBias = self.useBias, dropout = self.dropout, recurrentDropout = self.recurrentDropout,
                    stateClip = self.stateClip, statefull = self.statefull, goBackwards = self.goBackwards, returnSequenceLen = self.returnSequenceLen,
                    returnSequences = self.returnSequences, returnState = self.returnState, outputConfig = self.outputConfig)
    
    def keras_layer_build(self):
        if self.isKerasLayer:
            return 'self.%s = tf.keras.layers.GRU(%d)'%(self.name, self.layerUnits)
        else:
            return 'self.%s = GRU(%d)'%(self.name, self.layerUnits)

    def _activationToKerasName(self,activation):
        if activation == Train.Activation.Relu6 or activation == Train.Activation.Crelu:
            act = 'relu'
        elif activation == Train.Activation.HardSigmoid:
            act = 'hard_sigmoid'
        else:
            act = (Train.Activation.getName(activation)).split('.')[-1].lower()
        return act

    def keras_call_layer_build(self, reshape_output_name, output_reg, inputshape = None):
        new_shape = [None,self.returnSequenceLen,self.layerUnits]
        self.dropout = 1 - self.dropout / 100.
        self.recurrentDropout = 1 - self.recurrentDropout / 100.
        if reshape_output_name != output_reg:
            A = output_reg
            B = reshape_output_name
        else:
            A = output_reg
            B = output_reg

        layer_list, call_list = [],[]
        
        u_b = 'True' if self.useBias else 'False'
        d = self.dropout
        re_d = self.recurrentDropout
        r_st = 'True' if self.returnState else 'False'
        g_b = 'True' if self.goBackwards else 'False'
        sf = 'True' if self.statefull else 'False'

        if self.isKerasLayer:
            act = self.act_dict[int(self.stateCandidateActivation)]
            re_act = self.act_dict[int(self.resetGateActivation)]
            w_init = self.get_initiallizer(key = 'resetGateWeightConfig')
            re_w_init = self.get_initiallizer(key = 'resetGateRecurrentWeightConfig')
            b_init = self.get_initiallizer(key = 'resetGateBiasConfig')
            w_reg = self.get_regularizer(key = 'resetGateWeightConfig')
            re_w_reg = self.get_regularizer(key = 'resetGateRecurrentWeightConfig')
            b_reg = self.get_regularizer(key = 'resetGateBiasConfig')
            r_se = 'True' if self.returnSequences else 'False'
            layer = 'self.%s = tf.keras.layers.GRU(%d, activation = %s,recurrent_activation = %s, use_bias=%s,  kernel_initializer=%s, recurrent_initializer=%s, bias_initializer=%s,'\
                 'kernel_regularizer=%s, recurrent_regularizer=%s, bias_regularizer=%s, dropout=%f, recurrent_dropout=%f, return_sequences=%s, return_state=%s,'\
                 'go_backwards=%s, stateful=%s )'% (self.name, self.layerUnits, act, re_act, u_b, w_init, re_w_init, b_init, w_reg, re_w_reg, b_reg, d, re_d, r_se, r_st, g_b, sf)

        else:
            #res_act = self._activationToKerasName(self.resetGateActivation)
            res_act = self.act_dict[int(self.resetGateActivation)]
            #up_act = self._activationToKerasName(self.updateGateActivation)
            up_act = self.act_dict[int(self.updateGateActivation)]
            #sta_act = self._activationToKerasName(self.stateCandidateActivation)
            sta_act = self.act_dict[int(self.stateCandidateActivation)]
            res_w_init = self.get_initiallizer(key = 'resetGateWeightConfig')
            up_w_init = self.get_initiallizer(key = 'updateGateWeightConfig')
            sta_w_init = self.get_initiallizer(key = 'stateCandidateWeightConfig')
            res_re_w_init = self.get_initiallizer(key = 'resetGateRecurrentWeightConfig')
            up_re_w_init = self.get_initiallizer(key = 'updateGateRecurrentWeightConfig')
            sta_re_w_init = self.get_initiallizer(key = 'stateCandidateRecurrentWeightConfig')
            res_b_init = self.get_initiallizer(key = 'resetGateBiasConfig')
            up_b_init = self.get_initiallizer(key = 'updateGateBiasConfig')
            sta_b_init = self.get_initiallizer(key = 'stateCandidateBiasConfig')
            res_w_reg = self.get_regularizer(key = 'resetGateWeightConfig')
            up_w_reg = self.get_regularizer(key = 'updateGateWeightConfig')
            sta_w_reg = self.get_regularizer(key = 'stateCandidateWeightConfig')
            res_re_w_reg = self.get_regularizer(key = 'resetGateRecurrentWeightConfig')
            up_re_w_reg = self.get_regularizer(key = 'updateGateRecurrentWeightConfig')
            sta_re_w_reg = self.get_regularizer(key = 'stateCandidateRecurrentWeightConfig')
            res_b_reg = self.get_regularizer(key = 'resetGateBiasConfig')
            up_b_reg = self.get_regularizer(key = 'updateGateBiasConfig')
            sta_b_reg = self.get_regularizer(key = 'stateCandidateBiasConfig') 
            sta_c = 'None' if self.stateClip is None else str(self.stateClip)
            r_se_l = 'None' if self.returnSequenceLen is None else str(self.returnSequenceLen)
            layer = 'self.%s = GRU(%d, activation = GRU.Activation(%s,%s,%s),kernelInitializer = GRU.Initializer(%s,%s,%s),'\
                'recurrentInitializer = GRU.Initializer(%s,%s,%s),biasInitializer = GRU.Initializer(%s,%s,%s),'\
                'kernelRegularizer = GRU.Regularizer(%s,%s,%s),recurrentRegularizer = GRU.Regularizer(%s,%s,%s),'\
                'biasRegularizer = GRU.Regularizer(%s,%s,%s),useBias = %s,dropout =%f,recurrentDropout =%f,'\
                'stateClip = %s, statefull = %s, goBackwards = %s, returnSequenceLen = %s, returnState = %s)'% \
                (self.name, self.layerUnits, res_act, up_act, sta_act, res_w_init, up_w_init, sta_w_init,
                    res_re_w_init, up_re_w_init, sta_re_w_init, 
                    res_b_init, up_b_init, sta_b_init, res_w_reg, up_w_reg, sta_w_reg,
                    res_re_w_reg, up_re_w_reg, sta_re_w_reg,
                    res_b_reg, up_b_reg, sta_b_reg, u_b, d, re_d, sta_c, sf, g_b, r_se_l, r_st)
        layer_list.append(layer)
        
        #if self.refLayerName is not None:
        #    raise ValueError('GRULayer can\'t mirrors for refernced referenced kernal from ohter layer' )

        call = '%s = self.%s(%s)'% (A,self.name,B)
        call_list.append(call)
        
        if self.batchNorm:
            layer_list.append('self.%s_bn = BatchNormalization()'%(self.name))
            call_list.append('%s = self.%s_bn(%s, training=training)'%(A,self.name,A))
            return layer_list, call_list, output_reg, new_shape
        else:
            return layer_list, call_list, output_reg, new_shape
    
    def keras_call_auto_reshape_build(self, input_shape, combine_output_name, output_reg):
        layer_obj = []
        call_obj = []

        if combine_output_name != output_reg:
            A = output_reg
            B = combine_output_name
        else:
            A = output_reg
            B = output_reg
        output_name = output_reg

        if len(input_shape) != 3:
            if len(input_shape) > 3 :
                if len(input_shape) < 5:
                    new_shape = input_shape[0:2]
                    new_shape.append(-1)
                    string = 'self.%s_auto_reshape = tf.keras.layers.Reshape(('%(self.name)
                    string += ','.join([str(int(x)) for x in new_shape[1:]])
                    string += '))'
                    layer_obj.append(string)
                    string = '%s = self.%s_auto_reshape(%s)'%(A, self.name, B)
                    call_obj.append(string)
                else:
                    raise ValueError("When dimension of data is more than 4, it is not supported.")
            elif len(input_shape) == 2 :
                dim = input_shape[-1]
                new_shape = input_shape[0:1]
                new_shape.append(1)
                new_shape.append(dim)
                string = 'self.%s_auto_reshape = tf.keras.layers.Reshape(('%(self.name)
                string += ','.join(['1',str(int(dim))])
                string += '))'
                layer_obj.append(string)
                string = '%s = self.%s_auto_reshape(%s)'%(A, self.name, B)
                call_obj.append(string)
            else:
                raise ValueError("One dimension data is not supported.")
        else:
            new_shape = input_shape
            output_name = combine_output_name
        return layer_obj, call_obj, output_name, new_shape

class LSTMLayer(_ModelNodeLayerConfig):
    '''
			Class representing a LSTM layer.   --- UPDATED (CYK) 20190918
    '''       
    def __init__(self,name: str = None, layerUnits: int = 150, incomingConfig: 'IncomingConfig' = _ModelNodeLayerIncoming.Concat(),
                    isKerasLayer: bool = False, 
                    linearTransform: 'LSTMConfig' = LSTMConfig.createBasicConfig(),
                    inputGateActivation: int = Train.Activation.Sigmoid,forgetGateActivation: int = Train.Activation.Sigmoid,outputGateActivation: int = Train.Activation.Sigmoid,
                    memoryActivation: int = Train.Activation.Tanh,shadowGateActivation: int = Train.Activation.Tanh,
                    batchNorm: bool = True, useBias: bool = True, usePeephole: bool = False, dropout: float = 1, recurrentDropout: float = 1,
                    memoryCellClip: float = None, shadowStateClip: float = None, statefull: bool = False, goBackwards: bool = False, returnSequenceLen: int = None,
                    returnSequences: bool = False, returnState: bool = False, outputConfig: 'ModelNode.Layer.Output.Config' = _ModelNodeLayerOutput.Default()):
        '''
			Create a LSTM layer.   --- UPDATED (CYK) 20200117

            Parameters
            ------------------------------

            name            `str`   - The name of this layer.
            
            layerUnits      `int`   - The number of hidden units in this layer.

            incomingConfig  `IncomingConfig`    - Input configurations.

            isKerasLayer    `bool`  - Whether to use tf.Layer.keras or not.

            linearTransform `LSTMConfig` - The LSTM configuration.

            inputGateActivation       `int`   - The activation function of inputGate of this layer.

            forgetGateActivation      `int`   - The activation function of forgetGate of this layer.

            outputGateActivation      `int`   - The activation function of outputGate of this layer.

            memoryActivation          `int`   - The activation function of memory of this layer.

            shadowGateActivation      `int`   - The activation function of shadowGate of this layer.
            
            batchNorm       `bool`  - Whether to use batch normalization before activation function.

            useBias         `bool`  - whether the layer uses bias.

            usePeephole     `bool`  -  whether the layer uses Peephole.
            
            dropout         `float` - The keep probability of dropout during training.

            recurrentDropout    `float` - The keep probability of recurrentDropout during training.

            memoryCellClip  `float` - If provided the memory cell is clipped by this value prior to the shadow gate activation.

            shadowStateClip `float` - If provided the new shadow State is clipped by this value .

            statefull       `bool`  -  whether the last state for each sample at index i in a batch will be used as initial state for the sample of index i in the next batch.

            goBackwards     `bool`  - whether input sequence enter backwards.

            returnSequenceLen    `int` - The sequence length of output.(only for rl LSTM Layer)

            returnSequences  `bool`  - Whether to only return the last time step output , or the whole output sequence.(only for keras LSTM Layer)

            returnState     `bool`  - Whether to return the last states and output, The returned elements of the last states list are the memory cell and the shadow State.

            outputConfig    `OutputConfig`      - Output configurations.

        '''
        if linearTransform is None:
            linearTransform = LSTMConfig.createBasicConfig()
        super().__init__(ModelNode.Layer.Types.LSTM, name = name, layerUnits = layerUnits, incomingConfig = incomingConfig, linearTransform = linearTransform,
                    batchNorm = batchNorm, outputConfig = outputConfig)
        self.isKerasLayer = isKerasLayer
        if self.isKerasLayer:
            if (inputGateActivation != forgetGateActivation)or(inputGateActivation != outputGateActivation)or(forgetGateActivation != outputGateActivation):
                raise ValueError('The recurrent_activation of keras LSTM layer is wrong')
            if memoryActivation != shadowGateActivation:
                raise ValueError('The activation of keras LSTM layer is wrong')

        self.inputGateActivation = inputGateActivation
        self.forgetGateActivation = forgetGateActivation
        self.outputGateActivation = outputGateActivation
        self.memoryActivation = memoryActivation
        self.shadowGateActivation = shadowGateActivation
        self.useBias = useBias
        self.usePeephole = usePeephole
        self.dropout = dropout
        self.recurrentDropout = recurrentDropout
        self.memoryCellClip = memoryCellClip
        self.shadowStateClip = shadowStateClip
        self.statefull = statefull
        self.goBackwards = goBackwards
        self.returnSequenceLen = returnSequenceLen
        self.returnSequences = returnSequences
        self.returnState = returnState

    def copy(self,name):
        '''
			Copy this layer profile.   --- UPDATED (CYK) 20190818

            Parameters
            ------------------------------

            name    `str`   - The new name of the copied layer
        '''
        # Ensure this is not a subclass.
        if (self.__class__.__name__ != "LSTMLayer"):
            raise ValueError("This layer class (" + self.__class__.__name__ + ") has not supported for copying.")
        
        # Create a new Layer Profile on this.
        return LSTMLayer(name=name, layerUnits =  self.layerUnits, incomingConfig = self.incomingConfig,
                    isKerasLayer = self.isKerasLayer, linearTransform = self.linearTransform, inputGateActivation = self.inputGateActivation,
                    forgetGateActivation = self.forgetGateActivation, outputGateActivation = self.outputGateActivation,
                    memoryActivation = self.memoryActivation, shadowGateActivation = self.shadowGateActivation, batchNorm = self.batchNorm, 
                    useBias = self.useBias, usePeephole = self.usePeephole, dropout = self.dropout, recurrentDropout = self.recurrentDropout,
                    memoryCellClip = self.memoryCellClip, shadowStateClip = self.shadowStateClip, statefull = self.statefull, goBackwards = self.goBackwards,
                    returnSequenceLen = self.returnSequenceLen, returnSequences = self.returnSequences, returnState = self.returnState, outputConfig = self.outputConfig)
    
    def keras_layer_build(self):
        if self.isKerasLayer:
            return 'self.%s = tf.keras.layers.LSTM(%d)'%(self.name, self.layerUnits)
        else:
            return 'self.%s = LSTM(%d)'%(self.name, self.layerUnits)

    def _activationToKerasName(self,activation):
        if activation == Train.Activation.Relu6 or activation == Train.Activation.Crelu:
            act = 'relu'
        elif activation == Train.Activation.HardSigmoid:
            act = 'hard_sigmoid'
        else:
            act = (Train.Activation.getName(activation)).split('.')[-1].lower()
        return act

    def keras_call_layer_build(self, reshape_output_name, output_reg, inputshape = None):
        new_shape = [None,self.returnSequenceLen,self.layerUnits]
        self.dropout = 1 - self.dropout / 100.
        self.recurrentDropout = 1 - self.recurrentDropout / 100.
        if reshape_output_name != output_reg:
            A = output_reg
            B = reshape_output_name
        else:
            A = output_reg
            B = output_reg

        layer_list, call_list = [],[]
        
        u_b = 'True' if self.useBias else 'False'
        d = self.dropout
        re_d = self.recurrentDropout
        r_st = 'True' if self.returnState else 'False'
        g_b = 'True' if self.goBackwards else 'False'
        sf = 'True' if self.statefull else 'False'

        if self.isKerasLayer:
            #act = self._activationToKerasName(self.inputGateActivation)
            act = self.act_dict[int(self.inputGateActivation)]
            #re_act = self._activationToKerasName(self.memoryActivation)
            re_act = self.act_dict[int(self.memoryActivation)]
            w_init = self.get_initiallizer(key = 'inputGateWeightConfig')
            re_w_init = self.get_initiallizer(key = 'inputGateRecurrentWeightConfig')
            b_init = self.get_initiallizer(key = 'inputGateBiasConfig')
            w_reg = self.get_regularizer(key = 'inputGateWeightConfig')
            re_w_reg = self.get_regularizer(key = 'inputGateRecurrentWeightConfig')
            b_reg = self.get_regularizer(key = 'inputGateBiasConfig')
            r_se = 'True' if self.returnSequences else 'False'
            layer = 'self.%s = tf.keras.layers.LSTM(%d, activation = %s,recurrent_activation = %s, use_bias=%s,  kernel_initializer=%s, recurrent_initializer=%s, bias_initializer=%s,'\
                 'kernel_regularizer=%s, recurrent_regularizer=%s, bias_regularizer=%s, dropout=%f, recurrent_dropout=%f, return_sequences=%s, return_state=%s,'\
                 'go_backwards=%s, stateful=%s )'% (self.name, self.layerUnits, act, re_act, u_b, w_init, re_w_init, b_init, w_reg, re_w_reg, b_reg, d, re_d, r_se, r_st, g_b, sf)

        else:
            #in_act = self._activationToKerasName(self.inputGateActivation)
            in_act = self.act_dict[int(self.inputGateActivation)]
            #for_act = self._activationToKerasName(self.forgetGateActivation)
            for_act = self.act_dict[int(self.forgetGateActivation)]
            #out_act = self._activationToKerasName(self.outputGateActivation)
            out_act = self.act_dict[int(self.outputGateActivation)]
            #mem_act = self._activationToKerasName(self.memoryActivation)
            mem_act = self.act_dict[int(self.memoryActivation)]
            #shadow_act = self._activationToKerasName(self.shadowGateActivation)
            shadow_act = act = self.act_dict[int(self.shadowGateActivation)]
            in_w_init = self.get_initiallizer(key = 'inputGateWeightConfig')
            for_w_init = self.get_initiallizer(key = 'forgetGateWeightConfig')
            out_w_init = self.get_initiallizer(key = 'outputGateWeightConfig')
            mem_w_init = self.get_initiallizer(key = 'memoryWeightConfig')
            in_re_w_init = self.get_initiallizer(key = 'inputGateRecurrentWeightConfig')
            for_re_w_init = self.get_initiallizer(key = 'forgetGateRecurrentWeightConfig')
            out_re_w_init = self.get_initiallizer(key = 'outputGateRecurrentWeightConfig')
            mem_re_w_init = self.get_initiallizer(key = 'memoryRecurrentWeightConfig')
            in_peep_w_init = self.get_initiallizer(key = 'inputGatePeepholeWeightConfig')
            for_peep_w_init = self.get_initiallizer(key = 'forgetGatePeepholeWeightConfig')
            out_peep_w_init = self.get_initiallizer(key = 'outputGatePeepholeWeightConfig')
            in_b_init = self.get_initiallizer(key = 'inputGateBiasConfig')
            for_b_init = self.get_initiallizer(key = 'forgetGateBiasConfig')
            out_b_init = self.get_initiallizer(key = 'outputGateBiasConfig')
            mem_b_init = self.get_initiallizer(key = 'memoryBiasConfig')
            in_w_reg = self.get_regularizer(key = 'inputGateWeightConfig')
            for_w_reg = self.get_regularizer(key = 'forgetGateWeightConfig')
            out_w_reg = self.get_regularizer(key = 'outputGateWeightConfig')
            mem_w_reg = self.get_regularizer(key = 'memoryWeightConfig')
            in_re_w_reg = self.get_regularizer(key = 'inputGateRecurrentWeightConfig')
            for_re_w_reg = self.get_regularizer(key = 'forgetGateRecurrentWeightConfig')
            out_re_w_reg = self.get_regularizer(key = 'outputGateRecurrentWeightConfig')
            mem_re_w_reg = self.get_regularizer(key = 'memoryRecurrentWeightConfig')
            in_peep_w_reg = self.get_regularizer(key = 'inputGatePeepholeWeightConfig')
            for_peep_w_reg = self.get_regularizer(key = 'forgetGatePeepholeWeightConfig')
            out_peep_w_reg = self.get_regularizer(key = 'outputGatePeepholeWeightConfig')
            in_b_reg = self.get_regularizer(key = 'inputGateBiasConfig')
            for_b_reg = self.get_regularizer(key = 'forgetGateBiasConfig')
            out_b_reg = self.get_regularizer(key = 'outputGateBiasConfig')
            mem_b_reg = self.get_regularizer(key = 'memoryBiasConfig')    
            u_p = 'True' if self.usePeephole else 'False'
            mc_c = 'None' if self.memoryCellClip is None else str(self.memoryCellClip)
            ss_c = 'None' if self.shadowStateClip is None else str(self.shadowStateClip)
            r_se_l = 'None' if self.returnSequenceLen is None else str(self.returnSequenceLen)
            layer = 'self.%s = LSTM(%d, activation = LSTM.Activation(%s,%s,%s,%s,%s),kernelInitializer = LSTM.Initializer(%s,%s,%s,%s),'\
                'recurrentInitializer = LSTM.Initializer(%s,%s,%s,%s),peepholeInitializer = LSTM.Initializer(%s,%s,%s,None),biasInitializer = LSTM.Initializer(%s,%s,%s,%s),'\
                'kernelRegularizer = LSTM.Regularizer(%s,%s,%s,%s),recurrentRegularizer = LSTM.Regularizer(%s,%s,%s,%s),peepholeRegularizer = LSTM.Regularizer(%s,%s,%s,None),'\
                'biasRegularizer = LSTM.Regularizer(%s,%s,%s,%s),useBias = %s,usePeephole = %s,dropout =%f,recurrentDropout =%f,'\
                'memoryCellClip = %s, shadowStateClip = %s, statefull = %s, goBackwards = %s, returnSequenceLen = %s, returnState = %s)'% \
                (self.name, self.layerUnits, in_act, for_act, out_act, mem_act, shadow_act, in_w_init, for_w_init, out_w_init, mem_w_init,
                    in_re_w_init, for_re_w_init, out_re_w_init, mem_re_w_init, in_peep_w_init, for_peep_w_init, out_peep_w_init, 
                    in_b_init, for_b_init, out_b_init, mem_b_init, in_w_reg, for_w_reg, out_w_reg, mem_w_reg,
                    in_re_w_reg, for_re_w_reg, out_re_w_reg, mem_re_w_reg, in_peep_w_reg, for_peep_w_reg, out_peep_w_reg,
                    in_b_reg, for_b_reg, out_b_reg, mem_b_reg, u_b, u_p, d, re_d, mc_c, ss_c, sf, g_b, r_se_l, r_st)
        layer_list.append(layer)
        
        #if self.refLayerName is not None:
        #    raise ValueError('LSTMLayer can\'t mirrors for refernced referenced kernal from ohter layer' )

        call = '%s = self.%s(%s)'% (A,self.name,B)
        call_list.append(call)
        
        if self.batchNorm:
            layer_list.append('self.%s_bn = BatchNormalization()'%(self.name))
            call_list.append('%s = self.%s_bn(%s, training=training)'%(A,self.name,A))
            return layer_list, call_list, output_reg, new_shape
        else:
            return layer_list, call_list, output_reg, new_shape
    
    def keras_call_auto_reshape_build(self, input_shape, combine_output_name, output_reg):
        layer_obj = []
        call_obj = []

        if combine_output_name != output_reg:
            A = output_reg
            B = combine_output_name
        else:
            A = output_reg
            B = output_reg
        output_name = output_reg

        if len(input_shape) != 3:
            if len(input_shape) > 3 :
                if len(input_shape) < 5:
                    new_shape = input_shape[0:2]
                    new_shape.append(-1)
                    string = 'self.%s_auto_reshape = tf.keras.layers.Reshape(('%(self.name)
                    string += ','.join([str(int(x)) for x in new_shape[1:]])
                    string += '))'
                    layer_obj.append(string)
                    string = '%s = self.%s_auto_reshape(%s)'%(A, self.name, B)
                    call_obj.append(string)
                else:
                    raise ValueError("When dimension of data is more than 4, it is not supported.")
            elif len(input_shape) == 2 :
                dim = input_shape[-1]
                new_shape = input_shape[0:1]
                new_shape.append(1)
                new_shape.append(dim)
                string = 'self.%s_auto_reshape = tf.keras.layers.Reshape(('%(self.name)
                string += ','.join(['1',str(int(dim))])
                string += '))'
                layer_obj.append(string)
                string = '%s = self.%s_auto_reshape(%s)'%(A, self.name, B)
                call_obj.append(string)
            else:
                raise ValueError("One dimension data is not supported.")
        else:
            new_shape = input_shape
            output_name = combine_output_name
        return layer_obj, call_obj, output_name, new_shape

class BiRNNLayer(_ModelNodeLayerConfig):
    '''
			Class representing a bidirectional RNN layer.   --- UPDATED (CYK) 20191101
    '''
        
    def __init__(self,name: str = None, layerUnits: int = 150, incomingConfig: 'IncomingConfig' = _ModelNodeLayerIncoming.Concat(),
                    isKerasLayer: bool = False, 
                    linearTransform: 'RNNConfig' = RNNConfig.createBasicConfig(),
                    activation: int = Train.Activation.Tanh, activationParams: Dict = {},
                    batchNorm: bool = True, useBias: bool = True, dropout: float = 1, recurrentDropout: float = 1,
                    stateClip: float = None, statefull: bool = False,  mergeMode: str = 'concat',
                    outputConfig: 'ModelNode.Layer.Output.Config' = _ModelNodeLayerOutput.Default()):
        '''
			Create a RNN layer.   --- UPDATED (CYK) 20200117

            Parameters
            ------------------------------

            name            `str`   - The name of this layer.
            
            layerUnits      `int`   - The number of hidden units in this layer.

            incomingConfig  `IncomingConfig`    - Input configurations.

            isKerasLayer    `bool`  - Whether to use tf.Layer.keras or not.

            linearTransform `RNNConfig` - The RNN configuration.

            activation      `int`   - The activation function of this layer.
            
            activationParams    `dict{str:*}`   - Activation parameters as defined in TensorFlow.
            
            batchNorm       `bool`  - Whether to use batch normalization before activation function.

            useBias         `bool`  - whether the layer uses bias.
            
            dropout         `float` - The keep probability of dropout during training.

            recurrentDropout    `float` - The keep probability of recurrentDropout during training.

            stateClip       `float` - If provided the new state is clipped by this value .

            statefull       `bool`  -  whether the last state for each sample at index i in a batch will be used as initial state for the sample of index i in the next batch.

            mergeMode       `str`   - the mode of forwards and backward RNNs outputs will be merge.

            outputConfig    `OutputConfig`      - Output configurations.

        '''
        if linearTransform is None:
            linearTransform = RNNConfig.createBasicConfig()
        super().__init__(ModelNode.Layer.Types.BiRNN, name = name, layerUnits = layerUnits, incomingConfig = incomingConfig, linearTransform = linearTransform,
                    batchNorm = batchNorm, outputConfig = outputConfig)
        self.isKerasLayer = isKerasLayer
        self.activation =  activation
        self.activationParams = activationParams
        self.useBias = useBias
        self.dropout = dropout
        self.recurrentDropout = recurrentDropout
        self.stateClip = stateClip
        self.statefull = statefull
        self.mergeMode = mergeMode
     
    def copy(self,name):
        '''
			Copy this layer profile.   --- UPDATED (CYK) 20191101

            Parameters
            ------------------------------

            name    `str`   - The new name of the copied layer
        '''
        # Ensure this is not a subclass.
        if (self.__class__.__name__ != "BiRNNLayer"):
            raise ValueError("This layer class (" + self.__class__.__name__ + ") has not supported for copying.")
        
        # Create a new Layer Profile on this.
        return BiRNNLayer(name=name, layerUnits =  self.layerUnits, incomingConfig = self.incomingConfig,
                    isKerasLayer = self.isKerasLayer, linearTransform = self.linearTransform, activation = self.activation, activationParams = self.activationParams,
                    batchNorm = self.batchNorm, useBias = self.useBias, dropout = self.dropout, recurrentDropout = self.recurrentDropout,
                    stateClip = self.stateClip, statefull = self.statefull, mergeMode = self.mergeMode, outputConfig = self.outputConfig)
    
    def keras_layer_build(self):
        if self.isKerasLayer:
            return 'self.%s = tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(%d, return_sequences=True))'%(self.name, self.layerUnits)
        else:
            return 'self.%s = BiRNN(%d)'%(self.name, self.layerUnits)

    def _activationToKerasName(self,activation):
        if activation == Train.Activation.Relu6 or activation == Train.Activation.Crelu:
            act = 'relu'
        elif activation == Train.Activation.HardSigmoid:
            act = 'hard_sigmoid'
        else:
            act = (Train.Activation.getName(activation)).split('.')[-1].lower()
        return act

    def keras_call_layer_build(self, reshape_output_name, output_reg, inputshape = None):
        if self.mergeMode == 'concat':
            new_shape = [None,inputshape[1],self.layerUnits*2]
        else:
            new_shape = [None,inputshape[1],self.layerUnits]
        # dropout
        self.dropout = 1 - self.dropout / 100.
        self.recurrentDropout = 1 - self.recurrentDropout / 100.

        if reshape_output_name != output_reg:
            A = output_reg
            B = reshape_output_name
        else:
            A = output_reg
            B = output_reg

        layer_list, call_list = [],[]
        

        #act = self._activationToKerasName(self.activation)
        act = self.act_dict[self.activation.value]
        u_b = 'True' if self.useBias else 'False'
        w_init = self.get_initiallizer(key = 'weightConfig')
        re_w_init = self.get_initiallizer(key = 'recurrentWeightConfig')
        b_init = self.get_initiallizer(key = 'biasConfig')
        w_reg = self.get_regularizer(key = 'weightConfig')
        re_w_reg = self.get_regularizer(key = 'recurrentWeightConfig')
        b_reg = self.get_regularizer(key = 'biasConfig')
        d = self.dropout
        re_d = self.recurrentDropout
        sf = 'True' if self.statefull else 'False'
        mm = self.mergeMode

        if self.isKerasLayer:
            r_se = 'True'
            
            layer = 'self.%s =  tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(%d, activation = %s, use_bias=%s,  kernel_initializer=%s, recurrent_initializer=%s, bias_initializer=%s,'\
                 'kernel_regularizer=%s, recurrent_regularizer=%s, bias_regularizer=%s, dropout=%f, recurrent_dropout=%f, return_sequences=%s, return_state=%s,'\
                 'stateful=%s ),merge_mode=\'%s\')'% (self.name, self.layerUnits, act, u_b, w_init, re_w_init, b_init, w_reg, re_w_reg, b_reg, d, re_d, sf, mm)
        else:
            s_c = 'None' if self.stateClip is None else str(self.stateClip)

            layer = 'self.%s = BiRNN(%d, activation = %s,kernelInitializer = %s,recurrentInitializer = %s,biasInitializer = %s,'\
                'kernelRegularizer = %s,recurrentRegularizer = %s,biasRegularizer = %s,useBias = %s,dropout =%f,recurrentDropout =%f,'\
                'stateClip = %s, statefull = %s, mergeMode = \'%s\')'% \
                (self.name, self.layerUnits, act, w_init, re_w_init, b_init, w_reg ,re_w_reg,b_reg, u_b, d, re_d, s_c, sf, mm)
        
        layer_list.append(layer)
        
        #if self.refLayerName is not None:
        #    raise ValueError('RNNLayer can\'t mirrors for refernced referenced kernal from ohter layer' )

        call = '%s = self.%s(%s)'% (A,self.name,B)
        call_list.append(call)
        
        if self.batchNorm:
            layer_list.append('self.%s_bn = BatchNormalization()'%(self.name))
            call_list.append('%s = self.%s_bn(%s, training=training)'%(A,self.name,A))
            return layer_list, call_list, output_reg, new_shape
        else:
            return layer_list, call_list, output_reg, new_shape
    
    def keras_call_auto_reshape_build(self, input_shape, combine_output_name, output_reg):
        layer_obj = []
        call_obj = []

        if combine_output_name != output_reg:
            A = output_reg
            B = combine_output_name
        else:
            A = output_reg
            B = output_reg
        output_name = output_reg

        if len(input_shape) != 3:
            if len(input_shape) > 3 :
                if len(input_shape) < 5:
                    new_shape = input_shape[0:2]
                    new_shape.append(-1)
                    string = 'self.%s_auto_reshape = tf.keras.layers.Reshape(('%(self.name)
                    string += ','.join([str(int(x)) for x in new_shape[1:]])
                    string += '))'
                    layer_obj.append(string)
                    string = '%s = self.%s_auto_reshape(%s)'%(A, self.name, B)
                    call_obj.append(string)
                else:
                    raise ValueError("When dimension of data is more than 4, it is not supported.")
            elif len(input_shape) == 2 :
                dim = input_shape[-1]
                new_shape = input_shape[0:1]
                new_shape.append(1)
                new_shape.append(dim)
                string = 'self.%s_auto_reshape = tf.keras.layers.Reshape(('%(self.name)
                string += ','.join(['1',str(int(dim))])
                string += '))'
                layer_obj.append(string)
                string = '%s = self.%s_auto_reshape(%s)'%(A, self.name, B)
                call_obj.append(string)
            else:
                raise ValueError("One dimension data is not supported.")
        else:
            new_shape = input_shape
            output_name = combine_output_name
        return layer_obj, call_obj, output_name, new_shape

class BiGRULayer(_ModelNodeLayerConfig):
    '''
			Class representing a bidirectional GRU layer.   --- UPDATED (CYK) 20191101
    '''       
    def __init__(self,name: str = None, layerUnits: int = 150, incomingConfig: 'IncomingConfig' = _ModelNodeLayerIncoming.Concat(),
                    isKerasLayer: bool = False, 
                    linearTransform: 'GRUConfig' = GRUConfig.createBasicConfig(),
                    resetGateActivation: int = Train.Activation.Sigmoid,updateGateActivation: int = Train.Activation.Sigmoid,stateCandidateActivation: int = Train.Activation.Tanh,
                    batchNorm: bool = True, useBias: bool = True, dropout: float = 1, recurrentDropout: float = 1,
                    stateClip: float = None, statefull: bool = False,  mergeMode: str = 'concat',outputConfig: 'ModelNode.Layer.Output.Config' = _ModelNodeLayerOutput.Default()):
        '''
			Create a LSTM layer.   --- UPDATED (CYK) 20200117

            Parameters
            ------------------------------

            name            `str`   - The name of this layer.
            
            layerUnits      `int`   - The number of hidden units in this layer.

            incomingConfig  `IncomingConfig`    - Input configurations.

            isKerasLayer    `bool`  - Whether to use tf.Layer.keras or not.

            linearTransform `GRUConfig` - The GRU configuration.

            resetGateActivation       `int`   - The activation function of resetGate of this layer.

            updateGateActivation      `int`   - The activation function of updateGate of this layer.

            stateCandidateActivation      `int`   - The activation function of stateCandidate of this layer.
            
            batchNorm       `bool`  - Whether to use batch normalization before activation function.

            useBias         `bool`  - whether the layer uses bias.

            usePeephole     `bool`  -  whether the layer uses Peephole.
            
            dropout         `float` - The keep probability of dropout during training.

            recurrentDropout    `float` - The keep probability of recurrentDropout during training.

            stateClip `float` - If provided the new state is clipped by this value .

            statefull       `bool`  -  whether the last state for each sample at index i in a batch will be used as initial state for the sample of index i in the next batch.

            mergeMode       `str`   - the mode of forwards and backward GRUs outputs will be merge.

            outputConfig    `OutputConfig`      - Output configurations.

        '''
        if linearTransform is None:
            linearTransform = GRUConfig.createBasicConfig()
        super().__init__(ModelNode.Layer.Types.BiGRU, name = name, layerUnits = layerUnits, incomingConfig = incomingConfig, linearTransform = linearTransform,
                    batchNorm = batchNorm, outputConfig = outputConfig)
        self.isKerasLayer = isKerasLayer
        if self.isKerasLayer:
            if (resetGateActivation != updateGateActivation):
                raise ValueError('The recurrent_activation of keras GRU layer is wrong')

        self.resetGateActivation = resetGateActivation
        self.updateGateActivation = updateGateActivation
        self.stateCandidateActivation = stateCandidateActivation
        self.useBias = useBias
        self.dropout = dropout
        self.recurrentDropout = recurrentDropout
        self.stateClip = stateClip
        self.statefull = statefull
        self.mergeMode = mergeMode

    def copy(self,name):
        '''
			Copy this layer profile.   --- UPDATED (CYK) 20191101

            Parameters
            ------------------------------

            name    `str`   - The new name of the copied layer
        '''
        # Ensure this is not a subclass.
        if (self.__class__.__name__ != "BiGRULayer"):
            raise ValueError("This layer class (" + self.__class__.__name__ + ") has not supported for copying.")
        
        # Create a new Layer Profile on this.
        return BiGRULayer(name=name, layerUnits =  self.layerUnits, incomingConfig = self.incomingConfig,
                    isKerasLayer = self.isKerasLayer, linearTransform = self.linearTransform, resetGateActivation = self.resetGateActivation,
                    updateGateActivation = self.updateGateActivation, stateCandidateActivation = self.stateCandidateActivation,
                    batchNorm = self.batchNorm, useBias = self.useBias, dropout = self.dropout, recurrentDropout = self.recurrentDropout,
                    stateClip = self.stateClip, statefull = self.statefull, mergeMode = self.mergeMode, outputConfig = self.outputConfig)
    
    def keras_layer_build(self):
        if self.isKerasLayer:
            return 'self.%s = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(%d, return_sequences=True))'%(self.name, self.layerUnits)
        else:
            return 'self.%s = BiGRU(%d)'%(self.name, self.layerUnits)

    def _activationToKerasName(self,activation):
        if activation == Train.Activation.Relu6 or activation == Train.Activation.Crelu:
            act = 'relu'
        elif activation == Train.Activation.HardSigmoid:
            act = 'hard_sigmoid'
        else:
            act = (Train.Activation.getName(activation)).split('.')[-1].lower()
        return act

    def keras_call_layer_build(self, reshape_output_name, output_reg, inputshape = None):
        if self.mergeMode == 'concat':
            new_shape = [None,inputshape[1],self.layerUnits*2]
        else:
            new_shape = [None,inputshape[1],self.layerUnits]
        self.dropout = 1 - self.dropout / 100.
        self.recurrentDropout = 1 - self.recurrentDropout / 100.
        if reshape_output_name != output_reg:
            A = output_reg
            B = reshape_output_name
        else:
            A = output_reg
            B = output_reg

        layer_list, call_list = [],[]
        
        u_b = 'True' if self.useBias else 'False'
        d = self.dropout
        re_d = self.recurrentDropout
        sf = 'True' if self.statefull else 'False'
        mm = self.mergeMode

        if self.isKerasLayer:
            act = self.act_dict[int(self.stateCandidateActivation)]
            re_act = self.act_dict[int(self.resetGateActivation)]
            w_init = self.get_initiallizer(key = 'resetGateWeightConfig')
            re_w_init = self.get_initiallizer(key = 'resetGateRecurrentWeightConfig')
            b_init = self.get_initiallizer(key = 'resetGateBiasConfig')
            w_reg = self.get_regularizer(key = 'resetGateWeightConfig')
            re_w_reg = self.get_regularizer(key = 'resetGateRecurrentWeightConfig')
            b_reg = self.get_regularizer(key = 'resetGateBiasConfig')
            r_se = 'True' 
            layer = 'self.%s = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(%d, activation = %s,recurrent_activation = %s, use_bias=%s,  kernel_initializer=%s, recurrent_initializer=%s, bias_initializer=%s,'\
                 'kernel_regularizer=%s, recurrent_regularizer=%s, bias_regularizer=%s, dropout=%f, recurrent_dropout=%f, return_sequences=%s,'\
                 'stateful=%s ),merge_mode=\'%s\')'% (self.name, self.layerUnits, act, re_act, u_b, w_init, re_w_init, b_init, w_reg, re_w_reg, b_reg, d, re_d, r_se, sf, mm)

        else:
            #res_act = self._activationToKerasName(self.resetGateActivation)
            res_act = self.act_dict[int(self.resetGateActivation)]
            #up_act = self._activationToKerasName(self.updateGateActivation)
            up_act = self.act_dict[int(self.updateGateActivation)]
            #sta_act = self._activationToKerasName(self.stateCandidateActivation)
            sta_act = self.act_dict[int(self.stateCandidateActivation)]
            res_w_init = self.get_initiallizer(key = 'resetGateWeightConfig')
            up_w_init = self.get_initiallizer(key = 'updateGateWeightConfig')
            sta_w_init = self.get_initiallizer(key = 'stateCandidateWeightConfig')
            res_re_w_init = self.get_initiallizer(key = 'resetGateRecurrentWeightConfig')
            up_re_w_init = self.get_initiallizer(key = 'updateGateRecurrentWeightConfig')
            sta_re_w_init = self.get_initiallizer(key = 'stateCandidateRecurrentWeightConfig')
            res_b_init = self.get_initiallizer(key = 'resetGateBiasConfig')
            up_b_init = self.get_initiallizer(key = 'updateGateBiasConfig')
            sta_b_init = self.get_initiallizer(key = 'stateCandidateBiasConfig')
            res_w_reg = self.get_regularizer(key = 'resetGateWeightConfig')
            up_w_reg = self.get_regularizer(key = 'updateGateWeightConfig')
            sta_w_reg = self.get_regularizer(key = 'stateCandidateWeightConfig')
            res_re_w_reg = self.get_regularizer(key = 'resetGateRecurrentWeightConfig')
            up_re_w_reg = self.get_regularizer(key = 'updateGateRecurrentWeightConfig')
            sta_re_w_reg = self.get_regularizer(key = 'stateCandidateRecurrentWeightConfig')
            res_b_reg = self.get_regularizer(key = 'resetGateBiasConfig')
            up_b_reg = self.get_regularizer(key = 'updateGateBiasConfig')
            sta_b_reg = self.get_regularizer(key = 'stateCandidateBiasConfig') 
            sta_c = 'None' if self.stateClip is None else str(self.stateClip)
            layer = 'self.%s = BiGRU(%d, activation = BiGRU.Activation(%s,%s,%s),kernelInitializer = BiGRU.Initializer(%s,%s,%s),'\
                'recurrentInitializer = BiGRU.Initializer(%s,%s,%s),biasInitializer = BiGRU.Initializer(%s,%s,%s),'\
                'kernelRegularizer = BiGRU.Regularizer(%s,%s,%s),recurrentRegularizer = BiGRU.Regularizer(%s,%s,%s),'\
                'biasRegularizer = BiGRU.Regularizer(%s,%s,%s),useBias = %s,dropout =%f,recurrentDropout =%f,'\
                'stateClip = %s, statefull = %s, mergeMode = \'%s\')'% \
                (self.name, self.layerUnits, res_act, up_act, sta_act, res_w_init, up_w_init, sta_w_init,
                    res_re_w_init, up_re_w_init, sta_re_w_init, 
                    res_b_init, up_b_init, sta_b_init, res_w_reg, up_w_reg, sta_w_reg,
                    res_re_w_reg, up_re_w_reg, sta_re_w_reg,
                    res_b_reg, up_b_reg, sta_b_reg, u_b, d, re_d, sta_c, sf, mm)
        layer_list.append(layer)
        
        #if self.refLayerName is not None:
        #    raise ValueError('GRULayer can\'t mirrors for refernced referenced kernal from ohter layer' )

        call = '%s = self.%s(%s)'% (A,self.name,B)
        call_list.append(call)
        
        if self.batchNorm:
            layer_list.append('self.%s_bn = BatchNormalization()'%(self.name))
            call_list.append('%s = self.%s_bn(%s, training=training)'%(A,self.name,A))
            return layer_list, call_list, output_reg, new_shape
        else:
            return layer_list, call_list, output_reg, new_shape
    
    def keras_call_auto_reshape_build(self, input_shape, combine_output_name, output_reg):
        layer_obj = []
        call_obj = []

        if combine_output_name != output_reg:
            A = output_reg
            B = combine_output_name
        else:
            A = output_reg
            B = output_reg
        output_name = output_reg

        if len(input_shape) != 3:
            if len(input_shape) > 3 :
                if len(input_shape) < 5:
                    new_shape = input_shape[0:2]
                    new_shape.append(-1)
                    string = 'self.%s_auto_reshape = tf.keras.layers.Reshape(('%(self.name)
                    string += ','.join([str(int(x)) for x in new_shape[1:]])
                    string += '))'
                    layer_obj.append(string)
                    string = '%s = self.%s_auto_reshape(%s)'%(A, self.name, B)
                    call_obj.append(string)
                else:
                    raise ValueError("When dimension of data is more than 4, it is not supported.")
            elif len(input_shape) == 2 :
                dim = input_shape[-1]
                new_shape = input_shape[0:1]
                new_shape.append(1)
                new_shape.append(dim)
                string = 'self.%s_auto_reshape = tf.keras.layers.Reshape(('%(self.name)
                string += ','.join(['1',str(int(dim))])
                string += '))'
                layer_obj.append(string)
                string = '%s = self.%s_auto_reshape(%s)'%(A, self.name, B)
                call_obj.append(string)
            else:
                raise ValueError("One dimension data is not supported.")
        else:
            new_shape = input_shape
            output_name = combine_output_name
        return layer_obj, call_obj, output_name, new_shape

class BiLSTMLayer(_ModelNodeLayerConfig):
    '''
			Class representing a bidirectional LSTM layer.   --- UPDATED (CYK) 20191101
    '''       
    def __init__(self,name: str = None, layerUnits: int = 150, incomingConfig: 'IncomingConfig' = _ModelNodeLayerIncoming.Concat(),
                    isKerasLayer: bool = False, 
                    linearTransform: 'LSTMConfig' = LSTMConfig.createBasicConfig(),
                    inputGateActivation: int = Train.Activation.Sigmoid,forgetGateActivation: int = Train.Activation.Sigmoid,outputGateActivation: int = Train.Activation.Sigmoid,
                    memoryActivation: int = Train.Activation.Tanh,shadowGateActivation: int = Train.Activation.Tanh,
                    batchNorm: bool = True, useBias: bool = True, usePeephole: bool = False, dropout: float = 1, recurrentDropout: float = 1,
                    memoryCellClip: float = None, shadowStateClip: float = None, statefull: bool = False, mergeMode: str = 'concat',
                    outputConfig: 'ModelNode.Layer.Output.Config' = _ModelNodeLayerOutput.Default()):
        '''
			Create a LSTM layer.   --- UPDATED (CYK) 20200117

            Parameters
            ------------------------------

            name            `str`   - The name of this layer.
            
            layerUnits      `int`   - The number of hidden units in this layer.

            incomingConfig  `IncomingConfig`    - Input configurations.

            isKerasLayer    `bool`  - Whether to use tf.Layer.keras or not.

            linearTransform `LSTMConfig` - The LSTM configuration.

            inputGateActivation       `int`   - The activation function of inputGate of this layer.

            forgetGateActivation      `int`   - The activation function of forgetGate of this layer.

            outputGateActivation      `int`   - The activation function of outputGate of this layer.

            memoryActivation          `int`   - The activation function of memory of this layer.

            shadowGateActivation      `int`   - The activation function of shadowGate of this layer.
            
            batchNorm       `bool`  - Whether to use batch normalization before activation function.

            useBias         `bool`  - whether the layer uses bias.

            usePeephole     `bool`  -  whether the layer uses Peephole.
            
            dropout         `float` - The keep probability of dropout during training.

            recurrentDropout    `float` - The keep probability of recurrentDropout during training.

            memoryCellClip  `float` - If provided the memory cell is clipped by this value prior to the shadow gate activation.

            shadowStateClip `float` - If provided the new shadow State is clipped by this value .

            statefull       `bool`  -  whether the last state for each sample at index i in a batch will be used as initial state for the sample of index i in the next batch.

            mergeMode       `str`   - the mode of forwards and backward LSTMs outputs will be merge.

            outputConfig    `OutputConfig`      - Output configurations.

        '''
        if linearTransform is None:
            linearTransform = LSTMConfig.createBasicConfig()
        super().__init__(ModelNode.Layer.Types.BiLSTM, name = name, layerUnits = layerUnits, incomingConfig = incomingConfig, linearTransform = linearTransform,
                    batchNorm = batchNorm, outputConfig = outputConfig)
        self.isKerasLayer = isKerasLayer
        if self.isKerasLayer:
            if (inputGateActivation != forgetGateActivation)or(inputGateActivation != outputGateActivation)or(forgetGateActivation != outputGateActivation):
                raise ValueError('The recurrent_activation of keras LSTM layer is wrong')
            if memoryActivation != shadowGateActivation:
                raise ValueError('The activation of keras LSTM layer is wrong')

        self.inputGateActivation = inputGateActivation
        self.forgetGateActivation = forgetGateActivation
        self.outputGateActivation = outputGateActivation
        self.memoryActivation = memoryActivation
        self.shadowGateActivation = shadowGateActivation
        self.useBias = useBias
        self.usePeephole = usePeephole
        self.dropout = dropout
        self.recurrentDropout = recurrentDropout
        self.memoryCellClip = memoryCellClip
        self.shadowStateClip = shadowStateClip
        self.statefull = statefull
        self.mergeMode = mergeMode

    def copy(self,name):
        '''
			Copy this layer profile.   --- UPDATED (CYK) 20191101

            Parameters
            ------------------------------

            name    `str`   - The new name of the copied layer
        '''
        # Ensure this is not a subclass.
        if (self.__class__.__name__ != "BiLSTMLayer"):
            raise ValueError("This layer class (" + self.__class__.__name__ + ") has not supported for copying.")
        
        # Create a new Layer Profile on this.
        return BiLSTMLayer(name=name, layerUnits =  self.layerUnits, incomingConfig = self.incomingConfig,
                    isKerasLayer = self.isKerasLayer, linearTransform = self.linearTransform, inputGateActivation = self.inputGateActivation,
                    forgetGateActivation = self.forgetGateActivation, outputGateActivation = self.outputGateActivation,
                    memoryActivation = self.memoryActivation, shadowGateActivation = self.shadowGateActivation, batchNorm = self.batchNorm, 
                    useBias = self.useBias, usePeephole = self.usePeephole, dropout = self.dropout, recurrentDropout = self.recurrentDropout,
                    memoryCellClip = self.memoryCellClip, shadowStateClip = self.shadowStateClip, statefull = self.statefull, mergeMode = self.mergeMode, outputConfig = self.outputConfig)
    
    def keras_layer_build(self):
        if self.isKerasLayer:
            return 'self.%s = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(%d, return_sequences=True))'%(self.name, self.layerUnits)
        else:
            return 'self.%s = BiLSTM(%d)'%(self.name, self.layerUnits)

    def _activationToKerasName(self,activation):
        if activation == Train.Activation.Relu6 or activation == Train.Activation.Crelu:
            act = 'relu'
        elif activation == Train.Activation.HardSigmoid:
            act = 'hard_sigmoid'
        else:
            act = (Train.Activation.getName(activation)).split('.')[-1].lower()
        return act

    def keras_call_layer_build(self, reshape_output_name, output_reg, inputshape = None):
        if self.mergeMode == 'concat':
            new_shape = [None,inputshape[1],self.layerUnits*2]
        else:
            new_shape = [None,inputshape[1],self.layerUnits]
        self.dropout = 1 - self.dropout / 100.
        self.recurrentDropout = 1 - self.recurrentDropout / 100.
        if reshape_output_name != output_reg:
            A = output_reg
            B = reshape_output_name
        else:
            A = output_reg
            B = output_reg

        layer_list, call_list = [],[]
        
        u_b = 'True' if self.useBias else 'False'
        d = self.dropout
        re_d = self.recurrentDropout
        sf = 'True' if self.statefull else 'False'
        mm = self.mergeMode

        if self.isKerasLayer:
            #act = self._activationToKerasName(self.inputGateActivation)
            act = self.act_dict[int(self.inputGateActivation)]
            #re_act = self._activationToKerasName(self.memoryActivation)
            re_act = self.act_dict[int(self.memoryActivation)]
            w_init = self.get_initiallizer(key = 'inputGateWeightConfig')
            re_w_init = self.get_initiallizer(key = 'inputGateRecurrentWeightConfig')
            b_init = self.get_initiallizer(key = 'inputGateBiasConfig')
            w_reg = self.get_regularizer(key = 'inputGateWeightConfig')
            re_w_reg = self.get_regularizer(key = 'inputGateRecurrentWeightConfig')
            b_reg = self.get_regularizer(key = 'inputGateBiasConfig')
            r_se = 'True'
            layer = 'self.%s = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(%d, activation = %s,recurrent_activation = %s, use_bias=%s,  kernel_initializer=%s, recurrent_initializer=%s, bias_initializer=%s,'\
                 'kernel_regularizer=%s, recurrent_regularizer=%s, bias_regularizer=%s, dropout=%f, recurrent_dropout=%f, return_sequences=%s,  stateful=%s ),merge_mode=\'%s\')'% (self.name, self.layerUnits, act, re_act, u_b, w_init, re_w_init, b_init, w_reg, re_w_reg, b_reg, d, re_d, r_se, sf,mm)

        else:
            #in_act = self._activationToKerasName(self.inputGateActivation)
            in_act = self.act_dict[int(self.inputGateActivation)]
            #for_act = self._activationToKerasName(self.forgetGateActivation)
            for_act = self.act_dict[int(self.forgetGateActivation)]
            #out_act = self._activationToKerasName(self.outputGateActivation)
            out_act = self.act_dict[int(self.outputGateActivation)]
            #mem_act = self._activationToKerasName(self.memoryActivation)
            mem_act = self.act_dict[int(self.memoryActivation)]
            #shadow_act = self._activationToKerasName(self.shadowGateActivation)
            shadow_act = act = self.act_dict[int(self.shadowGateActivation)]
            in_w_init = self.get_initiallizer(key = 'inputGateWeightConfig')
            for_w_init = self.get_initiallizer(key = 'forgetGateWeightConfig')
            out_w_init = self.get_initiallizer(key = 'outputGateWeightConfig')
            mem_w_init = self.get_initiallizer(key = 'memoryWeightConfig')
            in_re_w_init = self.get_initiallizer(key = 'inputGateRecurrentWeightConfig')
            for_re_w_init = self.get_initiallizer(key = 'forgetGateRecurrentWeightConfig')
            out_re_w_init = self.get_initiallizer(key = 'outputGateRecurrentWeightConfig')
            mem_re_w_init = self.get_initiallizer(key = 'memoryRecurrentWeightConfig')
            in_peep_w_init = self.get_initiallizer(key = 'inputGatePeepholeWeightConfig')
            for_peep_w_init = self.get_initiallizer(key = 'forgetGatePeepholeWeightConfig')
            out_peep_w_init = self.get_initiallizer(key = 'outputGatePeepholeWeightConfig')
            in_b_init = self.get_initiallizer(key = 'inputGateBiasConfig')
            for_b_init = self.get_initiallizer(key = 'forgetGateBiasConfig')
            out_b_init = self.get_initiallizer(key = 'outputGateBiasConfig')
            mem_b_init = self.get_initiallizer(key = 'memoryBiasConfig')
            in_w_reg = self.get_regularizer(key = 'inputGateWeightConfig')
            for_w_reg = self.get_regularizer(key = 'forgetGateWeightConfig')
            out_w_reg = self.get_regularizer(key = 'outputGateWeightConfig')
            mem_w_reg = self.get_regularizer(key = 'memoryWeightConfig')
            in_re_w_reg = self.get_regularizer(key = 'inputGateRecurrentWeightConfig')
            for_re_w_reg = self.get_regularizer(key = 'forgetGateRecurrentWeightConfig')
            out_re_w_reg = self.get_regularizer(key = 'outputGateRecurrentWeightConfig')
            mem_re_w_reg = self.get_regularizer(key = 'memoryRecurrentWeightConfig')
            in_peep_w_reg = self.get_regularizer(key = 'inputGatePeepholeWeightConfig')
            for_peep_w_reg = self.get_regularizer(key = 'forgetGatePeepholeWeightConfig')
            out_peep_w_reg = self.get_regularizer(key = 'outputGatePeepholeWeightConfig')
            in_b_reg = self.get_regularizer(key = 'inputGateBiasConfig')
            for_b_reg = self.get_regularizer(key = 'forgetGateBiasConfig')
            out_b_reg = self.get_regularizer(key = 'outputGateBiasConfig')
            mem_b_reg = self.get_regularizer(key = 'memoryBiasConfig')    
            u_p = 'True' if self.usePeephole else 'False'
            mc_c = 'None' if self.memoryCellClip is None else str(self.memoryCellClip)
            ss_c = 'None' if self.shadowStateClip is None else str(self.shadowStateClip)
            layer = 'self.%s = BiLSTM(%d, activation = BiLSTM.Activation(%s,%s,%s,%s,%s),kernelInitializer = BiLSTM.Initializer(%s,%s,%s,%s),'\
                'recurrentInitializer = BiLSTM.Initializer(%s,%s,%s,%s),peepholeInitializer = BiLSTM.Initializer(%s,%s,%s,None),biasInitializer = BiLSTM.Initializer(%s,%s,%s,%s),'\
                'kernelRegularizer = BiLSTM.Regularizer(%s,%s,%s,%s),recurrentRegularizer = BiLSTM.Regularizer(%s,%s,%s,%s),peepholeRegularizer = BiLSTM.Regularizer(%s,%s,%s,None),'\
                'biasRegularizer = BiLSTM.Regularizer(%s,%s,%s,%s),useBias = %s,usePeephole = %s,dropout =%f,recurrentDropout =%f,'\
                'memoryCellClip = %s, shadowStateClip = %s, statefull = %s, mergeMode = \'%s\')'% \
                (self.name, self.layerUnits, in_act, for_act, out_act, mem_act, shadow_act, in_w_init, for_w_init, out_w_init, mem_w_init,
                    in_re_w_init, for_re_w_init, out_re_w_init, mem_re_w_init, in_peep_w_init, for_peep_w_init, out_peep_w_init, 
                    in_b_init, for_b_init, out_b_init, mem_b_init, in_w_reg, for_w_reg, out_w_reg, mem_w_reg,
                    in_re_w_reg, for_re_w_reg, out_re_w_reg, mem_re_w_reg, in_peep_w_reg, for_peep_w_reg, out_peep_w_reg,
                    in_b_reg, for_b_reg, out_b_reg, mem_b_reg, u_b, u_p, d, re_d, mc_c, ss_c, sf, mm)
        layer_list.append(layer)

        call = '%s = self.%s(%s)'% (A,self.name,B)
        call_list.append(call)
        
        if self.batchNorm:
            layer_list.append('self.%s_bn = BatchNormalization()'%(self.name))
            call_list.append('%s = self.%s_bn(%s, training=training)'%(A,self.name,A))
            return layer_list, call_list, output_reg, new_shape
        else:
            return layer_list, call_list, output_reg, new_shape
    
    def keras_call_auto_reshape_build(self, input_shape, combine_output_name, output_reg):
        layer_obj = []
        call_obj = []

        if combine_output_name != output_reg:
            A = output_reg
            B = combine_output_name
        else:
            A = output_reg
            B = output_reg
        output_name = output_reg

        if len(input_shape) != 3:
            if len(input_shape) > 3 :
                if len(input_shape) < 5:
                    new_shape = input_shape[0:2]
                    new_shape.append(-1)
                    string = 'self.%s_auto_reshape = tf.keras.layers.Reshape(('%(self.name)
                    string += ','.join([str(int(x)) for x in new_shape[1:]])
                    string += '))'
                    layer_obj.append(string)
                    string = '%s = self.%s_auto_reshape(%s)'%(A, self.name, B)
                    call_obj.append(string)
                else:
                    raise ValueError("When dimension of data is more than 4, it is not supported.")
            elif len(input_shape) == 2 :
                dim = input_shape[-1]
                new_shape = input_shape[0:1]
                new_shape.append(1)
                new_shape.append(dim)
                string = 'self.%s_auto_reshape = tf.keras.layers.Reshape(('%(self.name)
                string += ','.join(['1',str(int(dim))])
                string += '))'
                layer_obj.append(string)
                string = '%s = self.%s_auto_reshape(%s)'%(A, self.name, B)
                call_obj.append(string)
            else:
                raise ValueError("One dimension data is not supported.")
        else:
            new_shape = input_shape
            output_name = combine_output_name
        return layer_obj, call_obj, output_name, new_shape

class _ModelNodeTFHub():
    '''
			Class representing a Computational Unit module.   --- UPDATED (Trista) 20200812
    '''
    @staticmethod
    def createFromJSON(obj: Dict[str, Any], train: 'Train') -> 'ModelNode.TFHub.Config':
        """
            Parse a previously saved object into a new @ModelNode.ModelNode.Config object. This will auto-determine the sub-class of the object, and pass the JSON object to the inner method to continue to parse.   --- UPDATED (Trista) 20200812

            Parameters
            ------------------------------

            obj `dict<str, *>` - JSON object from Project file.
            
            train `Train` - The train of that the layers attach to.

            Returns
            ------------------------------

            `ModelNode.ComputationalUnit.Config` - A @ModelNode.ModelNode.Config object.
        """
        modelNode = ModelNode.TFHub.Config()

        # Parse the layer profile.
        modelNode.parseJSON(obj, train)
        return modelNode

    class Config(_ModelNodeConfig):
        def __init__(self, name:str = None, unitNamespace:str = None, 
                           trainable:bool = True, url:str = "https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/4"):
            '''
                Create a TF Hub.   --- UPDATED (Trista) 20200812

                Parameters
                ------------------------------

                name                `str`                           - The name of this layer.

                unitNamespace `str` - The computational unit namespace.
            '''
            super().__init__(nodeType = ModelNode.Types.TFHub, name = name)
            # `bool` - Whether this Hub is trainable.
            self.trainable = trainable
            # `str` - The URL of the TensorFlow Hub.
            self.url = url
            # `list<list<number>>` - A list of shapes in the order of each build. Each shape should be a list of number or `None`.
            self.outputShape = [[]]

        def parseJSON(self, obj: Dict[str, Any], train: 'Train'):
            """
                Parse a previously saved object into this class of @ComputationalUnitConfig .   --- UPDATED (Trista) 20200812

                Parameters
                ------------------------------

                obj `dict<str,*>` - JSON object from Project file

                train `Train` - The train of that the layers attach to.
            """
            for (k, v) in obj.items():
                if (k == "name"):
                    setattr(self, k, ModelNode.updateNodeName(v).replace(' ',''))
                elif (k in ["shape", "outputShape"]):
                    # Backward compatibility for one-dimension shape or order.   --- MAXVER 1908
                    if (isinstance(v, list) and len(v) > 0):
                        setattr(self, k, v)
                    else:
                        setattr(self, k, [v])
                elif (k == "_order"):
                    # Backward compatibility for one-dimension shape or order.   --- MAXVER 1908
                    if (isinstance(v, list) and len(v)):
                        if (len(np.array(v)) == 2):
                            setattr(self, k, [s[0] for s in v])
                        else:
                            setattr(self, k, v)
                    else:
                        setattr(self, k, [v])
                elif (k in ["fromNode", "toNode"]):
                    setattr(self, k, [[s.replace(' ','') for s in v[0]]])
                elif (k not in ["_train", "_nodeType"]):
                    setattr(self, k, v)

            # Assign this layer to the train object.
            self.train = train

        def keras_call_combineIncomingTensors(self, input_sources, input_tensors, input_sources_name, input_tensors_name, output_reg, buildNo = 0):
            '''
                Get the feed-in tensors from the previous layer modules or sources.   --- UPDATED (Trista) 20200812

                Parameters
                ------------------------------

                input_sources          `list<list<int>>`    - list of input sources shape.

                input_tensors          `list<list<int>>`    - list of input tensors shape.
                
                input_sources_name     `list<str>`          - list of input sources name.
                
                input_tensors_name     `list<str>`          - list of input tensors name.

                output_reg             `str`                - Ragister name for this layer

                buildNo                `int`               - The build number to be built.           
                
                Returns
                ------------------------------

                `tuple<list<int>, list<str>, list<str>, str>` - Output shape, list of code, list of code, register name
            '''
            incoming_names = []
            incoming_names.extend(input_sources_name)
            incoming_names.extend(input_tensors_name)
            incoming_shapes = []
            incoming_shapes.extend(input_sources)
            incoming_shapes.extend(input_tensors)
            return incoming_shapes, [], [], incoming_names

        def keras_call_auto_reshape_build(self, input_shape, combine_output_name, output_reg):
            '''
                Create keras reshape layer code   --- UPDATED (Trista) 20200812

                Parameters
                ------------------------------

                inputshape              `list<tuple<int>>`          - list of input tensor shape.
                
                combine_output_name     `str`                       - register name from auto reshape stage

                output_reg              `str`                       - register name
                            
                Returns
                ------------------------------

                `list<str>, list<str>, <str>, list<tuple<int>>` - list of code, list of code, register name, Output shape
            '''
            return [], [], combine_output_name, input_shape

        def keras_call_layer_build(self, reshape_output_name, output_reg, inputshape = None):
            '''
                Create keras layer code   --- UPDATED (Trista) 20200812

                Parameters
                ------------------------------

                reshape_output_name     `str`                   - register name from auto reshape stage

                output_reg              `str`                   - register name
                
                inputshape              `list<tuple<int>>`      - list of input tensor shape.
                            
                Returns
                ------------------------------

                `list<str>, list<str>, <str>, list<tuple<int>>` - list of code, list of code, register name, Output shape
            '''
            layer_list = []
            call_list = []
            layer = 'self.%s = hub.KerasLayer("%s", trainable=%s)'% (self.name, self.url, str(self.trainable))
            layer_list.append(layer)

            if self._type == "BERT":
                call = '%s, _ = self.%s(' % (output_reg,self.name)
            else:
                call = '%s = self.%s(' % (output_reg,self.name)
            if len(reshape_output_name) > 1:
                call += '['
                call += ','.join(reshape_output_name)
                call += '], training=training)'
            else:
                call += '%s, training=training)'% (reshape_output_name[0])
            call_list.append(call)

            return layer_list, call_list, output_reg, inputshape

        def keras_call_processOutputTensor(self, input_shape, layer_output_name, output_reg, buildNo = 0) :
            return [], []

class _ModelNodeLayerTaskConfig(_ModelNodeLayerConfig):
    '''
			Class representing a final layer (training task).   --- UPDATED (Dexter) 20180630
    '''
    def __init__(self, taskType: 'ModelNode.Layer.Task.Types', name: str = "", lossDppKey: str = "target", measurement: str = "accuracy", measurementOptions: Dict[str, Any] = {}, cuzBuild: Callable = None,
            incomingConfig: 'ModelNode.Layer.Incoming.Config' = _ModelNodeLayerIncoming.Concat(), 
            linearTransform: 'Train.Variable.LinearTransform' = Train.Variable.LinearTransform.createBasicConfig(weightAvg = 0, weightStdDev = 0.04, weightL1Loss = False, weightL2Loss = True, weightL2Decay = 0.004, biasInitial = 0.001),
            activation: 'Train.Activation' = Train.Activation.Relu, activationParams: Dict[str, Any] = {}, 
            batchNorm: bool = True, batchNormParams: Dict[str, Any] = {}, dropout: float = 1, 
            totalTasks = 1, taskDimension = 1, buildNo = 0, weightDecayRate = 0.004, biasInitial = 0.001, weightAvg=0, weightStdDev=0.04, weightL1Loss=False, weightL2Loss=True):
        '''
			Creates a final layer.   --- UPDATED (Dexter) 20200120

            Parameters
            ------------------------------

            taskType `ModelNode.Layer.Task.Types` - The task type.

            name                `str`   - The name of this layer.
            
            lossDppKey          `str`   - The key for the column config of the target this task is comparing with.
            
            measurement         `str`   - The measurement of the training task, like MSE or accuracy, in comparing the target data and model predictions.

            measurementOptions  `dict<str, *>`  - The measurement options of the measurement of this training task.
            
            cuzBuild            `Callable`  - A callback function for processing tensors during model building and returning a loss value.   [BETA]

            incomingConfig      `ModelNode.Layer.Incoming.Config`    - Input configurations.

            linearTransform     `Train.Variable.LinearTransform` - The linear transformation configuration.

            activation          `Train.Activation`   - The activation function of this layer.
            
            activationParams    `dict{str:*}`   - Activation parameters as defined in TensorFlow.
            
            batchNorm       `bool`  - Whether to use batch normalization before activation function.
            
            batchNormParams     `dict{str:*}`   - Batch normalization parameters as defined in TensorFlow.
            
            dropout         `float` - The keep probability of dropout during training.
            
            totalTasks          `int`   - The number of tasks for multi-task learning.   [Deprecated]
            
            taskDimension       `tuple(int+)`   - The task output dimension.   [Deprecated]
            
            weightDecayRate     `float` - The constant for weighting L2-loss of weights.   [Deprecated]
            
            weightL1Loss        `bool`  - Whether to take L1-loss on the weights.   [Deprecated]
            
            weightL2Loss        `bool`  - Whether to take L2-loss on the weights.   [Deprecated]
            
            weightAvg           `float` - The average value of the initialization of weights.   [Deprecated]
            
            weightStdDev        `float` - The standard deviation of the initialization of weights.   [Deprecated]
            
            biasInitial         `float` - Constant initial value of biases.   [Deprecated]
            
            buildNo             `int`   - The build number of staged training   [Deprecated]
        '''
        super().__init__(ModelNode.Layer.Types.Task, name = name, layerUnits = -1, final=True, 
                        incomingConfig = incomingConfig, linearTransform = linearTransform,
                        activation = activation, activationParams = activationParams, 
                        batchNorm = batchNorm, batchNormParams = batchNormParams, dropout = dropout,
                        outputConfig = None)
        # `ModelNode.Layer.Task.Types` - The type of the NOM task, as defined in @ModelNode.Layer.Task.Types .
        self._taskType = taskType
        # `str` - The measurement of the training task, like MSE or accuracy, in comparing the target data and model predictions.
        self.measurement = measurement
        # `dict<str,*>` - The measurement options of the measurement of this training task.
        self.measurementOptions = measurementOptions
        # `str` - The key for the data preprocessing node of the target source this task is comparing with.   --- DEPRECATED --- MAXVER TBA
        self.lossDppKey = lossDppKey

        ### Training Properties
        # `tf.Tensor` - The placeholder tensor of the target data preprocessing node.
        self._targetTensors = None
        # `tf.Tensor` - The prediction tensor of the logit resulting just before getting the loss value.
        self._predictionTensors = None
        self._evalTotal = 0
        self._evalCumScore = 0
        self._evalCumList = []
        self._evalTotList = []
        self._customizeBuild = cuzBuild

    def _clearTempTensors(self):
        '''
			Clear temp tensors that are on previous graphs, usually called for a new build.   --- UPDATED (Dexter) 20181114
        '''
        super()._clearTempTensors()
        self._predictionTensors = None
        self._clearEvalInfo()
    
    def _clearEvalInfo(self):
        '''
			Clear evaluation information, usually called for a new build.   --- UPDATED (Dexter) 20181114
        '''
        self._evalTotal = 0
        self._evalCumScore = 0
        self._evalCumList = []
        self._evalTotList = []

    def _updateOrder(self, buildNo: int = 0):
        '''
			Update the order of this layer, noted it should be the largest order.   --- UPDATED (Dexter) 20190730

            Parameters
            ------------------------------

            buildNo     `int`   - The build number to be built.
        '''
        self._order[buildNo] = max([*[lp._order[buildNo] for lp in self.train.getEndingNodes(buildNo=buildNo) if lp != self], *[lp._order[buildNo] for lp in self.fromNode[buildNo]], 0]) + 1

    def getTraceTarget(self):
        '''
			Get the target values of the trace items.   --- UPDATED (Dexter) 20190509
            
            Returns
            ------------------------------

            `list[list[*+]]`     - A list of trace values, each row prefixed with comparing tensor info and item index columns.
        '''
        if self.lossDppKey not in self.train.traceItems:
            raise ValueError("Trace Item Structure not matching with training source structure.")

        traceItems = self.train.traceItems[self.lossDppKey]
        # TODO
        return [[self.lossDppKey, tii, *ti] for tii, ti in enumerate(self.train.dppNodes[self.lossDppKey].getPrintableItems(traceItems, recovered = False))]

    def getTraceItems(self, allPredictedValues: Optional[Dict[str, 'np.ndarray']] = None, buildNo: int = 0) -> List[List[Any]]:
        '''
			Get the predicted values of the trace items.   --- UPDATED (Dexter) 20190509
            
            Parameters
            ------------------------------

            allPredictedValues      `dict{str: np.ndarray}`     - Predicted values in a layername-value dictionary.

            buildNo                 `int`                       - The build number to be built.

            Returns
            ------------------------------

            `list[list[*+]]`     - A list of trace values, each row prefixed with comparing tensor info and item index columns.
        '''
        if self.lossDppKey not in self.train.traceItems:
            raise ValueError("Trace Item Structure not matching with training source structure.")
        
        predictedValues = (self.train.predict(x=self.train.traceItems, buildNo = buildNo) if allPredictedValues is None else allPredictedValues)[self.name]
        return [[self.lossDppKey, pii, *pi] for pii, pi in enumerate(self.train.dppNodes[self.lossDppKey].getPrintableItems(predictedValues))]

    def copy(self, name):
        '''
			Copy this layer profile.   --- UPDATED (Dexter) 20180623

            Parameters
            ------------------------------

            name    `str`   - The new name of the copied layer
        '''
        # Ensure this is not a subclass.
        raise ValueError("Task Layer is not copyable.")
        
    def _build(self, buildNo: int):
        '''
			Build the TensorFlow Graph of this layer.   --- BETA --- UPDATED (Dexter) 20190506

            Parameters
            ------------------------------

            buildNo     `int`   - The build number to be built.
        '''
        self._clearTempTensors()

        # 0. Ensure all incoming nodes have been built.
        if (all([n._built for n in self.fromNode[buildNo]])):
            # 1. Collect all incoming tensors, and confirm there is incoming tensor
            fromTensor = self._combineIncomingTensors(buildNo = buildNo)
            targetTensors = self._targetTensors = self.train._sourceTensors[self.lossDppKey]
            
            # TensorFlow Graph building, using the layer name as the scope
            with tf.compat.v1.variable_scope(self.name, reuse=tf.compat.v1.AUTO_REUSE) as scope:
                # 2. If there is a custom build function, apply it and it should return a loss.
                if (self._customizeBuild is not None):
                    tf.compat.v1.add_to_collection('losses', self._customizeBuild(self, fromTensor, targetTensors))
                else:
                    raise ValueError("Task Layer is not buildable unless custom build function is given. Other Task Layer classes like ModelNode.Layer.Task.Classifier or ModelNode.Layer.Task.Regressor can be used instead.")

            self._built = True
    
    def partialEvaluate(self, allTestData, allPredictedResults, metrics=None):
        '''
			Partial evaluate some predicted results, typically during batched tests. No action taken for this class.   --- UPDATED (Dexter) 20180630

            Parameters
            ------------------------------

            allTestData         `np.ndarray|list`    - A list of test data (actual result).

            allPredictedResults `np.ndarray|list`    - A list of predicted data (model output result).
        '''
        pass
    
    def getTestScore(self):
        '''
			Get the test score of the test results. No action taken for this class.   --- UPDATED (Dexter) 20180630
        '''
        pass

    def recoverPredictedResults(self, predictedData):
        '''
			Recover the predicted results to raw-data format of the predicted data, like getting the label, undo normalization, etc.   --- UPDATED (Dexter) 20190509

            Parameters
            ------------------------------

            predictedData       `np.ndarray|list`    - A list of predicted data (model output result).

            Returns
            ------------------------------

            `np.ndarray|list`    - A list of recovered predicted data.
        '''
        return self.train.dppNodes[self.lossDppKey].recoverToRawData(predictedData)

    def get_initiallizer(self, key = 'weightConfig'):
        '''
			get initiallizer name   --- UPDATED (kai Hsiang, Dexter) 20200120

            Parameters
            ------------------------------

            key     `str`     - config type
                        
            Returns
            ------------------------------

            `str` - keras initializer code
        '''
        '''initializer_type = self.keras_initializer_info[key]['initializer']['_type']
        default = {'value':0., 'mean':0., 'stddev': 0.02, 'gain': 1.0, 'maxval': 1.0, 'minval':0.}
        for parm_name, value in self.keras_initializer_info[key]['initializer'].items():
            try:
                default[parm_name] = float(value)
            except:
                pass
        self.initializer_dictionary = \
        {'Constant':'initializers.Constant(value=%g)'%(default['value']),
         'Zeros':'initializers.Zeros()',
         'RandomNormal':'initializers.RandomNormal(mean=%g, stddev=%g, seed=1234)'%(default['mean'], default['stddev']),
         'Ones':'initializers.Ones()',
         'TruncatedNormal':'initializers.TruncatedNormal(mean=%g, stddev=%g, seed=1234)'%(default['mean'], default['stddev']),
         'RandomUniform':'initializers.RandomUniform(minval=%g, maxval=%g, seed=1234)'%(default['minval'], default['maxval']),
         'Orthogonal':'initializers.Orthogonal(gain=%g, seed=1234)'%(default['gain']),
         'Identity':'initializers.Identity(gain=%g)'%(default['gain']),
         'GlorotNormal':'initializers.glorot_normal(seed=1234)',
         'GloroUniform':'initializers.glorot_uniform(seed=1234)',
         'HeNormal':'initializers.he_normal(seed=1234)',
         'HeUniform':'initializers.he_uniform(seed=1234)',
         'LecunNormal':'initializers.lecun_normal(seed=1234)',
         'LecunUniform':'initializers.lecun_uniform(seed=1234)'}
        try:
            return self.initializer_dictionary[initializer_type]
        except:
            return None'''
        return "initializers." + getattr(self.linearTransform, key).initializer.getConstructorString().replace('auto', '0.05')
    
    def get_regularizer(self, key = 'weightConfig'):
        '''
            get regularizer name   --- UPDATED (kai Hsiang, Dexter) 20200120

            Parameters
            ------------------------------

            key     `str`     - config type
                        
            Returns
            ------------------------------

            `str` - keras regularizer code
        '''
        L1 = getattr(self.linearTransform, key).l1Loss
        L2 = getattr(self.linearTransform, key).l2Loss
        L1Decay = getattr(self.linearTransform, key).l1Decay
        L2Decay = getattr(self.linearTransform, key).l2Decay
        if (L1 and L2) and (L1Decay != 0 and L2Decay != 0):
            return 'regularizers.l1_l2(l1=%g, l2=%g)'%(float(L1Decay),float(L2Decay))
        if L1 and L1Decay != 0:
            return 'regularizers.l1(%g)' % (float(L1Decay))
        if L2 and L2Decay != 0:
            return 'regularizers.l2(%g)' % (float(L2Decay))
        
        return 'None'
 
class _Task(_ModelNodeLayerTaskConfig):
    def __init__(self, name: str = "Task", classCount: int = 1, lossDppKey: str = "target", measurement: str = "accuracy", measurementOptions: Dict[str, Any] = {}, 
                incomingConfig: 'ModelNode.Layer.Incoming.Config' = _ModelNodeLayerIncoming.Concat(), 
                linearTransform: 'Train.Variable.LinearTransform' = Train.Variable.LinearTransform.createBasicConfig(weightAvg = 0, weightStdDev = "auto", weightL1Loss = False, weightL2Loss = True, weightL2Decay = 0.0, biasInitial = 0.0),
                activation: 'Train.Activation' = Train.Activation.Relu, activationParams: Dict[str, Any] = {},
                buildNo = 0, weightDecayRate = 0, weightL1Loss = False, weightL2Loss = True, weightAvg = 0, weightStdDev = "auto", biasInitial = 0.0):
        
        super().__init__(ModelNode.Layer.Task.Types.Classifier, name = name, lossDppKey=lossDppKey, measurement=measurement, measurementOptions=measurementOptions, 
                        incomingConfig = incomingConfig, linearTransform = linearTransform,
                        activation = activation, activationParams=activationParams, 
                        batchNorm=False)
        self.trainingConfig = {}
        self.comparisons = []
        self.applyGradient = []
        self.optimizer_dict = {'adam':'tf.keras.optimizers.Adam',
                          'rmsprop':'tf.keras.optimizers.RMSprop',
                          'grad':'tf.keras.optimizers.SGD',
                          'momentum':'tf.compat.v1.train.MomentumOptimizer',
                          'adagrad':'tf.keras.optimizers.Adagrad',
                          'adadelta':'tf.keras.optimizers.Adadelta',
                          'ftrl':'tf.keras.optimizers.Ftrl'}
        self.loss_dict = {
            'binary_crossentropy': 'tf.reduce_mean(tf.keras.losses.binary_crossentropy',
            'categorical_crossentropy': 'tf.reduce_mean(tf.keras.losses.categorical_crossentropy',
            'categorical_hinge': 'tf.reduce_mean(tf.keras.losses.categorical_hinge',
            'cosine_similarity': 'tf.reduce_mean(tf.keras.losses.cosine_similarity',
            'hinge': 'tf.reduce_mean(tf.keras.losses.hinge',
            'KLD': 'tf.reduce_mean(tf.keras.losses.KLD',
            'logcosh': 'tf.reduce_mean(tf.keras.losses.logcosh',
            'MAE': 'tf.reduce_mean(tf.keras.losses.MAE',
            'MAPE': 'tf.reduce_mean(tf.keras.losses.MAPE',
            'MSE': 'tf.reduce_mean(tf.keras.losses.MSE',
            'MSLE': 'tf.reduce_mean(tf.keras.losses.MSLE',
            'poisson': 'tf.reduce_mean(tf.keras.losses.poisson',
            'sparse_categorical_crossentropy': 'tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy',
            'squared_hinge': 'tf.reduce_mean(tf.keras.losses.squared_hinge'
        }
        self.metrics_dict = {
            'binary_crossentropy': tf.keras.metrics.BinaryAccuracy(),
            'categorical_crossentropy': tf.keras.metrics.CategoricalAccuracy(),
            'categorical_hinge': tf.keras.metrics.CategoricalHinge(),
            'cosine_similarity': tf.keras.metrics.CosineSimilarity(),
            'hinge': tf.keras.metrics.Hinge(),
            'KLD': tf.keras.metrics.KLDivergence(),
            'logcosh': tf.keras.metrics.LogCoshError(),
            'MAE': tf.keras.metrics.MeanAbsoluteError(),
            'MAPE': tf.keras.metrics.MeanAbsolutePercentageError(),
            'MSE': tf.keras.metrics.MeanSquaredError(),
            'MSLE': tf.keras.metrics.MeanSquaredLogarithmicError(),
            'poisson': tf.keras.metrics.Poisson(),
            'sparse_categorical_crossentropy': tf.keras.metrics.SparseCategoricalAccuracy(),
            'squared_hinge': tf.keras.metrics.SquaredHinge()
        }
        self._evalTotal = {}   # 0
        self._evalCumScore = {}   # 0
        self._evalCumList = {}   # []
        self._evalTotList = {}   # []
        self._evalMetricResult = {}

    def _clearEvalInfo(self):
        '''
			Clear evaluation information, usually called for a new build.   --- UPDATED (Dexter) 20181114
        '''
        self._evalTotal = {}   # 0
        self._evalCumScore = {}   # 0
        self._evalCumList = {}   # []
        self._evalTotList = {}   # []
        self._evalMetricResult = {}

    def keras_call(self, i):
        layer_list = []
        loss_list = []
        update_list = []

        apply_grad = []
        try:
            apply_grad = self.applyGrandient
            apply_grad = map(lambda x: 'self.' + x, apply_grad)
        except:
            pass

        train_conf = {}
        train_conf['initialLearningRate'] = self.trainingConfig['initialLearningRate']
        train_conf['learningRate'] = self.trainingConfig['initialLearningRate']
        train_conf['learningRateDecay'] = self.trainingConfig['learningRateDecay']
        if train_conf['learningRateDecay'] == True:
            train_conf['learningRateDecayFactor'] = self.trainingConfig['learningRateDecayFactor']
            train_conf['numEpochsPerDecay'] = self.trainingConfig['numEpochsPerDecay']
        else:
            train_conf['learningRateDecayFactor'] = 0
            train_conf['numEpochsPerDecay'] = 0

        layer_list.append('self.%s_learningConfig = %s' % (self.name, str(train_conf)))
        layer_list.append('self.%s_optimizer = %s(learning_rate=self.%s_learningConfig["learningRate"])'%(self.name, self.optimizer_dict[self.trainingConfig['optimizer']], self.name))

        for m in self.comparisons:
            loss_fun = self.keras_loss(m['measurement'])
            y_true = m['target'].replace(' ', '')
            y_pred = m['input'].replace(' ', '')
            loss_list.append('%s(self.%s_output,self.%s_output))' % (loss_fun, y_true, y_pred))
        loss_list = ' + '.join(loss_list)
        # loss_list = 'self.%s_loss = %s' % (self.name,loss_list)
        # loss_list = loss_list + '\n\t\tprint(self.Generator1.losses)'

        grad_string = ', '.join(apply_grad)

        # loss_list = 'self.' + self.name + '_loss = ' + loss_list + ' if not ' + ' + '.join(apply_grad) + ' else ' + loss_list + ' + ' + ' + '.join(apply_grad)

        loss_return = 'self.%s_loss_grad = 0\n' % (self.name)
        loss_return += '\t\tfor g in [%s]:\n' % (grad_string)
        loss_return += '\t\t\tfor loss in g.losses:\n'
        loss_return += '\t\t\t\tself.%s_loss_grad += loss\n' % (self.name)
        # loss_return += '\t\t\tif g.losses:\n'
        # loss_return += '\t\t\t\tself.%s_loss_grad += g.losses[0]\n' % (self.name)

        loss_return += '\t\tself.%s_loss = %s + self.%s_loss_grad\n' % (self.name, loss_list, self.name)

        loss_var = []
        loss_var.append('self.%s_loss'%(self.name))

        # print(grad_string)
        # exit()
        
        # loss_list = 'self.%s_loss = %s if not (%s) else %s + %s' % (self.name, loss_list, grad_string, loss_list, grad_string)
        # loss_list = loss_list + '\n\t\tprint(self.%s_loss)'%self.name
        var_list = []

        gradient_is_empty = True
        for v in self.applyGradient:
            if not v is None:
                gradient_is_empty = False
                var_list.append('self.%s.trainable_variables'%(v))
            else:
                var_list.append('self.trainable_variables')
                break
        if gradient_is_empty:
            var_list.append('self.trainable_variables')

        var_list = '+'.join(var_list)
        # update_list.append('grads = tape[%d].gradient(self.%s_loss, %s)'%(self.tape,self.name,var_list))
        
        update_list.append('# Task %s updates the trainable variables' % i)
        update_list.append('if task_order == %s:' % i)
        update_list.append('\tgrads = tape.gradient(tf.reduce_mean(self.%s_loss), %s)'%(self.name,var_list))
        update_list.append('\tself.%s_optimizer.apply_gradients(zip(grads, %s))'%(self.name,var_list))

        return layer_list,[loss_return, loss_var],update_list
    
    def keras_loss(self, loss_name = 'categorical_crossentropy'):
        # return 'tf.keras.losses.binary_crossentropy'
        return self.loss_dict[loss_name]

    def keras_get_loss(self):
        '''
			Create tensorflow loss code   --- UPDATED (kai Hsiang) 20190909
                        
            Returns
            ------------------------------

            `str` - loss code
        '''
        return '%s_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits\\\n\t\t(labels=self.%s_target, logits=self.%s_output, name=\'crossEntropyPerExample\'))' %(self.name,self.name,self.name)

    def recoverPredictedResults(self, predictedData, lossDppKey):
        '''
			Recover the predicted results to raw-data format of the predicted data, like getting the label, undo normalization, etc.   --- UPDATED (Dexter) 20190509

            Parameters
            ------------------------------

            predictedData       `np.ndarray|list`    - A list of predicted data (model output result).

            Returns
            ------------------------------

            `np.ndarray|list`    - A list of recovered predicted data.
        '''
        return self.train.dppNodes[lossDppKey].recoverToRawData(predictedData)

    def partialEvaluate(self, testData: Union['np.ndarray', List[Any]], allPredictedResults: Union['np.ndarray', List[Any]], comparison: Dict[str, 'np.ndarray']=None):
        '''
			Partial evaluate some predicted results, typically during batched tests. No action taken for this class.   --- UPDATED (Dexter) 20191024

            Parameters
            ------------------------------

            allTestData         `np.ndarray|list`    - A list of test data (actual result).

            allPredictedResults `np.ndarray|list`    - A list of predicted data (model output result).
        '''
        # Collect the test data and recover to the original range.
        # testData = np.array(allTestData[self.lossDppKey])
        # testData = self.recoverPredictedResults(allTestData)
        taskName = self.name+'_'+comparison["input"]+'_'+comparison["target"]
        metrics = comparison["metrics"]
        compareCount = testData.shape[0]

        # Evaluate the result by accumulating the evaluation.
        if (metrics == "accuracy" or metrics == "sparse_categorical_crossentropy"):
            compareResults = testData == allPredictedResults
            try:
                self._evalCumScore[taskName] += (compareResults == True).sum()
                self._evalTotal[taskName] += compareCount
            except:
                self._evalCumScore[taskName] = (compareResults == True).sum()
                self._evalTotal[taskName] = compareCount
        elif (metrics == "nmi" or metrics == "f1score"):
            try:
                self._evalCumList[taskName][len(self._evalCumList):] = [*np.reshape(allPredictedResults, [-1])]
                self._evalTotList[taskName][len(self._evalCumList):] = [*np.reshape(testData, [-1])]
            except:
                self._evalCumList[taskName][len(self._evalCumList):] = [*np.reshape(allPredictedResults, [-1])]
                self._evalTotList[taskName][len(self._evalCumList):] = [*np.reshape(testData, [-1])]
        elif (metrics == "MSE" or metrics == "PSNR" or metrics == "SMAPE"):
            if (taskName not in self._evalCumScore):
                self._evalCumScore[taskName] = 0
                self._evalTotal[taskName] = 0
            if comparison["target"] in self.train.traceItems:
                colConfig = self.train.dppNodes[comparison["target"]]
                compareCount = testData.size
                if (metrics == "MSE" or metrics == "PSNR"):
                    # if comparison["target"] in self.train.traceItems:
                    if (not hasattr(colConfig, "circular")) or (len(colConfig.circular)==0):
                        self._evalCumScore[taskName] += np.sum(np.square(np.subtract(testData, allPredictedResults)))
                        self._evalTotal[taskName] += compareCount
                    elif (metrics != "PSNR"):
                        for idx in range(0, colConfig.getShape()[1]):
                            if idx in colConfig.colToCircular:
                                cc = colConfig.circular[colConfig.colToCircular[idx]]
                                minV, maxV = cc.min,  cc.max
                                rangeV = maxV - minV
                                self._evalCumScore[taskName] += np.sum(np.square(np.minimum(np.mod((testData[:,idx]-allPredictedResults[:,idx])+rangeV, rangeV), np.mod((allPredictedResults[:,idx]-testData[:,idx])+rangeV, rangeV))))
                            else:
                                self._evalCumScore[taskName] += np.sum(np.square(np.subtract(testData[:,idx], allPredictedResults[:,idx])))
                        self._evalTotal[taskName] += compareCount
                    else:
                        raise ValueError("PSNR is not supported for circular data.")
                elif (metrics == "SMAPE"):
                    if (not hasattr(colConfig, "circular")) or len(colConfig.circular) == 0:
                        self._evalCumScore[taskName] += np.sum(np.abs(np.subtract(testData, allPredictedResults))/((np.abs(testData) + np.abs(allPredictedResults))/2))
                        self._evalTotal[taskName] += compareCount
                    else:
                        raise ValueError("SMAPE is not supported for circular data.")
            else:
                if (metrics == "MSE" or metrics == "PSNR"):
                    self._evalCumScore[taskName] += np.sum(np.square(np.subtract(testData, allPredictedResults)))
                    self._evalTotal[taskName] += compareCount
                elif (metrics == "SMAPE"):
                    self._evalCumScore[taskName] += np.sum(np.abs(np.subtract(testData, allPredictedResults))/((np.abs(testData) + np.abs(allPredictedResults))/2))
                    self._evalTotal[taskName] += compareCount
        elif (metrics in self.metrics_dict):
            metric = self.metrics_dict[metrics]
            metric.update_state(testData, allPredictedResults)
            # self._evalMetricResult[taskName] = metric.result().numpy()
            try:
                self._evalCumScore[taskName] += metric.result().numpy() * compareCount
                self._evalTotal[taskName] += compareCount
            except:
                self._evalCumScore[taskName] = metric.result().numpy() * compareCount
                # self._evalCumScore[taskName] *= compareCount
                self._evalTotal[taskName] = compareCount
    
    def getTestScore(self, comparison: Dict[str, 'np.ndarray']=None):
        '''
			Get the test score of the test results. No action taken for this class.   --- UPDATED (Dexter) 20180630

            Returns
            ------------------------------

            `float`     - The test score.
        '''
        taskName = self.name+'_'+comparison["input"]+'_'+comparison["target"]
        metrics = comparison["metrics"]
        if (metrics in self.metrics_dict) or (metrics == "accuracy" or metrics == "MSE" or metrics == "SMAPE"):
            return self._evalCumScore[taskName] / self._evalTotal[taskName]
        elif (metrics == "nmi"):
            return normalized_mutual_info_score(self._evalTotList[taskName], self._evalCumList[taskName])
        elif (metrics == "PSNR"):
            return 10 * math.log10((self.measurementOptions["max"]) ** 2 / (self._evalCumScore[taskName] / self._evalTotal[taskName]))
        elif (metrics in self.metrics_dict):
            return self._evalCumScore[taskName] / self._evalTotal[taskName]

    def getTraceItems(self, allPredictedValues: Optional[Dict[str, 'np.ndarray']] = None, buildNo: int = 0, comparison: Dict[str, 'np.ndarray']=None) -> List[List[Any]]:
        '''
			Get the predicted values of the trace items.   --- UPDATED (Dexter) 20190509
            
            Parameters
            ------------------------------

            allPredictedValues      `dict{str: np.ndarray}`     - Predicted values in a layername-value dictionary.

            buildNo                 `int`                       - The build number to be built.

            Returns
            ------------------------------

            `list[list[*+]]`     - A list of trace values, each row prefixed with comparing tensor info and item index columns.
        '''
        # if self.lossDppKey not in self.train.traceItems:
        if comparison["target"] not in self.train.traceItems:
            raise ValueError("Trace Item Structure not matching with training source structure.")
        
        predictedValues = (self.train.predict(x=self.train.traceItems, buildNo = buildNo) if allPredictedValues is None else allPredictedValues)[self.name+'_'+comparison["input"]+'_'+comparison["target"]]
        return [[comparison["target"], pii, *pi] for pii, pi in enumerate(self.train.dppNodes[comparison["target"]].getPrintableItems(predictedValues))]

    
class _ModelNodeLayerTaskClassifier(_ModelNodeLayerTaskConfig):
    '''
			Class representing a classifier.   --- UPDATED (Dexter) 20180630
    '''
    def __init__(self, name: str = "classifier", classCount: int = 1, lossDppKey: str = "target", measurement: str = "accuracy", measurementOptions: Dict[str, Any] = {}, 
                incomingConfig: 'ModelNode.Layer.Incoming.Config' = _ModelNodeLayerIncoming.Concat(), 
                linearTransform: 'Train.Variable.LinearTransform' = Train.Variable.LinearTransform.createBasicConfig(weightAvg = 0, weightStdDev = "auto", weightL1Loss = False, weightL2Loss = True, weightL2Decay = 0.0, biasInitial = 0.0),
                activation: 'Train.Activation' = Train.Activation.Relu, activationParams: Dict[str, Any] = {},
                buildNo = 0, weightDecayRate = 0, weightL1Loss = False, weightL2Loss = True, weightAvg = 0, weightStdDev = "auto", biasInitial = 0.0):
        '''
			Create a classifier, comparing the class as the last dimension of the actual and predicted data.   --- UPDATED (Dexter) 20200117

            Parameters
            ------------------------------

            name                `str`   - The name of the layer.

            classCount          `int`   - The number of class. -1: Arbitrary class depending on training data source (BETA).

            lossDppKey          `str`   - The key for the column config of the target this task is comparing with.
            
            measurement         `str`   - The measurement of the training task, like MSE or accuracy, in comparing the target data and model predictions.

            measurementOptions  `dict{str:*}`           - The measurement options of the measurement of this training task.
            
            incomingConfig      `ModelNode.Layer.Incoming.Config`        - Input configurations.

            linearTransform     `Train.Variable.LinearTransform` - The linear transformation configuration.

            activation          `Train.Activation`   - The activation function of this layer.
            
            activationParams    `dict{str:*}`   - Activation parameters as defined in TensorFlow.
            
            weightDecayRate     `float` - The constant for weighting L2-loss of weights.   [Deprecated]
            
            weightL1Loss        `bool`  - Whether to take L1-loss on the weights.   [Deprecated]
            
            weightL2Loss        `bool`  - Whether to take L2-loss on the weights.   [Deprecated]
            
            weightAvg           `float` - The average value of the initialization of weights.   [Deprecated]
            
            weightStdDev        `float` - The standard deviation of the initialization of weights.   [Deprecated]
            
            biasInitial         `float` - Constant initial value of biases.   [Deprecated]
            
            buildNo             `int`   - The build number of staged training   [Deprecated]
        '''
        super().__init__(ModelNode.Layer.Task.Types.Classifier, name = name, lossDppKey=lossDppKey, measurement=measurement, measurementOptions=measurementOptions, 
                        incomingConfig = incomingConfig, linearTransform = linearTransform,
                        activation = activation, activationParams=activationParams, 
                        batchNorm=False)
        self.classCount = classCount

    def copy(self, name) -> 'ModelNode.Layer.Task.Classifier':
        '''
			Copy this layer profile.   --- UPDATED (Dexter) 20190822

            Parameters
            ------------------------------

            name    `str`   - The new name of the copied layer

            Returns
            ------------------------------

            `ModelNode.Layer.Task.Classifier`    - A copied profile of this layer.
        '''
        # Ensure this is not a subclass.
        if (isinstance(self, ModelNode.Layer.Task.Classifier)):
            raise ValueError("This layer class (" + self.__class__.__name__ + ") has not supported for copying.")
        
        # Create a new Layer Profile on this.
        return ModelNode.Layer.Task.Classifier(name=name, classCount = self.classCount, lossDppKey = self.lossDppKey, measurement = self.measurement, 
                incomingConfig = self.incomingConfig, linearTransform = self.linearTransform,
                activation = self.activation, activationParams=self.activationParams)

    def _build(self, buildNo: int):
        '''
			Build the TensorFlow Graph of this layer.   --- UPDATED (Dexter) 20190921

            Parameters
            ------------------------------

            buildNo     `int`   - The build number to be built.
        '''
        self._clearTempTensors()

        # 0. Ensure all incoming nodes have been built.
        if (all([n._built for n in self.fromNode[buildNo]])):
            # 1. Collect all incoming tensors, and confirm there is incoming tensor
            mid = self._combineIncomingTensors(buildNo = buildNo)
            targetTensor = self._targetTensors = tf.cast(self.train._sourceTensors[self.lossDppKey], tf.int32)

            # 2. TensorFlow Graph building, using the layer name as the scope
            with tf.compat.v1.variable_scope(self.name, reuse=tf.compat.v1.AUTO_REUSE) as scope:
                # 3. Consolidate merged input shape and target tensor shape
                fromShape = tuple(s for s in mid.shape)
                toShape = tuple(s for s in targetTensor.shape)

                # 4. Determine the from and to shape mapping, the first dimension should be the same based on the batched sampling
                # 4a. If the target tensor has the last dimension of 1 but originally flattened, reshape it.
                originalByRow = False
                if (toShape[-1] == 1):
                    targetTensor = tf.reshape(targetTensor, [*toShape[:-2], -1])
                    toShape = tuple(s for s in targetTensor.shape)
                    originalByRow = True

                # 4b. Handling arbitrary class counts.
                if (self.classCount == -1):
                    self.classCount = self.train.dppNodes[self.lossDppKey].getClassCount()

                # 4c. Check if the previous shape dimension is more or equal to the comparing shape.
                targetDim = len(toShape)
                if (fromShape[:targetDim] != toShape[:targetDim]):
                    raise ValueError("The shape of the output of previous layers cannot match with the target tensor.")

                # 5. Linear transform to flatten the previous layer up to the feature-realted dimensions and fully connected to the target shape.
                linearTransformResults = self.linearTransform.buildOn(mid, axis = targetDim, toUnit = self.classCount, defaultDevice = self.train.device)
                mid = linearTransformResults.results
                self._weights.extend(linearTransformResults.weights)

                # 6. Get the argument max prediction.
                argmaxPrediction = tf.argmax(tf.nn.softmax(mid), axis=targetDim)
                self._predictionTensors = tf.reshape(argmaxPrediction, [*Train.Variable.setAsReshape(argmaxPrediction.shape), 1]) if originalByRow else argmaxPrediction

                # 7. Get loss function.
                targetTensor = tf.cast(targetTensor, tf.int64)
                crossEntropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=targetTensor, logits=mid, name='crossEntropyPerExample')
                crossEntropyMean = tf.reduce_mean(crossEntropy, name='crossEntropy')
                tf.compat.v1.add_to_collection('losses', crossEntropyMean)
            
            self._built = True
        
    def partialEvaluate(self, allTestData: Union['np.ndarray', List[Any]], allPredictedResults: Union['np.ndarray', List[Any]]):
        '''
			Partial evaluate some predicted results, typically during batched tests. No action taken for this class.   --- UPDATED (Dexter) 20191024

            Parameters
            ------------------------------

            allTestData         `np.ndarray|list`    - A list of test data (actual result).

            allPredictedResults `np.ndarray|list`    - A list of predicted data (model output result).
        '''
        # Collect the test data and recover to the original range.
        testData = np.array(allTestData[self.lossDppKey])
        testData = self.recoverPredictedResults(testData)

        # Evaluate the result by accumulating the evaluation.
        if (self.measurement == "accuracy"):
            compareResults = testData == allPredictedResults
            compareCount = testData.size
            self._evalCumScore += (compareResults == True).sum()
            self._evalTotal += compareCount
        elif (self.measurement == "nmi"):
            self._evalCumList[len(self._evalCumList):] = [*np.reshape(allPredictedResults, [-1])]
            self._evalTotList[len(self._evalCumList):] = [*np.reshape(testData, [-1])]
    
    def getTestScore(self):
        '''
			Get the test score of the test results. No action taken for this class.   --- UPDATED (Dexter) 20180630

            Returns
            ------------------------------

            `float`     - The test score.
        '''
        if (self.measurement == "accuracy"):
            return self._evalCumScore / self._evalTotal
        elif (self.measurement == "nmi"):
            return normalized_mutual_info_score(self._evalTotList, self._evalCumList)
    
    def keras_call_layer_build(self, reshape_output_name, output_reg, inputshape = None):
        '''
			Create keras layer code   --- UPDATED (kai Hsiang) 20200120

            Parameters
            ------------------------------

            reshape_output_name     `str`                   - register name from auto reshape stage

            output_reg              `str`                   - register name
            
            inputshape              `list<tuple<int>>`      - list of input tensor shape.
                        
            Returns
            ------------------------------

            `list<str>, list<str>, <str>, list<tuple<int>>` - list of code, list of code, register name, Output shape
        '''
        target_shape = self.train.dppNodes[self.lossDppKey].getShape()
        target_shape = [x if x != 'None' else None for x in target_shape ]
        call = ['self.%s_target = tf.cast(batch_source[\'%s\'],tf.int64)'%(self.name, self.lossDppKey),]
        if (target_shape[-1] == 1):
            new_shape = [*target_shape[:-2], -1]
            string = 'self.%s_target = tf.reshape(self.%s_target,('%(self.name,self.name)
            string += ','.join([str(int(x)) for x in new_shape])
            string += ',))'
            call.append(string)
        call.append('self.%s_output = %s'%(self.name, reshape_output_name))
        return [],call, reshape_output_name, inputshape

    def keras_call_auto_reshape_build(self, input_shape, combine_output_name, output_reg, buildNo = 0):
        '''
			Create keras reshape layer code   --- UPDATED (kai Hsiang) 20200120

            Parameters
            ------------------------------

            inputshape              `list<tuple<int>>`          - list of input tensor shape.
            
            combine_output_name     `str`                       - register name from auto reshape stage

            output_reg              `str`                       - register name
                        
            Returns
            ------------------------------

            `list<str>, list<str>, <str>, list<tuple<int>>` - list of code, list of code, register name, Output shape
        '''
        layer_obj,call_obj = [],[]

        if combine_output_name != output_reg:
            A = output_reg
            B = combine_output_name
        else:
            A = output_reg
            B = output_reg
        output_name = combine_output_name
        # 2. TensorFlow Graph building, using the layer name as the scope
        #with tf.variable_scope(self.name, reuse=tf.AUTO_REUSE) as scope:
        # 3. Consolidate merged input shape and target tensor shape
        fromShape = input_shape
        toShape = self.train.dppNodes[self.lossDppKey].getShape()
        toShape = [x if x!='None' else None for x in toShape]
        # 4. Determine the from and to shape mapping, the first dimension should be the same based on the batched sampling
        # 4a. If the target tensor has the last dimension of 1 but originally flattened, reshape it.
        originalByRow = False
        if (toShape[-1] == 1):
            originalByRow = True

            # 4b. Handling arbitrary class counts.
        if (self.classCount == -1):
            self.classCount = self.train.dppNodes[self.lossDppKey].getClassCount()

            # 4c. Check if the previous shape dimension is more or equal to the comparing shape.
        targetDim = len(toShape) - 1
        
        if (fromShape[:targetDim] != toShape[:targetDim]):
            raise ValueError("The shape of the output of previous layers cannot match with the target tensor.")
        if fromShape != [*toShape[:-1], self.classCount]:
            axis = targetDim
            if axis == len(fromShape) - 1:
                pass
            elif axis == 1:
                new_shape = fromShape[0:axis]
                new_shape.append(functools.reduce(lambda x,y: x*y, fromShape[axis:], 1))
                string = 'self.%s_auto_reshape2 = tf.keras.layers.Flatten()'%(self.name)
                layer_obj.append(string)
                string = '%s = self.%s_auto_reshape2(%s)'%(A, self.name, B)
                call_obj.append(string)
                B = A
                fromShape = new_shape
            elif axis > 1 and axis < len(fromShape) - 1:
                new_shape = fromShape[0:axis]
                new_shape.append(functools.reduce(lambda x,y: x*y, fromShape[axis:], 1))
                string = 'self.%s_auto_reshape2 = tf.keras.layers.Reshape(('%(self.name)
                string += ','.join([str(int(x)) for x in new_shape[1:]])
                string += '))'
                layer_obj.append(string)
                string = '%s = self.%s_auto_reshape2(%s)'%(A, self.name, B)
                call_obj.append(string)
                B = A
                fromShape = new_shape
            else:
                raise ValueError("Linear Transformation axis should within the dimension of the incoming tensor.")

            layer = 'self.%s_auto_reshape3 = tf.keras.layers.Dense(%d, kernel_initializer=%s, bias_initializer=%s, kernel_regularizer=%s, bias_regularizer=%s)'% \
                (self.name, self.classCount, self.get_initiallizer(), self.get_initiallizer(key="biasConfig"), self.get_regularizer(), self.get_regularizer(key="biasConfig"))
            layer_obj.append(layer)
            call = '%s = self.%s_auto_reshape3(%s)'% (A,self.name,B)
            call_obj.append(call)
            B = A
            output_name = A
        else:
            pass

        return layer_obj,call_obj, output_name, [*toShape[:-1], self.classCount]

    def keras_get_loss(self):
        '''
			Create tensorflow loss code   --- UPDATED (kai Hsiang) 20190909
                        
            Returns
            ------------------------------

            `str` - loss code
        '''
        return '%s_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits\\\n\t\t(labels=self.%s_target, logits=self.%s_output, name=\'crossEntropyPerExample\'))' %(self.name,self.name,self.name)

class _ModelNodeLayerTaskRegressor(_ModelNodeLayerTaskConfig):
    '''
			Class representing a regressor.   --- UPDATED (Dexter) 20180630
    '''
    def __init__(self, name: str = "regressor", lossDppKey: str = "target", loss: str = "MSE", measurement: str = "MSE", measurementOptions: Dict[str, Any] = {}, 
                incomingConfig: 'ModelNode.Layer.Incoming.Config' = _ModelNodeLayerIncoming.Concat(), 
                linearTransform: 'Train.Variable.LinearTransform' = Train.Variable.LinearTransform.createBasicConfig(weightAvg = 0, weightStdDev = 0.5, weightL1Loss = False, weightL2Loss = True, weightL2Decay = 0.0, biasInitial = 0.0),
                activation: 'Train.Activation' = Train.Activation.Relu, activationParams: Dict[str, Any] = {},
                multiTask = (1,), buildNo = 0, weightDecayRate = 0, weightL1Loss = False, weightL2Loss = True, weightAvg = 0, weightStdDev = 0.5, biasInitial = 0.0):
        '''
			Create a regressor.   --- UPDATED (Dexter) 20200117

            Parameters
            ------------------------------

            name                `str`   - The name of this layer.
            
            lossDppKey          `str`   - The key for the column config of the target this task is comparing with.
            
            loss                `str`   - The loss function of the training task. If not specified, it will equal to the measurement function.
            
            measurement         `str`   - The measurement of the training task, like MSE or accuracy, in comparing the target data and model predictions.

            measurementOptions  `dict{str:*}`           - The measurement options of the measurement of this training task.
            
            incomingConfig      `ModelNode.Layer.Incoming.Config`        - Input configurations.

            linearTransform     `Train.Variable.LinearTransform` - The linear transformation configuration.

            activation          `Train.Activation`   - The activation function of this layer.
            
            activationParams    `dict{str:*}`   - Activation parameters as defined in TensorFlow.

            multiTask           `str`   - The shape of the multitask of the target data.   [Deprecated]
            
            weightDecayRate     `float` - The constant for weighting L2-loss of weights.   [Deprecated]
            
            weightL1Loss        `bool`  - Whether to take L1-loss on the weights.   [Deprecated]
            
            weightL2Loss        `bool`  - Whether to take L2-loss on the weights.   [Deprecated]
            
            weightAvg           `float` - The average value of the initialization of weights.   [Deprecated]
            
            weightStdDev        `float` - The standard deviation of the initialization of weights.   [Deprecated]
            
            biasInitial         `float` - Constant initial value of biases.   [Deprecated]
            
            buildNo             `int`   - The build number of staged training.   [Deprecated]
        '''
        self.loss = measurement if (loss == None) else loss
        super().__init__(ModelNode.Layer.Task.Types.Regressor, name = name, lossDppKey=lossDppKey, measurement=measurement, measurementOptions=measurementOptions, 
                incomingConfig = incomingConfig, linearTransform = linearTransform,
                activation = activation, activationParams=activationParams, 
                batchNorm=False)

    def _build(self, buildNo: int):
        '''
			Build the TensorFlow Graph of this layer.   --- BETA --- UPDATED (Dexter) 20190917

            Parameters
            ------------------------------

            buildNo     `int`   - The build number to be built.
        '''
        self._clearTempTensors()

        # 0. Ensure all incoming nodes have been built.
        if (all([n._built for n in self.fromNode[buildNo]])):
            # 1. Collect all incoming tensors, and confirm there is incoming tensor
            mid = self._combineIncomingTensors(buildNo = buildNo)
            targetTensor = self._targetTensors = self.train._sourceTensors[self.lossDppKey]

            # 2. TensorFlow Graph building, using the layer name as the scope
            
            # 3. Consolidate merged input shape and target tensor shape
            fromShape = tuple(s for s in mid.shape)
            toShape = tuple(s for s in targetTensor.shape)
            
            # 4A.   If the shape is already matched, no conversion is needed.
            if (fromShape == toShape):
                self._predictionTensors = mid
            
            # 4B.   If the shape is different, futher modification is required.
            else:
                with tf.compat.v1.variable_scope(self.name, reuse=tf.compat.v1.AUTO_REUSE) as scope:
                    # 4B-1. Determine the from and to shape mapping, the first dimension should be the same based on the batch dimension.
                    #       If the target tensor has shape of [None] only, reshape as [None, 1] for weight multiplication.
                    if (len(toShape) == 1):
                        targetTensor = tf.reshape(targetTensor, [*Train.Variable.setAsReshape(toShape), 1])
                        toShape = tuple(s for s in targetTensor.shape)

                    # 4B-2. Determine the shared shape dimensions so as to flatten the previous layer. 
                    #       Supplement a dimension if the dimension of the lower dimensioned array has shape same with those dimensions of another array.
                    toAxis = 0
                    for d in range(0, min(len(fromShape), len(toShape))):
                        if (fromShape[d] != toShape[d]):
                            break
                        toAxis += 1
                    if (toAxis == len(fromShape)):
                        mid = tf.reshape(mid, [*Train.Variable.setAsReshape(fromShape), 1])

                    # 4B-3. Determine the hidden units and perform linear Transform.
                    toUnit = functools.reduce(lambda x,y: x*y, toShape[toAxis:], 1) if toAxis < len(toShape) else 1
                    linearTransformResults = self.linearTransform.buildOn(mid, axis = toAxis, toUnit = toUnit, defaultDevice = self.train.device)
                    mid = linearTransformResults.results
                    self._weights.extend(linearTransformResults.weights)
                
                    # 4B-4. If mutli-task dimension is over 1, reshape it:
                    if (len(toShape) != len(mid.shape)):
                        mid = tf.reshape(mid, [*Train.Variable.setAsReshape(toShape)])

                    self._predictionTensors = mid

            with tf.compat.v1.variable_scope(self.name, reuse=tf.compat.v1.AUTO_REUSE) as scope:
                # 7. Get loss function
                targetTensor = tf.cast(targetTensor, mid.dtype)
                colConfig = self.train.dppNodes[self.lossDppKey]
                if (self.loss == "MSE"):
                    if (not hasattr(colConfig, "circular")) or len(colConfig.circular) == 0:
                        mse = tf.reduce_mean(tf.math.squared_difference(targetTensor, mid))
                        tf.compat.v1.add_to_collection('losses', mse)
                    else:
                        for idx in range(0, colConfig.getShape()[1]):
                            if idx in colConfig.colToCircular:
                                cc = colConfig.circular[colConfig.colToCircular[idx]]
                                minV, maxV = cc.min, cc.max
                                rangeV = maxV - minV
                                midTmp = tf.math.mod((mid[:,idx] - minV), rangeV)
                                y2 = tf.math.mod((targetTensor[:,idx] - minV), rangeV)
                                midTmp = tf.minimum(tf.math.mod((midTmp-y2)+rangeV, rangeV), tf.math.mod((y2-midTmp)+rangeV, rangeV))
                                mse = tf.reduce_mean(tf.square(midTmp))
                                tf.compat.v1.add_to_collection('losses', mse)
                            else:
                                mse = tf.reduce_mean(tf.math.squared_difference(targetTensor[:,idx], mid[:,idx]))
                                tf.compat.v1.add_to_collection('losses', mse)
                else:
                    raise ValueError("Requested loss type is not supported.")
            self._built = True

    def partialEvaluate(self, allTestData, predictedData):
        '''
			Partial evaluate some predicted results, typically during batched tests. No action taken for this class.   --- UPDATED (Dexter) 20191024

            Parameters
            ------------------------------

            allTestData         `np.ndarray|list`    - A list of test data (actual result).

            allPredictedResults `np.ndarray|list`    - A list of predicted data (model output result).
        '''
        # Collect the test data and recover to the original range.
        testData = np.array(allTestData[self.lossDppKey])
        colConfig = self.train.dppNodes[self.lossDppKey]
        testData = self.recoverPredictedResults(testData)

        # Evaluate the result by accumulating the evaluation.
        compareCount = testData.size
        if (self.measurement == "MSE" or self.measurement == "PSNR"):
            if (not hasattr(colConfig, "circular")) or len(colConfig.circular) == 0:
                self._evalCumScore += np.sum(np.square(np.subtract(testData, predictedData)))
                self._evalTotal += compareCount
            elif (self.measurement != "PSNR"):
                for idx in range(0, colConfig.getShape()[1]):
                    if idx in colConfig.colToCircular:
                        cc = colConfig.circular[colConfig.colToCircular[idx]]
                        minV, maxV = cc.min,  cc.max
                        rangeV = maxV - minV
                        self._evalCumScore += np.sum(np.square(np.minimum(np.mod((testData[:,idx]-predictedData[:,idx])+rangeV, rangeV), np.mod((predictedData[:,idx]-testData[:,idx])+rangeV, rangeV))))
                    else:
                        self._evalCumScore += np.sum(np.square(np.subtract(testData[:,idx], predictedData[:,idx])))
                self._evalTotal += compareCount
            else:
                raise ValueError("PSNR is not supported for circular data.")
        elif (self.measurement == "SMAPE"):
            if (not hasattr(colConfig, "circular")) or len(colConfig.circular) == 0:
                self._evalCumScore += np.sum(np.abs(np.subtract(testData, predictedData))/((np.abs(testData) + np.abs(predictedData))/2))
                self._evalTotal += compareCount
            else:
                raise ValueError("SMAPE is not supported for circular data.")
    
    def getTestScore(self):
        '''
			Get the test score of the test results. No action taken for this class.   --- UPDATED (Dexter) 20181110

            Returns
            ------------------------------

            `float`     - The test score.
        '''
        if (self.measurement == "MSE" or self.measurement == "SMAPE"):
            return self._evalCumScore / self._evalTotal
        elif (self.measurement == "PSNR"):
            return 10 * math.log10((self.measurementOptions["max"]) ** 2 / (self._evalCumScore / self._evalTotal))

    def keras_build(self):
        target_geneator = self.batch_target(self.lossDppKey)
        loss_func = self.get_loss_function()
        return [target_geneator, loss_func]

    class batch_target():
        def __init__(self, lossDppKey):
            self.lossDppKey = lossDppKey
        
        def get_target(self, batch_source):
            return batch_source[self.lossDppKey]
 
    def keras_call_layer_build(self, reshape_output_name, output_reg, inputshape = None, buildNo = 0):
        '''
			Create keras layer code   --- UPDATED (kai Hsiang, Dexter) 20200120

            Parameters
            ------------------------------

            reshape_output_name     `str`                   - register name from auto reshape stage

            output_reg              `str`                   - register name
            
            inputshape              `list<tuple<int>>`      - list of input tensor shape.
                        
            Returns
            ------------------------------

            `list<str>, list<str>, <str>, list<tuple<int>>` - list of code, list of code, register name, Output shape
        '''
        call = ['self.%s_target = batch_source[\'%s\']'%(self.name, self.lossDppKey),]
        if (len(self._shape[buildNo]) == 1):
            self._shape[buildNo] = [None, 1] 
            string = 'self.%s_target = tf.keras.layers.Reshape((1,))(self.%s_target)'%(self.name, self.name)            
            call.append(string)
        call.append('self.%s_output = %s'%(self.name, reshape_output_name))
        return [],call, reshape_output_name, inputshape

    def keras_call_auto_reshape_build(self, input_shape, combine_output_name, output_reg, buildNo = 0):
        '''
			Create keras reshape layer code   --- UPDATED (kai Hsiang) 20190909

            Parameters
            ------------------------------

            inputshape              `list<tuple<int>>`          - list of input tensor shape.
            
            combine_output_name     `str`                       - register name from auto reshape stage

            output_reg              `str`                       - register name
                        
            Returns
            ------------------------------

            `list<str>, list<str>, <str>, list<tuple<int>>` - list of code, list of code, register name, Output shape
        '''
        layer_obj,call_obj = [],[]
        fromShape = list(input_shape)
        toShape = [x  if x != 'None' else None for x in self._shape[buildNo]]
        
        if combine_output_name != output_reg:
            A = output_reg
            B = combine_output_name
        else:
            A = output_reg
            B = output_reg
        output_name = combine_output_name    
        # 4A.   If the shape is already matched, no conversion is needed.
        if (fromShape == toShape):
            pass
            
        # 4B.   If the shape is different, futher modification is required.
        else:
            # 4B-1. Determine the from and to shape mapping, the first dimension should be the same based on the batch dimension.
            #       If the target tensor has shape of [None] only, reshape as [None, 1] for weight multiplication.
            if (len(toShape) == 1):
                toShape = [None, 1]

            # 4B-2. Determine the shared shape dimensions so as to flatten the previous layer. 
            #       Supplement a dimension if the dimension of the lower dimensioned array has shape same with those dimensions of another array.
            toAxis = 0
            for d in range(0, min(len(fromShape), len(toShape))):
                if (fromShape[d] != toShape[d]):
                    break
                toAxis += 1
            if (toAxis == len(fromShape)):
                string = 'self.%s_auto_reshape1 = tf.keras.layers.Reshape(('%(self.name)
                string += ','.join([str(int(x)) for x in from_shape[1:]])
                string += ',1))'
                layer_obj.append(string)
                string = '%s = self.%s_auto_reshape1(%s)'%(A, self.name, B)
                call_obj.append(string)
                B = A
                fromShape.append(1)

            # 4B-3. Determine the hidden units and perform linear Transform.
            toUnit = functools.reduce(lambda x,y: x*y, toShape[toAxis:], 1) if toAxis < len(toShape) else 1
            
            # auto reshape
            axis = toAxis
            print(axis)
            if axis == len(fromShape) - 1:
                pass
            elif axis == 1:
                new_shape = fromShape[0:axis]
                new_shape.append(functools.reduce(lambda x,y: x*y, fromShape[axis:], 1))
                string = 'self.%s_auto_reshape2 = tf.keras.layers.Flatten()'%(self.name)
                layer_obj.append(string)
                string = '%s = self.%s_auto_reshape2(%s)'%(A, self.name, B)
                call_obj.append(string)
                B = A
                fromShape = new_shape
            elif axis > 1 and axis < len(fromShape) - 1:
                new_shape = fromShape[0:axis]
                new_shape.append(functools.reduce(lambda x,y: x*y, fromShape[axis:], 1))
                string = 'self.%s_auto_reshape2 = tf.keras.layers.Reshape(('%(self.name)
                string += ','.join([str(int(x)) for x in new_shape[1:]])
                string += '))'
                layer_obj.append(string)
                string = '%s = self.%s_auto_reshape2(%s)'%(A, self.name, B)
                call_obj.append(string)
                B = A
                fromShape = new_shape
            else:
                raise ValueError("Linear Transformation axis should within the dimension of the incoming tensor.")


            layer = 'self.%s_auto_reshape3 = tf.keras.layers.Dense(%d, activation = \'relu\')'% \
                (self.name, toUnit)
            layer_obj.append(layer)
            call = '%s = self.%s_auto_reshape3(%s)'% (A,self.name,B)
            call_obj.append(call)
            B = A

                
            # 4B-4. If mutli-task dimension is over 1, reshape it:
            if (len(toShape) != len(fromShape)):
                string = 'self.%s_auto_reshape4 = tf.keras.layers.Reshape(('%(self.name)
                string += ','.join([str(int(x)) for x in toShape[1:]])
                string += '))'
                layer_obj.append(string)
                string = '%s = self.%s_auto_reshape4(%s)'%(A, self.name, B)
                call_obj.append(string)

            output_name = A
        
        return layer_obj, call_obj, output_name, toShape

    def keras_get_loss(self):
        '''
			Create tensorflow loss code   --- UPDATED (kai Hsiang) 20190909
                        
            Returns
            ------------------------------

            `str` - loss code
        '''
        # (TF 2.0) tf.squared_difference => tf.math.squared_difference
        return '%s_loss = tf.reduce_mean(tf.math.squared_difference(self.%s_target, tf.cast(self.%s_output, self.%s_target.dtype)))' %(self.name,self.name,self.name,self.name)
        
class ModelNode:
    """
        Module including different types of graph model nodes in a data model node.   --- RESERVED --- UPDATED (Dexter) 20190508
    """
    
    @staticmethod
    def createFromJSON(key: str, obj: Dict[str, Any], train: 'Train') -> 'ModelNode.Config':
        """
            Parse a previously saved object into a new @ModelNode.Layer.Config object. This will auto-determine the sub-class of the object, and pass the JSON object to the inner method to continue to parse.   --- UPDATED (Dexter) 20190508

            Parameters
            ------------------------------

            key     `str`           - Key for this model node object.

            obj     `Dict[str,Any]` - JSON representation of the model node object.

            train   `Train`         - The train of that the layers attach to.
            
            Returns
            ------------------------------

            `ModelNode.Config` - A @ModelNode.Config object.
        """
        # Parse the model node object.
        layer = getattr(ModelNode,ModelNode.Types.getName(obj["_nodeType"]) if obj["_nodeType"] else "Layer").createFromJSON(obj, train)

        # Set the model node to the train object; and return the model node object.
        train.modelNodes[key] = layer
        return layer
    
    @staticmethod
    def returnLayer(key: str, obj: Dict[str, Any], train: 'Train'):
        # Parse the model node object.
        layer = getattr(ModelNode,ModelNode.Types.getName(obj["_nodeType"]) if obj["_nodeType"] else "Layer").createFromJSON(obj, train)

        return layer

    
    @staticmethod
    def updateNodeName(nodeNameOrKey: str) -> str:
        """
            Replace old node name or keys of Ladder v19xx.   --- UPDATED (Dexter) 20191203

            Parameters
            ------------------------------

            nodeNameOrKey `str` - Old node name or key prior to NOM v2002.

            Returns
            ------------------------------

            `str` - New node name or key of Ladder from Ladder v2002.
        """
        return re.sub(r"\s","",nodeNameOrKey)

    Types = _ModelNodeTypes
    Config = _ModelNodeConfig
    ComputationalUnit = _ModelNodeComputationalUnit
    TFHub = _ModelNodeTFHub

    class Layer:
        """
            Module including different types of high-level Ladder model layer.   --- RESERVED --- UPDATED (Dexter) 20190508
        """
        class Types(Enumeration):
            """
                Enumeration managing the types of high-level Ladder model layer.   --- RESERVED --- UPDATED (Dexter) 20200105
            """
            # `int` - Abstract class representing a high level Ladder model layer.
            Config = 0
            # `int` - A collector layer, without any dimensional transformation from all the input layers.
            Collector = 1
            # `int` - A fully connected layer, aka dense layer, etc.
            FullyConnected = 2
            # `int` - A 2D convolutional layer, providing auto reshaping of data which are not 2D images.
            Convolution = 3
            # `int` - A 2D de-convolutional layer.
            Deconvolution = 4
            # `int` - A 1D convolutional layer, providing auto reshaping of data which are not 2D images.
            Convolution1D = 6
            # `int` - A 1D de-convolutional layer, providing auto reshaping of data which are not 2D images.
            Deconvolution1D = 7
            # `int` - A RMM layer.   ---- RESERVED
            RNN = 101
            # `int` - A GRU layer.   ---- RESERVED
            GRU = 102
            # `int` - A LSTM layer.   ---- RESERVED
            LSTM = 103
            # `int` - A bidirectional RMM layer.   ---- RESERVED
            BiRNN = 104
            # `int` - A bidirectional GRU layer.   ---- RESERVED
            BiGRU = 105
            # `int` - A bidirectional LSTM layer.   ---- RESERVED
            BiLSTM = 106
            # `int` - Class including different types of task layers.
            Task = 5

        @staticmethod
        def createFromJSON(obj: Dict[str, Any], train: 'Train') -> 'ModelNode.Layer.Config':
            """
                Parse a previously saved object into a new @ModelNode.Layer.Config object. This will auto-determine the sub-class of the object, and pass the JSON object to the inner method to continue to parse.   --- UPDATED (Dexter) 20190822

                Parameters
                ------------------------------

                obj     `Dict[str,Any]` - JSON object from Project file.

                train   `Train`         - The train of that the layers attach to.
                
                Returns
                ------------------------------

                - A @ModelNode.Layer.Config object.
            """
            if "_layerType" in obj:
                # If it's using the latest definition, it can create layers using the ModelNode structure.
                if (obj["_layerType"] == ModelNode.Layer.Types.Task.value):
                    lp = getattr(ModelNode.Layer.Task,ModelNode.Layer.Task.Types.getName(obj["_taskType"]))()
                else:
                    lp = getattr(ModelNode.Layer,ModelNode.Layer.Types.getName(obj["_layerType"]))()
            else:
                # Otherwise, use if logics to determine which type of layers to be created.
                layerUnits = obj["layerUnits"] if "layerUnits" in obj else None
                if (obj["_type"] == "Collector"):
                    lp = ModelNode.Layer.Collector()
                elif (obj["_type"] == "BasicLayer"):
                    lp = ModelNode.Layer.FullyConnected(layerUnits = layerUnits)
                elif (obj["_type"] == "CNNLayer"):
                    lp = ModelNode.Layer.Convolution(layerUnits = layerUnits)
                elif (obj["_type"] == "DCNNLayer"):
                    lp = ModelNode.Layer.Deconvolution(layerUnits = layerUnits)
                elif (obj["_type"] == "Classifier"):
                    lp = ModelNode.Layer.Task.Classifier()
                elif (obj["_type"] == "Group"):
                    lp = ModelNode.Layer.Group()
                elif (obj["_type"] == "Task"):
                    lp = ModelNode.Layer.Task.Task()
                elif (obj["_type"] == "Regressor"):
                    lp = ModelNode.Layer.Task.Regressor()
                else:
                    raise ValueError("Layer type not supported.")
            
            # Parse the layer profile.
            lp.parseJSON(obj, train)
            return lp
        
        Incoming = _ModelNodeLayerIncoming
        Output = _ModelNodeLayerOutput
        Config = _ModelNodeLayerConfig
        FullyConnected = _ModelNodeLayerFullyConnected
        Convolution = _ModelNodeLayerConvolution
        Deconvolution = _ModelNodeLayerDeconvolution
        Convolution1D = _ModelNodeLayerConvolution1D
        Deconvolution1D = _ModelNodeLayerDeconvolution1D
        Collector = _ModelNodeLayerCollector
        Group = _ModelNodeLayerGroup
        
        RNN = RNNLayer
        GRU = GRULayer
        LSTM = LSTMLayer
        BiRNN = BiRNNLayer
        BiGRU = BiGRULayer
        BiLSTM = BiLSTMLayer

        
        class Task:
            """
                Module including different types of high-level Ladder task layers.   --- RESERVED --- UPDATED (Dexter) 20190508
            """
            class Types(Enumeration):
                """
                    Enumeration managing the types of high-level Ladder task layer.   --- RESERVED --- UPDATED (Dexter) 20190508
                """
                # `int` - Abstract class representing a high level Ladder model layer. (Ref: @ModelNode.Layer.Config) 
                Config = 0
                # `int` - Class representing a NOM classifier (Ref: @ModelNode.Layer.Task.Classifier), to compare model predictions with discrete classes.
                Classifier = 1
                # `int` - Class representing a NOM regressor (Ref: @ModelNode.Layer.Task.Regressor), to compare model predictions with continous variables.
                Regressor = 2
                # `int` - Class representing a NOM Task (Ref: @ModelNode.Layer.Task.Task), to compare model predictions with other node (predict on).
                Task = 3
            
            Config = _ModelNodeLayerTaskConfig
            Classifier = _ModelNodeLayerTaskClassifier
            Regressor = _ModelNodeLayerTaskRegressor
            Task = _Task

class NOMTrainer(object):
    def __init__(self, config = {}):
        self.config = {}
        key_transfer = {'layerProfiles':'modelNodes', 'trainingProfiles':'buildConfigs'}
        for key, value in config.items():
            try:
                key = key_transfer[key]
            except:
                pass
            self.config[key] = value

        del config
        ####
        self.sources_idx = 0
        self.sources_idx_list = []
        self.input_idx = 0
        self.var = {}
        '''self.obj_dict = {'Convolution': CNNLayer, 'FullyConnected': BasicLayer,
        'Collector':Collector,
        'Deconvolution': DCNNLayer, 'FinalLayer': FinalLayer,
        'Classifier': Classifier, 'Regressor': Regressor,
        'TableSource': Source.Table, 'trainingProfiles': Train.BuildConfig, 'buildConfigs': Train.BuildConfig,
        'CSVSource': Source.CSV, 'ImageSource':Source.Image,
        'ComputationalUnit': ComputationalUnit.Config,'RNN':RNNLayer,'GRU':GRULayer,'LSTM':LSTMLayer,'BiRNN':BiRNNLayer,'BiGRU':BiGRULayer,'BiLSTM':BiLSTMLayer}
        '''
        # `Train` - A NOM train object.
        self.train = None

    @staticmethod
    def createFromFile(fileName: str) -> 'NOMTrainer':
        """
            Create an NOM Trainer object from a file.   --- UPDATED (Dexter) 20200410

            Parameters
            ------------------------------

            filName `str` - The JSON file of an NOM object to be read.

            Returns
            ------------------------------

            `NOMTrainer` - An NOM Trainer object.
        """
        with open(fileName, 'r', encoding = "utf-8") as f:
            config = f.read()#json.load(f)
        trainer = NOMTrainer()
        trainer.train = Train.createFromJSONString(config)
        return trainer
    
    @staticmethod
    def createFromJSONString_old(jsonStr: str) -> 'NOMTrainer':
        """
            Create an NOM Trainer object from a JSON String.   --- UPDATED 20190729

            Parameters
            ------------------------------

            jsonStr `str` - The JSON string of an NOM object.

            Returns
            ------------------------------

            `NOMTrainer` - An NOM Trainer object.
        """
        return NOMTrainer(json.loads(jsonStr))
    
    @staticmethod
    def createFromJSONString(jsonStr: str) -> 'Train':
        """
            Create an NOM Trainer object from a JSON String.   --- UPDATED (Dexter) 20200120

            Parameters
            ------------------------------

            jsonStr `str` - The JSON string of an NOM object.

            Returns
            ------------------------------

            `NOMTrainer` - An NOM Trainer object.
        """
        trainer = NOMTrainer()
        trainer.train = Train.createFromJSONString(jsonStr)
        return trainer

    def run(self, mode = 'execute2'):
        '''
            --- UPDATED (kai Hsiang, Dexter) 20200120
        '''
        if mode == 'execute':
            import os
            import sys
            nowPath = sys.argv[0]
            nowPathLength = len(nowPath)
            actualPath = nowPath[0:(max(nowPath.rfind('/'),0) or nowPathLength)][0:(max(nowPath.rfind('\\'),0) or 0)]
            if (len(actualPath)): os.chdir(actualPath)
            self.get_sources()
            self.get_config()
            self.get_train_object()
            self.get_layerProfiles()
            buildNo = 0
            optimizer = self.var['train'].buildConfigs[buildNo].optimizer
            batchsize = self.var['train'].sources[0].batchSize
            epochsize = self.var['train'].buildConfigs[buildNo].noOfEpoch
            self.var['train'].keras_fullModelTrain()
        elif mode == "execute2":
            self.train.keras_fullModelTrain2()
    
    def name_to_object(self, name):      
        try:
            return self.obj_dict[name]()
        except:
            print('object name not exist')
            exit()

    def get_sources(self):
        for source in self.config['sources']:
            ####### if source['_type'] == 'TableSource' or source['_type'] == 'CSVSource':
            if (source['_instanceClass'] in [Source.Types.Table.value, Source.Types.CSV.value] ):
                # create table object
                ####### if source['_type'] == 'TableSource':
                if (source['_instanceClass'] == Source.Types.Table.value):
                    #self.var['source' + str(self.sources_idx)] = self.name_to_object('TableSource')
                    self.var['source' + str(self.sources_idx)] = Source.Table()
                else:
                    ### Dexter:  Added encoding.
                    self.var['source' + str(self.sources_idx)] = Source.CSV(source['_path'], encoding=source['encoding'])
                # convert json attribute to object
                object_attr = ['colConfigs', 'dppNodes', '_oriData', 'ele', 'epochSize']
                ### colConfigs
                attr_colConfigs = {}
                for config in (source['colConfigs'] if "colConfigs" in source else source['dppNodes']):
                    key, value = config[0], config[1]
                    obj = DataPreprocessing.Node.createFromJSON(value)
                    attr_colConfigs[key] = obj
                ### _oriData
                ####### if source['_type'] == 'TableSource':
                if (source['_instanceClass'] == Source.Types.Table.value):
                    _oriData = np.array(source['_oriData'])
                    setattr(self.var['source' + str(self.sources_idx)], '_oriData', _oriData)
                ### set attribute
                setattr(self.var['source' + str(self.sources_idx)], 'dppNodes', attr_colConfigs)
                for key, value in source.items():
                    if key not in object_attr:
                        setattr(self.var['source' + str(self.sources_idx)], key, value)
                ##### Dexter: set transform, set circular output.
                for (dppKey, colConfig) in self.var['source' + str(self.sources_idx)].colConfigs.items():
                    if isinstance(colConfig, DataPreprocessing.Node.Columns):
                        for ti in colConfig.transformations:
                            self.var['source' + str(self.sources_idx)].setTransform(sourceDppKey = dppKey, cols = ti["cols"], scaleType = ti["scaleType"])
                        for ci in colConfig.circular:
                            self.var['source' + str(self.sources_idx)].setCircularOutput(sourceDppKey = dppKey, cols = ci["cols"], minV = ci["min"], maxV = ci["max"])
                self.var['source' + str(self.sources_idx)]._testRatio /= 100
            ####### elif source['_type'] == 'ImageSource':
            elif (source['_instanceClass'] == Source.Types.Image.value):
                object_attr = ['colConfigs', 'dppNodes', '_sourceType', 'coreDataDir', 'ele']
                self.var['source' + str(self.sources_idx)] = Source.Image(sourceType = source['_sourceType'],
                coreDataDir = source['coreDataDir'])
                attr_colConfigs = {}
                for config in (source['colConfigs'] if "colConfigs" in source else source['dppNodes']):
                    key, value = config[0], config[1]
                    obj = DataPreprocessing.Node.createFromJSON(value)
                    attr_colConfigs[key] = obj
                setattr(self.var['source' + str(self.sources_idx)], 'dppNodes', attr_colConfigs)
                for key, value in source.items():
                    if key not in object_attr:
                        setattr(self.var['source' + str(self.sources_idx)], key, value)
                self.var['source' + str(self.sources_idx)]._prepareItr()
            else:
                continue
            
            ####### if source['_type'] == 'TableSource' or source['_type'] == 'CSVSource':
            if (source['_instanceClass'] in [Source.Types.Table.value, Source.Types.CSV.value]):
                self.var['testSource' + str(self.sources_idx)] = self.var['source' + str(self.sources_idx)].splitTestDataset(
                test =self.var['source' + str(self.sources_idx)]._testRatio, shuffle = self.var['source' + str(self.sources_idx)].shuffle)
            else:
                self.var['testSource' + str(self.sources_idx)] = self.var['source' + str(self.sources_idx)].copyConfigAsNewSource(training = False)
                self.var['testSource' + str(self.sources_idx)]._prepareItr()
            self.sources_idx_list.append(self.sources_idx)
            self.sources_idx += 1

    def get_config(self):
        self.tp_idx = []
        for idx, tp in enumerate(self.config['buildConfigs']):
            self.tp_idx.append(idx)
            self.var['config' + str(idx)] = Train.BuildConfig.createFromJSON(tp)

    def get_train_object(self):
        if len(self.sources_idx_list):
            if len(self.sources_idx_list) == 1:
                slist = self.var['source%d'%(self.sources_idx_list[0])]
                tslist = self.var['testSource%d'%(self.sources_idx_list[0])]
                tplist = self.var['config0']
            else:
                slist = [self.var['source%d'%(x)] for x in self.sources_idx_list]
                tslist = [self.var['testSource%d'%(x)] for x in self.sources_idx_list]
                tplist = [self.var['config%d'%(x)] for x in self.tp_idx]
            self.var['train'] = Train(TimeHelper.getDateTimeStr(), source=slist, testSource=tslist,
            buildConfig=tplist, folder=self.config['folder'], restorePath=self.config['restorePath'], device=self.config['device'], logFreq=float(self.config['logFreq']),
            saveFreq=float(self.config['saveFreq']), testFreq=float(self.config['testFreq']), traceFreq=float(self.config['traceFreq']), weightLogFreq=float(self.config['weightLogFreq']), filterFreq=float(self.config['filterFreq']),
            traceRecord=self.config['traceRecord'])
            
    def get_layerProfiles(self):
        self.layerProfiles_obj = {}
        for lpf in self.config['modelNodes']:
            layer_name, layer_config = lpf[0], lpf[1]
            layer_name = layer_name.replace(' ','')
            if layer_config['_nodeType'] == ModelNode.Types.Layer.value:
                # incoming config
                class_name = IncomingConfig.Types(layer_config['incomingConfig']['_instanceClass']).name
                Dim_class_name = IncomingConfig.MergeDimMethods(layer_config['incomingConfig']['mergeDim'])
                incomingConfig = getattr(IncomingConfig, class_name)(mergeDim = Dim_class_name,
                coreNode = layer_config['incomingConfig']['coreNode'], axis = int(layer_config['incomingConfig']['axis']))
                # output config
                try:
                    class_name = ModelNode.Layer.Output.Types(layer_config['outputConfig']['_instanceClass']).name
                    outputConfig = getattr(OutputConfig, class_name)()
                except:
                    class_name = ModelNode.Layer.Output.Types(ModelNode.Layer.Output.Types.Default.value).name
                    outputConfig = getattr(OutputConfig, class_name)()
                # get transform
                if layer_config['linearTransform'] is not None:
                    if len(layer_config['linearTransform']) == 2:
                        weightConfig = VarConfig()
                        class_name = layer_config['linearTransform']['weightConfig']['initializer']['_type']
                        initializer = getattr(VarConfig, class_name)()
                        # set initializer
                        for key, value in layer_config['linearTransform']['weightConfig']['initializer'].items():
                            if key != '_type':
                                try:
                                    setattr(initializer, key, value)
                                except:
                                    print('set attribute error')
                                    exit()
                        setattr(weightConfig, 'initializer', initializer)
                        # set weight
                        for key, value in layer_config['linearTransform']['weightConfig'].items():
                            setattr(weightConfig, key, value)

                        biasConfig = VarConfig()
                        class_name = layer_config['linearTransform']['biasConfig']['initializer']['_type']
                        initializer = getattr(VarConfig, class_name)()
                        # set initializer
                        for key, value in layer_config['linearTransform']['biasConfig']['initializer'].items():
                            if key != '_type':
                                try:
                                    setattr(initializer, key, value)
                                except:
                                    print('setattr error')
                                    exit()
                        setattr(biasConfig, 'initializer', initializer)
                        # set weight
                        for key, value in layer_config['linearTransform']['biasConfig'].items():
                            setattr(biasConfig, key, value)
                    else:
                        config_list = list()
                        if (layer_config['_layerType'] == ModelNode.Layer.Types.RNN.value)|(layer_config['_layerType'] == ModelNode.Layer.Types.BiRNN.value):
                            #RNN&BiRNN
                            config_list_index = ['weightConfig','recurrentWeightConfig','biasConfig']
                        elif (layer_config['_layerType'] == ModelNode.Layer.Types.GRU.value)|(layer_config['_layerType'] == ModelNode.Layer.Types.BiGRU.value):
                            #GRU&BiGRU
                            config_list_index = ['resetGateWeightConfig','updateGateWeightConfig','stateCandidateWeightConfig','resetGateRecurrentWeightConfig',
                                        'updateGateRecurrentWeightConfig','stateCandidateRecurrentWeightConfig','resetGateBiasConfig','updateGateBiasConfig','stateCandidateBiasConfig']
                        else:
                            #LSTM%BiLSTM
                            config_list_index = ['inputGateWeightConfig','forgetGateWeightConfig','outputGateWeightConfig','memoryWeightConfig','inputGateRecurrentWeightConfig',
                                        'forgetGateRecurrentWeightConfig','outputGateRecurrentWeightConfig','memoryRecurrentWeightConfig','inputGatePeepholeWeightConfig',
                                        'forgetGatePeepholeWeightConfig','outputGatePeepholeWeightConfig','inputGateBiasConfig','forgetGateBiasConfig','outputGateBiasConfig',
                                        'memoryBiasConfig']
                        for index in config_list_index:
                            tempConfig = VarConfig()
                            class_name = layer_config['linearTransform'][index]['initializer']['_type']
                            initializer = getattr(VarConfig, class_name)()
                            # set initializer
                            for key, value in layer_config['linearTransform'][index]['initializer'].items():
                                if key != '_type':
                                    try:
                                        setattr(initializer, key, value)
                                    except:
                                        print('setattr error')
                                        exit()
                            setattr(tempConfig, 'initializer', initializer)
                            # set weight
                            for key, value in layer_config['linearTransform'][index].items():
                                setattr(tempConfig, key, value)
                            config_list.append(tempConfig)


            if layer_config['_nodeType'] == ModelNode.Types.ComputationalUnit.value:
                self.layerProfiles_obj[layer_name] = self.name_to_object('ComputationalUnit')
                layer_config['_final'] = False
            else:
                layer_class_name = ModelNode.Layer.Types(layer_config['_layerType']).name
                if layer_class_name == 'Task':
                    layer_class_name = ModelNode.Layer.Task.Types(layer_config['_taskType']).name
                self.layerProfiles_obj[layer_name] = self.name_to_object(layer_class_name)
            
            # setattr(use for loop)
            special_parm = ['incomingConfig', 'outputConfig', 'linearTransform','fromNode','toNode','fromSource','_order','name']
            for attr, value in layer_config.items():
                if attr not in special_parm:
                    if value == 'None':
                        value = None
                    setattr(self.layerProfiles_obj[layer_name], attr, value)
                if attr == 'name':
                    setattr(self.layerProfiles_obj[layer_name], attr, value.replace(' ',''))
                if attr == 'fromSource':
                    Source_dict = {}
                    for build_no, source_list in enumerate(value):
                        Source_dict[build_no] = []
                        for s in source_list:
                            Source_dict[build_no].append((s[0],s[1]))
                    setattr(self.layerProfiles_obj[layer_name], attr, Source_dict.copy())
                if attr == '_order':
                    self.layerProfiles_obj[layer_name]._order = {}
                    for build_no, order_list in enumerate(layer_config['_order']):
                        self.layerProfiles_obj[layer_name]._order[build_no] = order_list
            if layer_config['_nodeType'] == 1:
                setattr(self.layerProfiles_obj[layer_name], 'keras_initializer_info', layer_config['linearTransform'])
                setattr(self.layerProfiles_obj[layer_name], 'train', self.var['train'])
                setattr(self.layerProfiles_obj[layer_name], 'incomingConfig', incomingConfig)
                setattr(self.layerProfiles_obj[layer_name], 'outputConfig', outputConfig)

                if len(layer_config['linearTransform']) == 2:
                    setattr(self.layerProfiles_obj[layer_name], 'linearTransform', LinearTransformConfig(weightConfig = weightConfig, biasConfig = biasConfig))
                else:
                    if (layer_config['_layerType'] == ModelNode.Layer.Types.RNN.value)|(layer_config['_layerType'] == ModelNode.Layer.Types.BiRNN.value):
                        #RNN&BiRNN
                        weightConfigs = config_list[0]
                        recurrentWeightConfigs = config_list[1]
                        biasConfigs = config_list[2]
                        setattr(self.layerProfiles_obj[layer_name], 'linearTransform', RNNConfig(weightConfigs = weightConfigs, recurrentWeightConfigs = recurrentWeightConfigs, 
                                biasConfigs = biasConfigs))
                    
                    elif (layer_config['_layerType'] == ModelNode.Layer.Types.GRU.value)|(layer_config['_layerType'] == ModelNode.Layer.Types.BiGRU.value):
                        #GRU&BiGRU
                        weightConfigs = GRUConfig.GRUVarConfigs(config_list[0],config_list[1],config_list[2])
                        recurrentWeightConfigs = GRUConfig.GRUVarConfigs(config_list[3],config_list[4],config_list[5])
                        biasConfigs = GRUConfig.GRUVarConfigs(config_list[6],config_list[7],config_list[8])
                        setattr(self.layerProfiles_obj[layer_name], 'linearTransform', GRUConfig(weightConfigs = weightConfigs, recurrentWeightConfigs = recurrentWeightConfigs, 
                                biasConfigs = biasConfigs))
                    else:
                        #LSTM%BiLSTM
                        weightConfigs = LSTMConfig.LSTMVarConfigs(config_list[0],config_list[1],config_list[2],config_list[3])
                        recurrentWeightConfigs = LSTMConfig.LSTMVarConfigs(config_list[4],config_list[5],config_list[6],config_list[7]) 
                        peepholeWeightConfigs = LSTMConfig.LSTMVarConfigs(config_list[8],config_list[9],config_list[10],None)
                        biasConfigs = LSTMConfig.LSTMVarConfigs(config_list[11],config_list[12],config_list[13],config_list[14])
                        setattr(self.layerProfiles_obj[layer_name], 'linearTransform', LSTMConfig(weightConfigs = weightConfigs, recurrentWeightConfigs = recurrentWeightConfigs, 
                                peepholeWeightConfigs = peepholeWeightConfigs, biasConfigs = biasConfigs))

        # set fromNode , toNode
        for lpf in self.config['modelNodes']:
            layer_name, layer_config = lpf[0], lpf[1]
            layer_name = layer_name.replace(' ','')
            self.layerProfiles_obj[layer_name].fromNode = {}
            for build_no, node_list in enumerate(layer_config['fromNode']):
                layer_pointer = []
                for node in node_list:
                    node = node.replace(' ','')
                    layer_pointer.append(self.layerProfiles_obj[node])
                self.layerProfiles_obj[layer_name].fromNode[build_no] = layer_pointer
            self.layerProfiles_obj[layer_name].toNode = {}
            for build_no, node_list in enumerate(layer_config['toNode']):
                layer_pointer = []
                for node in node_list:
                    node = node.replace(' ','')
                    layer_pointer.append(self.layerProfiles_obj[node])
                self.layerProfiles_obj[layer_name].toNode[build_no] = layer_pointer
        self.var['train'].modelNodes = self.layerProfiles_obj


import sys, json
nowPath = sys.argv[0]
nowPathLength = len(nowPath)
actualPath = nowPath[0:(max(nowPath.rfind('/'),0) or nowPathLength)][0:(max(nowPath.rfind('\\'),0) or 0)]
if (len(actualPath)): os.chdir(actualPath)

train = NOMTrainer.createFromJSONString("{\"trainName\":\"train\",\"folder\":\"D:/tmp/\",\"testDatasetType\":1,\"testRatio\":0.2,\"testShuffle\":true,\"logFreq\":100,\"saveFreq\":0,\"testFreq\":500,\"testLogger\":null,\"weightLogFreq\":0,\"traceFreq\":300,\"filterFreq\":0,\"device\":\"/gpu:0\",\"dppNodes\":[[\"input\",{\"_instanceClass\":2,\"source\":0,\"crops\":[],\"sourceCol\":\"None:None\",\"_epochSize\":null,\"_getDataMode\":\"Propagate\",\"outputset\":\"input\",\"dtype\":\"tf.float32\",\"_order\":0,\"_shape\":[32,32,3],\"transformations\":[{\"_instanceClass\":3,\"_method\":2,\"_requirePreExtraction\":false,\"colSel\":\"None:None\",\"flipDirection\":1},{\"_instanceClass\":3,\"_method\":7,\"_requirePreExtraction\":false,\"colSel\":\"None:None\",\"maxDelta\":0.2},{\"_instanceClass\":3,\"_method\":5,\"_requirePreExtraction\":false,\"colSel\":\"None:None\",\"lower\":0.8,\"upper\":1.2},{\"_instanceClass\":2,\"_method\":1,\"_requirePreExtraction\":false,\"colSel\":\"None:None\"}],\"preprocessInBatch\":false,\"_dataShape\":[\"None\",32,32,3],\"_key\":\"input\"}],[\"target\",{\"_instanceClass\":1,\"source\":0,\"crops\":[],\"sourceCol\":\"None:None\",\"_epochSize\":null,\"_getDataMode\":\"Propagate\",\"outputset\":\"target\",\"dtype\":\"tf.int64\",\"_order\":0,\"_shape\":[1],\"transformations\":[],\"preprocessInBatch\":true,\"_dataShape\":[\"None\",1],\"_key\":\"target\",\"oneHotColumns\":[],\"circular\":[],\"_classCount\":10}]],\"modelNodes\":[[\"Keras Conv2D1\",{\"_nodeType\":2,\"name\":\"Keras Conv2D1\",\"cuzName\":\"\",\"fromSource\":[[\"input\"]],\"fromNode\":[[]],\"toNode\":[[\"Keras MaxPooling2D3\"]],\"_shape\":[[\"None\",28,28,6]],\"_order\":[1],\"_ltTemp\":\"$$$ 1\",\"_lt\":\"Keras Conv2D\",\"_langMap\":[[\"en-US\",\"Keras Conv2D\"],[\"zh-TW\",\"Keras 2D 卷積層\"]],\"_inputCollections\":[],\"_outputTensors\":[],\"_built\":false,\"_weights\":[],\"weightLogging\":true,\"_unitNamespace\":\"ModelNode.ComputationalUnit.KerasLayers.Conv2D\",\"_tfFunctionName\":\"tf.keras.layers.Conv2D\",\"_tfParams\":[[0,{\"defaultVal\":150,\"val\":6,\"pyArgumentValue\":\"6\",\"_langMap\":\"propHiddenSize\",\"required\":true,\"_paramNamespace\":\"ModelNode.ComputationalUnit.Param.Number\",\"core\":true,\"min\":1,\"max\":null,\"isInteger\":true,\"isCircular\":false}],[1,{\"defaultVal\":\"(3, 3)\",\"val\":\"(5, 5)\",\"pyArgumentValue\":\"(5, 5)\",\"_langMap\":\"propFilterWidth\",\"required\":true,\"_paramNamespace\":\"ModelNode.ComputationalUnit.Param.Tuple\",\"core\":true,\"length\":2}],[\"strides\",{\"defaultVal\":\"(1, 1)\",\"val\":\"(1, 1)\",\"pyArgumentValue\":\"(1, 1)\",\"_langMap\":\"propStride\",\"required\":false,\"_paramNamespace\":\"ModelNode.ComputationalUnit.Param.Tuple\",\"core\":false,\"length\":2}],[\"padding\",{\"defaultVal\":\"valid\",\"val\":\"valid\",\"_langMap\":\"propCovPadding\",\"required\":false,\"_paramNamespace\":\"ModelNode.ComputationalUnit.Param.Padding\",\"core\":false,\"options\":[[\"valid\",\"Valid\"],[\"same\",\"Same\"]],\"pyArgumentValue\":\"'valid'\"}],[\"dilation_rate\",{\"defaultVal\":\"(1, 1)\",\"val\":\"(1, 1)\",\"pyArgumentValue\":\"(1, 1)\",\"_langMap\":\"propDilation\",\"required\":false,\"_paramNamespace\":\"ModelNode.ComputationalUnit.Param.Tuple\",\"core\":false,\"length\":2}],[\"activation\",{\"defaultVal\":1,\"val\":3,\"pyArgumentValue\":\"tf.nn.tanh\",\"_langMap\":\"propActv\",\"required\":false,\"_paramNamespace\":\"ModelNode.ComputationalUnit.Param.Activations\",\"core\":false}],[\"use_bias\",{\"defaultVal\":true,\"val\":true,\"pyArgumentValue\":\"True\",\"_langMap\":\"propUseBias\",\"required\":false,\"_paramNamespace\":\"ModelNode.ComputationalUnit.Param.Boolean\",\"core\":false}],[\"kernel_initializer\",{\"defaultVal\":{\"_type\":\"GlorotUniform\"},\"val\":{\"_type\":\"GlorotUniform\"},\"pyArgumentValue\":\"initializers.glorot_uniform()\",\"_langMap\":\"propKernelInit\",\"required\":false,\"_paramNamespace\":\"ModelNode.ComputationalUnit.Param.Initializers\",\"core\":false}],[\"bias_initializer\",{\"defaultVal\":{\"_type\":\"Zeros\"},\"val\":{\"_type\":\"Zeros\"},\"pyArgumentValue\":\"initializers.zeros()\",\"_langMap\":\"propBiasInit\",\"required\":false,\"_paramNamespace\":\"ModelNode.ComputationalUnit.Param.Initializers\",\"core\":false}],[\"kernel_regularizer\",{\"defaultVal\":\"None\",\"val\":\"None\",\"_langMap\":\"propKernelReg\",\"required\":false,\"_paramNamespace\":\"ModelNode.ComputationalUnit.Param.KerasRegularizers\",\"core\":false,\"options\":[[\"None\",\"None\"],[\"l1\",\"propL1\"],[\"l2\",\"propL2\"]],\"pyArgumentValue\":\"None\"}],[\"bias_regularizer\",{\"defaultVal\":\"None\",\"val\":\"None\",\"_langMap\":\"propBiasReg\",\"required\":false,\"_paramNamespace\":\"ModelNode.ComputationalUnit.Param.KerasRegularizers\",\"core\":false,\"options\":[[\"None\",\"None\"],[\"l1\",\"propL1\"],[\"l2\",\"propL2\"]],\"pyArgumentValue\":\"None\"}],[\"activity_regularizer\",{\"defaultVal\":\"None\",\"val\":\"None\",\"_langMap\":\"actvReg\",\"required\":false,\"_paramNamespace\":\"ModelNode.ComputationalUnit.Param.KerasRegularizers\",\"core\":false,\"options\":[[\"None\",\"None\"],[\"l1\",\"propL1\"],[\"l2\",\"propL2\"]],\"pyArgumentValue\":\"None\"}],[\"kernel_constraint\",{\"defaultVal\":\"None\",\"val\":\"None\",\"_langMap\":\"propKernelConstraint\",\"required\":false,\"_paramNamespace\":\"ModelNode.ComputationalUnit.Param.KerasConstraints\",\"core\":false,\"options\":[[\"None\",\"None\"],[\"max_norm\",\"propMaxNorm\"],[\"min_max_norm\",\"propMinMaxNorm\"],[\"unit_norm\",\"propUnitNorm\"],[\"non_neg\",\"propNonNeg\"]],\"pyArgumentValue\":\"None\"}],[\"bias_constraint\",{\"defaultVal\":\"None\",\"val\":\"None\",\"_langMap\":\"propBiasConstraint\",\"required\":false,\"_paramNamespace\":\"ModelNode.ComputationalUnit.Param.KerasConstraints\",\"core\":false,\"options\":[[\"None\",\"None\"],[\"max_norm\",\"propMaxNorm\"],[\"min_max_norm\",\"propMinMaxNorm\"],[\"unit_norm\",\"propUnitNorm\"],[\"non_neg\",\"propNonNeg\"]],\"pyArgumentValue\":\"None\"}]],\"_tfMaxIncomingCount\":1,\"_tfTrainingStage\":false,\"_tfInit\":true,\"_isBigLayer\":false,\"_tfIncomingOrder\":\"\",\"_tfList\":true,\"ele\":{},\"forLoss\":false}],[\"Keras MaxPooling2D3\",{\"_nodeType\":2,\"name\":\"Keras MaxPooling2D3\",\"cuzName\":\"\",\"fromSource\":[[]],\"fromNode\":[[\"Keras Conv2D1\"]],\"toNode\":[[\"Keras Conv2D4\"]],\"_shape\":[[\"None\",14,14,6]],\"_order\":[2],\"_ltTemp\":\"$$$ 3\",\"_lt\":\"Keras MaxPooling2D\",\"_langMap\":[[\"en-US\",\"Keras MaxPooling2D\"],[\"zh-TW\",\"Keras 2D 最大池化層\"]],\"_inputCollections\":[],\"_outputTensors\":[],\"_built\":false,\"_weights\":[],\"weightLogging\":true,\"_unitNamespace\":\"ModelNode.ComputationalUnit.KerasLayers.MaxPooling2D\",\"_tfFunctionName\":\"tf.keras.layers.MaxPooling2D\",\"_tfParams\":[[\"pool_size\",{\"defaultVal\":\"(2, 2)\",\"val\":\"(2, 2)\",\"pyArgumentValue\":\"(2, 2)\",\"_langMap\":\"propPoolSize\",\"required\":false,\"_paramNamespace\":\"ModelNode.ComputationalUnit.Param.Tuple\",\"core\":false,\"length\":2}],[\"strides\",{\"defaultVal\":\"None\",\"val\":\"None\",\"pyArgumentValue\":\"None\",\"_langMap\":\"propStride\",\"required\":false,\"_paramNamespace\":\"ModelNode.ComputationalUnit.Param.Tuple\",\"core\":false,\"length\":2}],[\"padding\",{\"defaultVal\":\"valid\",\"val\":\"valid\",\"_langMap\":\"propCovPadding\",\"required\":false,\"_paramNamespace\":\"ModelNode.ComputationalUnit.Param.Padding\",\"core\":false,\"options\":[[\"valid\",\"Valid\"],[\"same\",\"Same\"]],\"pyArgumentValue\":\"'valid'\"}]],\"_tfMaxIncomingCount\":1,\"_tfTrainingStage\":false,\"_tfInit\":true,\"_isBigLayer\":false,\"_tfIncomingOrder\":\"\",\"_tfList\":true,\"ele\":{},\"forLoss\":false}],[\"Keras Conv2D4\",{\"_nodeType\":2,\"name\":\"Keras Conv2D4\",\"cuzName\":\"\",\"fromSource\":[[]],\"fromNode\":[[\"Keras MaxPooling2D3\"]],\"toNode\":[[\"Keras MaxPooling2D5\"]],\"_shape\":[[\"None\",10,10,16]],\"_order\":[3],\"_ltTemp\":\"$$$ 4\",\"_lt\":\"Keras Conv2D\",\"_langMap\":[[\"en-US\",\"Keras Conv2D\"],[\"zh-TW\",\"Keras 2D 卷積層\"]],\"_inputCollections\":[],\"_outputTensors\":[],\"_built\":false,\"_weights\":[],\"weightLogging\":true,\"_unitNamespace\":\"ModelNode.ComputationalUnit.KerasLayers.Conv2D\",\"_tfFunctionName\":\"tf.keras.layers.Conv2D\",\"_tfParams\":[[0,{\"defaultVal\":150,\"val\":16,\"pyArgumentValue\":\"16\",\"_langMap\":\"propHiddenSize\",\"required\":true,\"_paramNamespace\":\"ModelNode.ComputationalUnit.Param.Number\",\"core\":true,\"min\":1,\"max\":null,\"isInteger\":true,\"isCircular\":false}],[1,{\"defaultVal\":\"(3, 3)\",\"val\":\"(5, 5)\",\"pyArgumentValue\":\"(5, 5)\",\"_langMap\":\"propFilterWidth\",\"required\":true,\"_paramNamespace\":\"ModelNode.ComputationalUnit.Param.Tuple\",\"core\":true,\"length\":2}],[\"strides\",{\"defaultVal\":\"(1, 1)\",\"val\":\"(1, 1)\",\"pyArgumentValue\":\"(1, 1)\",\"_langMap\":\"propStride\",\"required\":false,\"_paramNamespace\":\"ModelNode.ComputationalUnit.Param.Tuple\",\"core\":false,\"length\":2}],[\"padding\",{\"defaultVal\":\"valid\",\"val\":\"valid\",\"_langMap\":\"propCovPadding\",\"required\":false,\"_paramNamespace\":\"ModelNode.ComputationalUnit.Param.Padding\",\"core\":false,\"options\":[[\"valid\",\"Valid\"],[\"same\",\"Same\"]],\"pyArgumentValue\":\"'valid'\"}],[\"dilation_rate\",{\"defaultVal\":\"(1, 1)\",\"val\":\"(1, 1)\",\"pyArgumentValue\":\"(1, 1)\",\"_langMap\":\"propDilation\",\"required\":false,\"_paramNamespace\":\"ModelNode.ComputationalUnit.Param.Tuple\",\"core\":false,\"length\":2}],[\"activation\",{\"defaultVal\":1,\"val\":3,\"pyArgumentValue\":\"tf.nn.tanh\",\"_langMap\":\"propActv\",\"required\":false,\"_paramNamespace\":\"ModelNode.ComputationalUnit.Param.Activations\",\"core\":false}],[\"use_bias\",{\"defaultVal\":true,\"val\":true,\"pyArgumentValue\":\"True\",\"_langMap\":\"propUseBias\",\"required\":false,\"_paramNamespace\":\"ModelNode.ComputationalUnit.Param.Boolean\",\"core\":false}],[\"kernel_initializer\",{\"defaultVal\":{\"_type\":\"GlorotUniform\"},\"val\":{\"_type\":\"GlorotUniform\"},\"pyArgumentValue\":\"initializers.glorot_uniform()\",\"_langMap\":\"propKernelInit\",\"required\":false,\"_paramNamespace\":\"ModelNode.ComputationalUnit.Param.Initializers\",\"core\":false}],[\"bias_initializer\",{\"defaultVal\":{\"_type\":\"Zeros\"},\"val\":{\"_type\":\"Zeros\"},\"pyArgumentValue\":\"initializers.zeros()\",\"_langMap\":\"propBiasInit\",\"required\":false,\"_paramNamespace\":\"ModelNode.ComputationalUnit.Param.Initializers\",\"core\":false}],[\"kernel_regularizer\",{\"defaultVal\":\"None\",\"val\":\"None\",\"_langMap\":\"propKernelReg\",\"required\":false,\"_paramNamespace\":\"ModelNode.ComputationalUnit.Param.KerasRegularizers\",\"core\":false,\"options\":[[\"None\",\"None\"],[\"l1\",\"propL1\"],[\"l2\",\"propL2\"]],\"pyArgumentValue\":\"None\"}],[\"bias_regularizer\",{\"defaultVal\":\"None\",\"val\":\"None\",\"_langMap\":\"propBiasReg\",\"required\":false,\"_paramNamespace\":\"ModelNode.ComputationalUnit.Param.KerasRegularizers\",\"core\":false,\"options\":[[\"None\",\"None\"],[\"l1\",\"propL1\"],[\"l2\",\"propL2\"]],\"pyArgumentValue\":\"None\"}],[\"activity_regularizer\",{\"defaultVal\":\"None\",\"val\":\"None\",\"_langMap\":\"actvReg\",\"required\":false,\"_paramNamespace\":\"ModelNode.ComputationalUnit.Param.KerasRegularizers\",\"core\":false,\"options\":[[\"None\",\"None\"],[\"l1\",\"propL1\"],[\"l2\",\"propL2\"]],\"pyArgumentValue\":\"None\"}],[\"kernel_constraint\",{\"defaultVal\":\"None\",\"val\":\"None\",\"_langMap\":\"propKernelConstraint\",\"required\":false,\"_paramNamespace\":\"ModelNode.ComputationalUnit.Param.KerasConstraints\",\"core\":false,\"options\":[[\"None\",\"None\"],[\"max_norm\",\"propMaxNorm\"],[\"min_max_norm\",\"propMinMaxNorm\"],[\"unit_norm\",\"propUnitNorm\"],[\"non_neg\",\"propNonNeg\"]],\"pyArgumentValue\":\"None\"}],[\"bias_constraint\",{\"defaultVal\":\"None\",\"val\":\"None\",\"_langMap\":\"propBiasConstraint\",\"required\":false,\"_paramNamespace\":\"ModelNode.ComputationalUnit.Param.KerasConstraints\",\"core\":false,\"options\":[[\"None\",\"None\"],[\"max_norm\",\"propMaxNorm\"],[\"min_max_norm\",\"propMinMaxNorm\"],[\"unit_norm\",\"propUnitNorm\"],[\"non_neg\",\"propNonNeg\"]],\"pyArgumentValue\":\"None\"}]],\"_tfMaxIncomingCount\":1,\"_tfTrainingStage\":false,\"_tfInit\":true,\"_isBigLayer\":false,\"_tfIncomingOrder\":\"\",\"_tfList\":true,\"ele\":{},\"forLoss\":false}],[\"Keras MaxPooling2D5\",{\"_nodeType\":2,\"name\":\"Keras MaxPooling2D5\",\"cuzName\":\"\",\"fromSource\":[[]],\"fromNode\":[[\"Keras Conv2D4\"]],\"toNode\":[[\"Keras Conv2D6\"]],\"_shape\":[[\"None\",5,5,16]],\"_order\":[4],\"_ltTemp\":\"$$$ 5\",\"_lt\":\"Keras MaxPooling2D\",\"_langMap\":[[\"en-US\",\"Keras MaxPooling2D\"],[\"zh-TW\",\"Keras 2D 最大池化層\"]],\"_inputCollections\":[],\"_outputTensors\":[],\"_built\":false,\"_weights\":[],\"weightLogging\":true,\"_unitNamespace\":\"ModelNode.ComputationalUnit.KerasLayers.MaxPooling2D\",\"_tfFunctionName\":\"tf.keras.layers.MaxPooling2D\",\"_tfParams\":[[\"pool_size\",{\"defaultVal\":\"(2, 2)\",\"val\":\"(2, 2)\",\"pyArgumentValue\":\"(2, 2)\",\"_langMap\":\"propPoolSize\",\"required\":false,\"_paramNamespace\":\"ModelNode.ComputationalUnit.Param.Tuple\",\"core\":false,\"length\":2}],[\"strides\",{\"defaultVal\":\"None\",\"val\":\"None\",\"pyArgumentValue\":\"None\",\"_langMap\":\"propStride\",\"required\":false,\"_paramNamespace\":\"ModelNode.ComputationalUnit.Param.Tuple\",\"core\":false,\"length\":2}],[\"padding\",{\"defaultVal\":\"valid\",\"val\":\"valid\",\"_langMap\":\"propCovPadding\",\"required\":false,\"_paramNamespace\":\"ModelNode.ComputationalUnit.Param.Padding\",\"core\":false,\"options\":[[\"valid\",\"Valid\"],[\"same\",\"Same\"]],\"pyArgumentValue\":\"'valid'\"}]],\"_tfMaxIncomingCount\":1,\"_tfTrainingStage\":false,\"_tfInit\":true,\"_isBigLayer\":false,\"_tfIncomingOrder\":\"\",\"_tfList\":true,\"ele\":{},\"forLoss\":false}],[\"Keras Conv2D6\",{\"_nodeType\":2,\"name\":\"Keras Conv2D6\",\"cuzName\":\"\",\"fromSource\":[[]],\"fromNode\":[[\"Keras MaxPooling2D5\"]],\"toNode\":[[\"Keras Flatten7\"]],\"_shape\":[[\"None\",1,1,120]],\"_order\":[5],\"_ltTemp\":\"$$$ 6\",\"_lt\":\"Keras Conv2D\",\"_langMap\":[[\"en-US\",\"Keras Conv2D\"],[\"zh-TW\",\"Keras 2D 卷積層\"]],\"_inputCollections\":[],\"_outputTensors\":[],\"_built\":false,\"_weights\":[],\"weightLogging\":true,\"_unitNamespace\":\"ModelNode.ComputationalUnit.KerasLayers.Conv2D\",\"_tfFunctionName\":\"tf.keras.layers.Conv2D\",\"_tfParams\":[[0,{\"defaultVal\":150,\"val\":120,\"pyArgumentValue\":\"120\",\"_langMap\":\"propHiddenSize\",\"required\":true,\"_paramNamespace\":\"ModelNode.ComputationalUnit.Param.Number\",\"core\":true,\"min\":1,\"max\":null,\"isInteger\":true,\"isCircular\":false}],[1,{\"defaultVal\":\"(3, 3)\",\"val\":\"(5, 5)\",\"pyArgumentValue\":\"(5, 5)\",\"_langMap\":\"propFilterWidth\",\"required\":true,\"_paramNamespace\":\"ModelNode.ComputationalUnit.Param.Tuple\",\"core\":true,\"length\":2}],[\"strides\",{\"defaultVal\":\"(1, 1)\",\"val\":\"(1, 1)\",\"pyArgumentValue\":\"(1, 1)\",\"_langMap\":\"propStride\",\"required\":false,\"_paramNamespace\":\"ModelNode.ComputationalUnit.Param.Tuple\",\"core\":false,\"length\":2}],[\"padding\",{\"defaultVal\":\"valid\",\"val\":\"valid\",\"_langMap\":\"propCovPadding\",\"required\":false,\"_paramNamespace\":\"ModelNode.ComputationalUnit.Param.Padding\",\"core\":false,\"options\":[[\"valid\",\"Valid\"],[\"same\",\"Same\"]],\"pyArgumentValue\":\"'valid'\"}],[\"dilation_rate\",{\"defaultVal\":\"(1, 1)\",\"val\":\"(1, 1)\",\"pyArgumentValue\":\"(1, 1)\",\"_langMap\":\"propDilation\",\"required\":false,\"_paramNamespace\":\"ModelNode.ComputationalUnit.Param.Tuple\",\"core\":false,\"length\":2}],[\"activation\",{\"defaultVal\":1,\"val\":3,\"pyArgumentValue\":\"tf.nn.tanh\",\"_langMap\":\"propActv\",\"required\":false,\"_paramNamespace\":\"ModelNode.ComputationalUnit.Param.Activations\",\"core\":false}],[\"use_bias\",{\"defaultVal\":true,\"val\":true,\"pyArgumentValue\":\"True\",\"_langMap\":\"propUseBias\",\"required\":false,\"_paramNamespace\":\"ModelNode.ComputationalUnit.Param.Boolean\",\"core\":false}],[\"kernel_initializer\",{\"defaultVal\":{\"_type\":\"GlorotUniform\"},\"val\":{\"_type\":\"GlorotUniform\"},\"pyArgumentValue\":\"initializers.glorot_uniform()\",\"_langMap\":\"propKernelInit\",\"required\":false,\"_paramNamespace\":\"ModelNode.ComputationalUnit.Param.Initializers\",\"core\":false}],[\"bias_initializer\",{\"defaultVal\":{\"_type\":\"Zeros\"},\"val\":{\"_type\":\"Zeros\"},\"pyArgumentValue\":\"initializers.zeros()\",\"_langMap\":\"propBiasInit\",\"required\":false,\"_paramNamespace\":\"ModelNode.ComputationalUnit.Param.Initializers\",\"core\":false}],[\"kernel_regularizer\",{\"defaultVal\":\"None\",\"val\":\"None\",\"_langMap\":\"propKernelReg\",\"required\":false,\"_paramNamespace\":\"ModelNode.ComputationalUnit.Param.KerasRegularizers\",\"core\":false,\"options\":[[\"None\",\"None\"],[\"l1\",\"propL1\"],[\"l2\",\"propL2\"]],\"pyArgumentValue\":\"None\"}],[\"bias_regularizer\",{\"defaultVal\":\"None\",\"val\":\"None\",\"_langMap\":\"propBiasReg\",\"required\":false,\"_paramNamespace\":\"ModelNode.ComputationalUnit.Param.KerasRegularizers\",\"core\":false,\"options\":[[\"None\",\"None\"],[\"l1\",\"propL1\"],[\"l2\",\"propL2\"]],\"pyArgumentValue\":\"None\"}],[\"activity_regularizer\",{\"defaultVal\":\"None\",\"val\":\"None\",\"_langMap\":\"actvReg\",\"required\":false,\"_paramNamespace\":\"ModelNode.ComputationalUnit.Param.KerasRegularizers\",\"core\":false,\"options\":[[\"None\",\"None\"],[\"l1\",\"propL1\"],[\"l2\",\"propL2\"]],\"pyArgumentValue\":\"None\"}],[\"kernel_constraint\",{\"defaultVal\":\"None\",\"val\":\"None\",\"_langMap\":\"propKernelConstraint\",\"required\":false,\"_paramNamespace\":\"ModelNode.ComputationalUnit.Param.KerasConstraints\",\"core\":false,\"options\":[[\"None\",\"None\"],[\"max_norm\",\"propMaxNorm\"],[\"min_max_norm\",\"propMinMaxNorm\"],[\"unit_norm\",\"propUnitNorm\"],[\"non_neg\",\"propNonNeg\"]],\"pyArgumentValue\":\"None\"}],[\"bias_constraint\",{\"defaultVal\":\"None\",\"val\":\"None\",\"_langMap\":\"propBiasConstraint\",\"required\":false,\"_paramNamespace\":\"ModelNode.ComputationalUnit.Param.KerasConstraints\",\"core\":false,\"options\":[[\"None\",\"None\"],[\"max_norm\",\"propMaxNorm\"],[\"min_max_norm\",\"propMinMaxNorm\"],[\"unit_norm\",\"propUnitNorm\"],[\"non_neg\",\"propNonNeg\"]],\"pyArgumentValue\":\"None\"}]],\"_tfMaxIncomingCount\":1,\"_tfTrainingStage\":false,\"_tfInit\":true,\"_isBigLayer\":false,\"_tfIncomingOrder\":\"\",\"_tfList\":true,\"ele\":{},\"forLoss\":false}],[\"Keras Flatten7\",{\"_nodeType\":2,\"name\":\"Keras Flatten7\",\"cuzName\":\"\",\"fromSource\":[[]],\"fromNode\":[[\"Keras Conv2D6\"]],\"toNode\":[[\"Keras Dense8\"]],\"_shape\":[[\"None\",120]],\"_order\":[6],\"_ltTemp\":\"$$$ 7\",\"_lt\":\"Keras Flatten\",\"_langMap\":[[\"en-US\",\"Keras Flatten\"],[\"zh-TW\",\"Keras 展平層\"]],\"_inputCollections\":[],\"_outputTensors\":[],\"_built\":false,\"_weights\":[],\"weightLogging\":true,\"_unitNamespace\":\"ModelNode.ComputationalUnit.KerasLayers.Flatten\",\"_tfFunctionName\":\"tf.keras.layers.Flatten\",\"_tfParams\":[],\"_tfMaxIncomingCount\":1,\"_tfTrainingStage\":false,\"_tfInit\":true,\"_isBigLayer\":false,\"_tfIncomingOrder\":\"\",\"_tfList\":true,\"ele\":{},\"forLoss\":false}],[\"Keras Dense8\",{\"_nodeType\":2,\"name\":\"Keras Dense8\",\"cuzName\":\"\",\"fromSource\":[[]],\"fromNode\":[[\"Keras Flatten7\"]],\"toNode\":[[\"Keras Dense14\"]],\"_shape\":[[\"None\",84]],\"_order\":[7],\"_ltTemp\":\"$$$ 8\",\"_lt\":\"Keras Dense\",\"_langMap\":[[\"en-US\",\"Keras Dense\"],[\"zh-TW\",\"Keras 全連結層\"]],\"_inputCollections\":[],\"_outputTensors\":[],\"_built\":false,\"_weights\":[],\"weightLogging\":true,\"_unitNamespace\":\"ModelNode.ComputationalUnit.KerasLayers.Dense\",\"_tfFunctionName\":\"tf.keras.layers.Dense\",\"_tfParams\":[[0,{\"defaultVal\":150,\"val\":84,\"pyArgumentValue\":\"84\",\"_langMap\":\"propHiddenSize\",\"required\":true,\"_paramNamespace\":\"ModelNode.ComputationalUnit.Param.Number\",\"core\":true,\"min\":1,\"max\":null,\"isInteger\":true,\"isCircular\":false}],[\"activation\",{\"defaultVal\":1,\"val\":3,\"pyArgumentValue\":\"tf.nn.tanh\",\"_langMap\":\"propActv\",\"required\":false,\"_paramNamespace\":\"ModelNode.ComputationalUnit.Param.Activations\",\"core\":false}],[\"use_bias\",{\"defaultVal\":true,\"val\":true,\"pyArgumentValue\":\"True\",\"_langMap\":\"propUseBias\",\"required\":false,\"_paramNamespace\":\"ModelNode.ComputationalUnit.Param.Boolean\",\"core\":false}],[\"kernel_initializer\",{\"defaultVal\":{\"_type\":\"GlorotUniform\"},\"val\":{\"_type\":\"GlorotUniform\"},\"pyArgumentValue\":\"initializers.glorot_uniform()\",\"_langMap\":\"propKernelInit\",\"required\":false,\"_paramNamespace\":\"ModelNode.ComputationalUnit.Param.Initializers\",\"core\":false}],[\"bias_initializer\",{\"defaultVal\":{\"_type\":\"Zeros\"},\"val\":{\"_type\":\"Zeros\"},\"pyArgumentValue\":\"initializers.zeros()\",\"_langMap\":\"propBiasInit\",\"required\":false,\"_paramNamespace\":\"ModelNode.ComputationalUnit.Param.Initializers\",\"core\":false}],[\"kernel_regularizer\",{\"defaultVal\":\"None\",\"val\":\"None\",\"_langMap\":\"propKernelReg\",\"required\":false,\"_paramNamespace\":\"ModelNode.ComputationalUnit.Param.KerasRegularizers\",\"core\":false,\"options\":[[\"None\",\"None\"],[\"l1\",\"propL1\"],[\"l2\",\"propL2\"]],\"pyArgumentValue\":\"None\"}],[\"bias_regularizer\",{\"defaultVal\":\"None\",\"val\":\"None\",\"_langMap\":\"propBiasReg\",\"required\":false,\"_paramNamespace\":\"ModelNode.ComputationalUnit.Param.KerasRegularizers\",\"core\":false,\"options\":[[\"None\",\"None\"],[\"l1\",\"propL1\"],[\"l2\",\"propL2\"]],\"pyArgumentValue\":\"None\"}],[\"activity_regularizer\",{\"defaultVal\":\"None\",\"val\":\"None\",\"_langMap\":\"actvReg\",\"required\":false,\"_paramNamespace\":\"ModelNode.ComputationalUnit.Param.KerasRegularizers\",\"core\":false,\"options\":[[\"None\",\"None\"],[\"l1\",\"propL1\"],[\"l2\",\"propL2\"]],\"pyArgumentValue\":\"None\"}],[\"kernel_constraint\",{\"defaultVal\":\"None\",\"val\":\"None\",\"_langMap\":\"propKernelConstraint\",\"required\":false,\"_paramNamespace\":\"ModelNode.ComputationalUnit.Param.KerasConstraints\",\"core\":false,\"options\":[[\"None\",\"None\"],[\"max_norm\",\"propMaxNorm\"],[\"min_max_norm\",\"propMinMaxNorm\"],[\"unit_norm\",\"propUnitNorm\"],[\"non_neg\",\"propNonNeg\"]],\"pyArgumentValue\":\"None\"}],[\"bias_constraint\",{\"defaultVal\":\"None\",\"val\":\"None\",\"_langMap\":\"propBiasConstraint\",\"required\":false,\"_paramNamespace\":\"ModelNode.ComputationalUnit.Param.KerasConstraints\",\"core\":false,\"options\":[[\"None\",\"None\"],[\"max_norm\",\"propMaxNorm\"],[\"min_max_norm\",\"propMinMaxNorm\"],[\"unit_norm\",\"propUnitNorm\"],[\"non_neg\",\"propNonNeg\"]],\"pyArgumentValue\":\"None\"}]],\"_tfMaxIncomingCount\":1,\"_tfTrainingStage\":false,\"_tfInit\":true,\"_isBigLayer\":false,\"_tfIncomingOrder\":\"\",\"_tfList\":true,\"ele\":{},\"forLoss\":false}],[\"Keras Dense14\",{\"_nodeType\":2,\"name\":\"Keras Dense14\",\"cuzName\":\"\",\"fromSource\":[[]],\"fromNode\":[[\"Keras Dense8\"]],\"toNode\":[[\"Task17\"]],\"_shape\":[[\"None\",10]],\"_order\":[8],\"_ltTemp\":\"$$$ 14\",\"_lt\":\"Keras Dense\",\"_langMap\":[[\"en-US\",\"Keras Dense\"],[\"zh-TW\",\"Keras 全連結層\"]],\"_inputCollections\":[],\"_outputTensors\":[],\"_built\":false,\"_weights\":[],\"weightLogging\":true,\"_unitNamespace\":\"ModelNode.ComputationalUnit.KerasLayers.Dense\",\"_tfFunctionName\":\"tf.keras.layers.Dense\",\"_tfParams\":[[0,{\"defaultVal\":150,\"val\":10,\"pyArgumentValue\":\"10\",\"_langMap\":\"propHiddenSize\",\"required\":true,\"_paramNamespace\":\"ModelNode.ComputationalUnit.Param.Number\",\"core\":true,\"min\":1,\"max\":null,\"isInteger\":true,\"isCircular\":false}],[\"activation\",{\"defaultVal\":1,\"val\":2,\"pyArgumentValue\":\"tf.nn.sigmoid\",\"_langMap\":\"propActv\",\"required\":false,\"_paramNamespace\":\"ModelNode.ComputationalUnit.Param.Activations\",\"core\":false}],[\"use_bias\",{\"defaultVal\":true,\"val\":true,\"pyArgumentValue\":\"True\",\"_langMap\":\"propUseBias\",\"required\":false,\"_paramNamespace\":\"ModelNode.ComputationalUnit.Param.Boolean\",\"core\":false}],[\"kernel_initializer\",{\"defaultVal\":{\"_type\":\"GlorotUniform\"},\"val\":{\"_type\":\"GlorotUniform\"},\"pyArgumentValue\":\"initializers.glorot_uniform()\",\"_langMap\":\"propKernelInit\",\"required\":false,\"_paramNamespace\":\"ModelNode.ComputationalUnit.Param.Initializers\",\"core\":false}],[\"bias_initializer\",{\"defaultVal\":{\"_type\":\"Zeros\"},\"val\":{\"_type\":\"Zeros\"},\"pyArgumentValue\":\"initializers.zeros()\",\"_langMap\":\"propBiasInit\",\"required\":false,\"_paramNamespace\":\"ModelNode.ComputationalUnit.Param.Initializers\",\"core\":false}],[\"kernel_regularizer\",{\"defaultVal\":\"None\",\"val\":\"None\",\"_langMap\":\"propKernelReg\",\"required\":false,\"_paramNamespace\":\"ModelNode.ComputationalUnit.Param.KerasRegularizers\",\"core\":false,\"options\":[[\"None\",\"None\"],[\"l1\",\"propL1\"],[\"l2\",\"propL2\"]],\"pyArgumentValue\":\"None\"}],[\"bias_regularizer\",{\"defaultVal\":\"None\",\"val\":\"None\",\"_langMap\":\"propBiasReg\",\"required\":false,\"_paramNamespace\":\"ModelNode.ComputationalUnit.Param.KerasRegularizers\",\"core\":false,\"options\":[[\"None\",\"None\"],[\"l1\",\"propL1\"],[\"l2\",\"propL2\"]],\"pyArgumentValue\":\"None\"}],[\"activity_regularizer\",{\"defaultVal\":\"None\",\"val\":\"None\",\"_langMap\":\"actvReg\",\"required\":false,\"_paramNamespace\":\"ModelNode.ComputationalUnit.Param.KerasRegularizers\",\"core\":false,\"options\":[[\"None\",\"None\"],[\"l1\",\"propL1\"],[\"l2\",\"propL2\"]],\"pyArgumentValue\":\"None\"}],[\"kernel_constraint\",{\"defaultVal\":\"None\",\"val\":\"None\",\"_langMap\":\"propKernelConstraint\",\"required\":false,\"_paramNamespace\":\"ModelNode.ComputationalUnit.Param.KerasConstraints\",\"core\":false,\"options\":[[\"None\",\"None\"],[\"max_norm\",\"propMaxNorm\"],[\"min_max_norm\",\"propMinMaxNorm\"],[\"unit_norm\",\"propUnitNorm\"],[\"non_neg\",\"propNonNeg\"]],\"pyArgumentValue\":\"None\"}],[\"bias_constraint\",{\"defaultVal\":\"None\",\"val\":\"None\",\"_langMap\":\"propBiasConstraint\",\"required\":false,\"_paramNamespace\":\"ModelNode.ComputationalUnit.Param.KerasConstraints\",\"core\":false,\"options\":[[\"None\",\"None\"],[\"max_norm\",\"propMaxNorm\"],[\"min_max_norm\",\"propMinMaxNorm\"],[\"unit_norm\",\"propUnitNorm\"],[\"non_neg\",\"propNonNeg\"]],\"pyArgumentValue\":\"None\"}]],\"_tfMaxIncomingCount\":1,\"_tfTrainingStage\":false,\"_tfInit\":true,\"_isBigLayer\":false,\"_tfIncomingOrder\":\"\",\"_tfList\":true,\"ele\":{},\"forLoss\":true}],[\"Task17\",{\"_nodeType\":1,\"name\":\"Task17\",\"cuzName\":\"\",\"fromSource\":[[]],\"fromNode\":[[\"Keras Dense14\"]],\"toNode\":[[]],\"_shape\":[[\"None\",1]],\"_order\":[9],\"_ltTemp\":\"$$$ 17\",\"_lt\":\"Task\",\"_langMap\":[],\"_inputCollections\":[],\"_outputTensors\":[],\"_built\":false,\"_weights\":[],\"weightLogging\":true,\"_layerType\":5,\"_taskType\":3,\"lossDppKey\":\"target\",\"compareTensorIdx\":\"target\",\"compareSourceID\":0,\"trainingConfig\":{\"initialLearningRate\":0.01,\"learningRateDecayFactor\":0.5,\"numEpochsPerDecay\":1,\"learningRateDecay\":true,\"optimizer\":\"grad\",\"parm\":{}},\"tape\":0,\"applyGradient\":[],\"_final\":true,\"_type\":\"Task\",\"comparisons\":[{\"input\":\"Keras Dense14\",\"target\":\"target\",\"weight\":1,\"measurement\":\"sparse_categorical_crossentropy\",\"metrics\":\"sparse_categorical_crossentropy\"}],\"ele\":{}}]],\"buildConfigs\":[{\"noOfEpoch\":10,\"batchSize\":64,\"shuffle\":true,\"initialLearningRate\":0.001,\"learningRateDecayFactor\":null,\"numEpochsPerDecay\":null,\"exponentialLossDecay\":0.9,\"exponentialVarDecay\":0.9999,\"optimizer\":\"adam\",\"optimizerParams\":{},\"crossValidationType\":null,\"crossValidationProp\":0.2,\"crossValidationCount\":0,\"_cvCount\":0}],\"traceRecord\":15,\"traceItems\":[],\"sources\":[{\"_instanceClass\":3,\"name\":\"\",\"splittable\":false,\"generatorController\":{\"_datasets\":{\"Train\":{\"_epochSize\":null,\"_oriShape\":null,\"_batchSize\":null,\"_shuffle\":null,\"sourceType\":null,\"_fileNames\":null,\"_imgData\":[],\"_labelData\":[],\"_imgRawShape\":[],\"_imgBytes\":0,\"_targetBytes\":0,\"outputsetInfo\":{}},\"ValidationTrain\":{\"_epochSize\":null,\"_oriShape\":null,\"_batchSize\":null,\"_shuffle\":null,\"sourceType\":null,\"_fileNames\":null,\"_imgData\":[],\"_labelData\":[],\"_imgRawShape\":[],\"_imgBytes\":0,\"_targetBytes\":0,\"outputsetInfo\":{}},\"Validation\":{\"_epochSize\":null,\"_oriShape\":null,\"_batchSize\":null,\"_shuffle\":null,\"sourceType\":null,\"_fileNames\":null,\"_imgData\":[],\"_labelData\":[],\"_imgRawShape\":[],\"_imgBytes\":0,\"_targetBytes\":0,\"outputsetInfo\":{}},\"Test\":{\"_epochSize\":null,\"_oriShape\":null,\"_batchSize\":null,\"_shuffle\":null,\"sourceType\":null,\"_fileNames\":null,\"_imgData\":[],\"_labelData\":[],\"_imgRawShape\":[],\"_imgBytes\":0,\"_targetBytes\":0,\"outputsetInfo\":{}},\"Prediction\":{\"_epochSize\":null,\"_oriShape\":null,\"_batchSize\":null,\"_shuffle\":null,\"sourceType\":null,\"_fileNames\":null,\"_imgData\":[],\"_labelData\":[],\"_imgRawShape\":[],\"_imgBytes\":0,\"_targetBytes\":0,\"outputsetInfo\":{}}}},\"outputsetInfo\":[[\"input\",{\"header\":[\"Images\"],\"shape\":[\"None\",32,32,3]}],[\"target\",{\"header\":[\"Class Labels\"],\"shape\":[\"None\",1]}]],\"_propagateDppNodes\":[\"input\"],\"_sourceType\":\"cifar10\",\"coreDataDir\":\"\",\"flattenImg\":false,\"_oriShape\":[\"None\",32,32,3],\"_labelShape\":[\"None\",1],\"_epochSize\":10000,\"dppNodes\":[[\"input\",{\"_instanceClass\":2,\"source\":0,\"crops\":[],\"sourceCol\":\"None:None\",\"_epochSize\":null,\"_getDataMode\":\"Propagate\",\"outputset\":\"input\",\"dtype\":\"tf.float32\",\"_order\":0,\"_shape\":[32,32,3],\"transformations\":[{\"_instanceClass\":3,\"_method\":2,\"_requirePreExtraction\":false,\"colSel\":\"None:None\",\"flipDirection\":1},{\"_instanceClass\":3,\"_method\":7,\"_requirePreExtraction\":false,\"colSel\":\"None:None\",\"maxDelta\":0.2},{\"_instanceClass\":3,\"_method\":5,\"_requirePreExtraction\":false,\"colSel\":\"None:None\",\"lower\":0.8,\"upper\":1.2},{\"_instanceClass\":2,\"_method\":1,\"_requirePreExtraction\":false,\"colSel\":\"None:None\"}],\"preprocessInBatch\":false,\"_dataShape\":[\"None\",32,32,3],\"_key\":\"input\"}],[\"target\",{\"_instanceClass\":1,\"source\":0,\"crops\":[],\"sourceCol\":\"None:None\",\"_epochSize\":null,\"_getDataMode\":\"Propagate\",\"outputset\":\"target\",\"dtype\":\"tf.int64\",\"_order\":0,\"_shape\":[1],\"transformations\":[],\"preprocessInBatch\":true,\"_dataShape\":[\"None\",1],\"_key\":\"target\",\"oneHotColumns\":[],\"circular\":[],\"_classCount\":10}]]}],\"restorePath\":null,\"runCount\":3,\"_editingBuild\":0,\"_editingDatasetType\":0,\"_layerIDInc\":18,\"_version\":210900,\"_oldVersion\":0,\"_currentSourceDataset\":0}")
train.run()